---
title: "Patient Segmentation"
author: "Jeremy Binagia and Sai Gourisankar"
date: "7/5/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notes/Assumptions
- As stated [here](https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data?answertab=votes#tab-top), k-means clustering can not be immediately used with categorical data. For this reason, we will focus on the quantitative data for the purpose of segmentation.
- We also do not want to categorize based on IDs since these are arbitrarily assigned and are not measuring some intrinsic property; thus, we exclude these columns as well.
- Segmentation is only based on variables measured on the initial day.
- Finally, including `PANSS_Total` is redundant so our relavent columns are just the 30 assessments.


## Setup
```{r Load Libraries, results='hide'}
#library(dplyr)
library(ggplot2)
library(hexbin)
library(RColorBrewer)
library(ggrepel)
library(ggfortify)
library(extrafont)
library(factoextra)
library(NbClust)
library(cluster)
library(clValid)
library(ggfortify)
```

```{r Load Data}
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(A_df)
```

## Clean and organize the data before unsupervised learning

### Only examine the first day
```{r first day}
A_df = subset(A_df, VisitDay==0)
B_df = subset(B_df, VisitDay==0)
C_df = subset(C_df, VisitDay==0)
D_df = subset(D_df, VisitDay==0)
E_df = subset(E_df, VisitDay==0)
```

```{r remove columns}
A_sub = A_df[ , -which(names(A_df) %in% c("Study","Country","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
B_sub = B_df[ , -which(names(B_df) %in% c("Study","Country","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
C_sub = C_df[ , -which(names(C_df) %in% c("Study","Country","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
D_sub = D_df[ , -which(names(D_df) %in% c("Study","Country","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
E_sub = E_df[ , -which(names(E_df) %in% c("Study","Country","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
names(A_sub)
```

### Merge dataframes
```{r merge}
combined_df = rbind(A_sub,B_sub,C_sub,D_sub,E_sub)
```

### Scale
```{r scale}
A_scale = scale(A_sub)
B_scale = scale(B_sub)
C_scale = scale(C_sub)
D_scale = scale(D_sub)
E_scale = scale(E_sub)
scaled_df = scale(combined_df)
summary(scaled_df)
```

## determine optimal number of clusters
```{r optimal-k}
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# Gap statistic
# nboot = 50 to keep the function speedy.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.

#set.seed(1)
#fviz_nbclust(scaled_df, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
#  labs(subtitle = "Gap statistic method")
```

## 30 indices
```{r 30-indices}
NbClust(data = scaled_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans");
```

## K-means clustering
```{r k-means}
set.seed(1)
chosen_k = 4

km.out = kmeans(scaled_df, chosen_k, nstart =50)
km.clusters =km.out$cluster

km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
#km.out$centers # cluster means
```

## visualize results
```{r k-means-visual}
#fviz_cluster(km.out, scaled_df,geom = c("point"),labels=combined.labs)
fviz_cluster(km.out, scaled_df,geom = c("point"))
```


## pam clustering
```{r pam}
pam.res <- pam(scaled_df, chosen_k)
 # Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
```

## choosing best algorithm
```{r choosing}
cvalid.out = clValid(scaled_df, nClust = 2:6, clMethods = c("kmeans","pam"), validation = "internal")
summary(cvalid.out)
```


## pca for data interpretation
```{r pca}
pca.out = prcomp(scaled_df, scale=TRUE)
ggplot2::autoplot(pca.out, label = FALSE, loadings.label = TRUE)
```

```{r pca-function}
pcaCharts <- function(x) {
    x.var <- x$sdev ^ 2
    x.pvar <- x.var/sum(x.var)
    print("proportions of variance:")
    print(x.pvar)
    
    par(mfrow=c(2,2))
    plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
    plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
    screeplot(x)
    screeplot(x,type="l")
    par(mfrow=c(1,1))
}

# check proportion of variance explained by each component 
pcaCharts(pca.out)
```
From this analysis, it would be reasonable to focus on the first three principal components in terms of describing the clusters. 

```{r pca-loadings}
pca.out$rotation[,1:3]
```


## Hierarchical clustering
```{r remove columns}
#keep country this time
A_sub_hc = A_df[ , -which(names(A_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
B_sub_hc = B_df[ , -which(names(B_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
C_sub_hc = C_df[ , -which(names(C_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
D_sub_hc = D_df[ , -which(names(D_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
E_sub_hc = E_df[ , -which(names(E_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
names(A_sub)
```

### Merge dataframes for hierarchical clustering
```{r merge}
combined_df_hc = rbind(A_sub_hc,B_sub_hc,C_sub_hc,D_sub_hc,E_sub_hc)
```

### Scale
```{r scale}
combined.data=combined_df_hc[,2:31]
combined.labs=combined_df_hc[,1]
scaled_data = scale(combined.data)
summary(scaled_data)
summary(combined.labs)
```

## Histogram of countries in each cluster
```{r histogram}
combined_df$Country=combined.labs
combined_df$Cluster=km.out$cluster
head(combined_df)

clust1_df=combined_df[which(combined_df$Cluster==1),]
clust2_df=combined_df[which(combined_df$Cluster==2),]
clust3_df=combined_df[which(combined_df$Cluster==3),]
clust4_df=combined_df[which(combined_df$Cluster==4),]

par(mfrow=c(2,2))

hist.c1=barplot(prop.table(table(as.factor(clust1_df$Country))),las=2)
hist.c2=barplot(prop.table(table(as.factor(clust2_df$Country))),las=2)
hist.c3=barplot(prop.table(table(as.factor(clust3_df$Country))),las=2)
hist.c4=barplot(prop.table(table(as.factor(clust4_df$Country))),las=2)

```

```{r hclust}
library("factoextra")
hc.complete=hclust (dist(combined.data),method="complete")
par(mfrow =c(1,1))
plot(hc.complete,labels=combined.labs,main="Complete Linkage",xlab="",sub="",ylab="")
fviz_dend(hc.complete,labels=combined.labs)
```
