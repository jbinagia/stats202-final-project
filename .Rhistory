#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005, # MSE tolerance
stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement
max_runtime_secs = 60*15 # limit how long it runs
)
# perform grid search
gbm_grid2 <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid2",
x = x,
y = y,
training_frame = train,
validation_frame = valid,
hyper_params = hyper_grid,
search_criteria = search_criteria, # add search criteria
ntrees = 10000,
#stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that
#stopping_tolerance = 0,
seed = 1
)
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
grid_id = "gbm_grid2",
sort_by = "mse",
decreasing = FALSE
)
grid_perf
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
rm(hyper_grid)
hyper_grid = list(
#max_depth = c(1), # depth of each tree
min_rows = c(1,2,3), # minimum observations in a terminal node
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(1), # 1 tends to always beat 0.99
sample_rate = c(.5, .75, 1), # row sample rate
col_sample_rate = c(.8, .9, 1)
)
# number of combinations
nrow(expand.grid(hyper_grid))
# # perform grid search
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x,
#   y = y,
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 5000,
#   stopping_rounds = 10,
#   stopping_tolerance = 0,
#   seed = 123
#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005, # MSE tolerance
stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement
max_runtime_secs = 60*15 # limit how long it runs
)
# perform grid search
gbm_grid2 <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid2",
x = x,
y = y,
training_frame = train,
validation_frame = valid,
hyper_params = hyper_grid,
search_criteria = search_criteria, # add search criteria
ntrees = 10000,
#stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that
#stopping_tolerance = 0,
seed = 1
)
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
grid_id = "gbm_grid2",
sort_by = "mse",
decreasing = FALSE
)
grid_perf
h2o.no_progress()
h2o.init(max_mem_size = "8g") # have 16g ram total
h2o.no_progress()
h2o.init(max_mem_size = "8g") # have 16g ram total
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005, # MSE tolerance
stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement
max_runtime_secs = 60*15 # limit how long it runs
)
# perform grid search
gbm_grid2 <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid2",
x = x,
y = y,
training_frame = train,
validation_frame = valid,
hyper_params = hyper_grid,
search_criteria = search_criteria, # add search criteria
ntrees = 10000,
#stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that
#stopping_tolerance = 0,
seed = 1
)
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
grid_id = "gbm_grid2",
sort_by = "mse",
decreasing = FALSE
)
grid_perf
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
library(h2o)          # a java-based platform
library(plyr)
library(ggplot2)
rm(list = ls()) # clear global environment
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
# # check that there are in fact duplicates
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
#
# # check disregarding assessment id
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
#
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values
#n_distinct(prediction.patients)   # gives same result
# number.visits = count(E_df, vars = "PatientID")
#
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
for (i in 1:dim(combined_df)[1]) {
id = combined_df[i,"PatientID"]
patient_df = subset(combined_df,PatientID == id)
final.day = max(patient_df$VisitDay)
#if (final.day==0){ # several patients must have dropped out immediately
#  print(combined_df[i,])
#}
combined_df[i,"FinalDay"] = final.day
}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data
library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[7] = "PANSS_Total"
dim(test_df)
test_df = subset(test_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
forecast_df = test_df
forecast_df = subset(forecast_df, select = c(PatientID, Country, TxGroup, VisitDay, Study))
forecast_df$VisitDay = forecast_df$VisitDay + 7
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
dim(combined_df)
combined_df = anti_join(combined_df, test_df)
dim(combined_df)
training_df = distinct(combined_df)
dim(training_df)[1]
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[7] = "PANSS_Total"
dim(training_df)
training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
h2o.no_progress()
h2o.init(max_mem_size = "8g") # have 16g ram total
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df[,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df[,-"PatientID"])
# training basic GBM model with defaults
h2o.fit1 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10
)
# assess model results
h2o.fit1
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
#max_runtime_secs = 60*1,
seed = 1
)
# model stopped after xx trees
h2o.fit2@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.fit2, xval = TRUE)^2
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
rm(hyper_grid)
hyper_grid = list(
max_depth = c(3,4,5,6,7), # depth of each tree
min_rows = c(1,5,10), # minimum observations in a terminal node
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(1), # 1 tends to always beat 0.99
sample_rate = c(.5, .75, 1), # row sample rate
col_sample_rate = c(.8, .9, 1)
)
# number of combinations
nrow(expand.grid(hyper_grid))
# # perform grid search
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x,
#   y = y,
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 5000,
#   stopping_rounds = 10,
#   stopping_tolerance = 0,
#   seed = 123
#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005, # MSE tolerance
stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement
max_runtime_secs = 60*15 # limit how long it runs
)
# perform grid search
gbm_grid2 <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid2",
x = x,
y = y,
training_frame = train,
validation_frame = valid,
hyper_params = hyper_grid,
search_criteria = search_criteria, # add search criteria
ntrees = 10000,
#stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that
#stopping_tolerance = 0,
seed = 1
)
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
grid_id = "gbm_grid2",
sort_by = "mse",
decreasing = FALSE
)
grid_perf
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
learn_rate = 0.05,
learn_rate_annealing = 1,
max_depth = 5,
min_rows = 1,
sample_rate = 0.4,
col_sample_rate = 0.8,
stopping_rounds = 10,
#stopping_tolerance = 0.005,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
h2o.varimp_plot(h2o.final, num_of_features = 5)
# convert test set to h2o object
test.h2o <- as.h2o(test_df)
# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)
# predict values with predict
h2o.predict(h2o.final, newdata = test.h2o) # predict with h2o.predict
test.h2o$prediction = predict(h2o.final, test.h2o) # gives same result as above
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(forecast_df)
forecast.h2o$PANSS_Total = predict(h2o.final, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'h2o-prediction.csv',force=TRUE)
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
max_runtime_secs = 60*1,
seed = 1
)
# model stopped after xx trees
h2o.fit2@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.fit2, xval = TRUE)^2
# assess model results
h2o.fit1
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
learn_rate = 0.05,
learn_rate_annealing = 1,
max_depth = 5,
min_rows = 1,
sample_rate = 0.4,
col_sample_rate = 0.8,
stopping_rounds = 10,
#stopping_tolerance = 0.005,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 4,
min_rows = 5,
sample_rate = 0.5,
col_sample_rate = 0.8,
stopping_rounds = 10,
#stopping_tolerance = 0.005,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
learn_rate = 0.05,
learn_rate_annealing = 1,
max_depth = 5,
min_rows = 1,
sample_rate = 0.4,
col_sample_rate = 0.8,
stopping_rounds = 10,
#stopping_tolerance = 0.005,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
grid_perf
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
learn_rate = 0.1,
learn_rate_annealing = 1,
max_depth = 4,
min_rows = 10,
sample_rate = 0.75,
col_sample_rate = 0.9,
stopping_rounds = 10,
#stopping_tolerance = 0.005,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
h2o.varimp_plot(h2o.final, num_of_features = 5)
# convert test set to h2o object
test.h2o <- as.h2o(test_df)
# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)
# predict values with predict
h2o.predict(h2o.final, newdata = test.h2o) # predict with h2o.predict
test.h2o$prediction = predict(h2o.final, test.h2o) # gives same result as above
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(forecast_df)
forecast.h2o$PANSS_Total = predict(h2o.final, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'h2o-prediction.csv',force=TRUE)
