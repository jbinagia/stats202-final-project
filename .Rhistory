test=E_df
glm.probs=predict(train.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
names(test)
test.output.glm=as.data.frame(test$AssessmentiD)
test.output.glm$LeadStatus=glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names=FALSE)
plot(test.output.glm$LeadStatus,nb.predictions_df$Flagged,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
full.glm=glm(LeadStatus~.,data=combined_df[,-1],family=binomial)
summary(full.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
test=E_df
glm.probs=predict(full.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
test.full.glm=as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus=glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names=FALSE)
plot(test.full.glm$LeadStatus,test.output.glm$LeadStatus)
abline(0,1)
# A_df = read.csv("Data/Study_A.csv")
# B_df = read.csv("Data/Study_B.csv")
# C_df = read.csv("Data/Study_C.csv")
# D_df = read.csv("Data/Study_D.csv")
# E_df = read.csv("Data/Study_E.csv")
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
str(combined.all) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
str(E_df)
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus)
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all)
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
test.all = E_df # study E is the test set
?str
# A_df = read.csv("Data/Study_A.csv")
# B_df = read.csv("Data/Study_B.csv")
# C_df = read.csv("Data/Study_C.csv")
# D_df = read.csv("Data/Study_D.csv")
# E_df = read.csv("Data/Study_E.csv")
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
# str(E_df)
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus)
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all)
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
test.all = E_df # study E is the test set
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
# str(E_df)
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus)
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all)
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
test.all = E_df # study E is the test set
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
library(dplyr)
library(ggplot2)
A_init_df=read.csv("Data/Study_A.csv")
B_init_df=read.csv("Data/Study_B.csv")
C_init_df=read.csv("Data/Study_C.csv")
D_init_df=read.csv("Data/Study_D.csv")
E_init_df=read.csv("Data/Study_E.csv")
#summary(E_init_df)
names(E_init_df)
dim(E_init_df)[1]
sample_submission_df = read.csv("Data/sample_submission_status.csv")
prediction.ids = sample_submission_df$AssessmentID # the AssessmentID #s we should use for Kaggle submission
length(unique(prediction.ids))
length(prediction.ids)
all(E_init_df$AssessmentiD==prediction.ids)
A_df = subset(A_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
B_df = subset(B_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
C_df = subset(C_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
D_df = subset(D_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
names(E_init_df)
E_df = subset(E_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,AssessmentiD))
names(E_df)
combined_df = rbind(A_df,B_df,C_df,D_df)
summary(combined_df)
names(combined_df)
dim(combined_df)[1]
combined_df=distinct(combined_df)
dim(combined_df)[1]
combined_df <- mutate_at(combined_df, vars(Country, TxGroup,LeadStatus), as.factor)
str(combined_df) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
str(E_df)
combined_df$LeadStatus[combined_df$LeadStatus!="Passed"]<-"Flagged"
combined_df$LeadStatus=factor(combined_df$LeadStatus)
table(combined_df$LeadStatus)
set.seed(1)
tot=1:dim(combined_df)[1] # total number of observations
train=sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train=combined_df[train,]
head(combined.train)
dev=tot[-train] # rest go into development set
combined.dev=combined_df[dev,]
test=E_df # study E is the test set
library(dplyr)
library(h2o)
library(caret)
library(corrplot)
Y.train="LeadStatus"
X.train=setdiff(names(combined.train),c(Y.train,"VisitDay"))
#names(X.train)
h2o.no_progress()
h2o.init()
combined.train.h2o <- combined.train %>%
mutate_if(is.factor, factor, ordered = FALSE) %>%
as.h2o()
str(combined.train.h2o)
train.nb <- h2o.naiveBayes(
x = X.train,
y = Y.train,
training_frame = combined.train.h2o,
nfolds = 10,
laplace = 0
)
# assess results on training data
cM.nb=h2o.confusionMatrix(train.nb)
accuracy.nb=(cM.nb[1,1]+cM.nb[2,2])/(cM.nb[3,1]+cM.nb[3,2])
print(cM.nb)
print(paste("Training accuracy: =",accuracy.nb))
# ROC curve on the development data
names(combined.dev)
combined.dev.h2o=combined.dev[,-3]#get rid of VisitDay
names(combined.dev.h2o)
combined.dev.h2o=combined.dev.h2o %>%
mutate_if(is.factor,factor,ordered=FALSE) %>%
as.h2o()
performance.train=h2o.performance(train.nb,xval=TRUE)
performance.dev=h2o.performance(train.nb,newdata=combined.dev.h2o)
logloss.train = h2o.logloss(performance.train,xval=TRUE)
logloss.dev=h2o.logloss(performance.dev,xval=TRUE)
auc.train <- h2o.auc(performance.train,xval=TRUE)
auc.dev <- h2o.auc(performance.dev)
fpr.dev <- h2o.fpr(performance.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(performance.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
ggplot(aes(fpr, tpr) ) +
geom_line() +
ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
# predict values with predict -->
names(test)
test.h2o=test[,-3]#get rid of VisitDay
#test.h2o=test.h2o[,-4]#get rid of AssessmentiD
test.h2o=test.h2o %>%
mutate_if(is.factor,factor,ordered=FALSE)%>%
as.h2o()
str(test.h2o)
nb.predictions=h2o.predict(train.nb,test.h2o)
nb.predictions_df=as.data.frame(nb.predictions)
test$LeadStatus=nb.predictions_df$Flagged
test.output=test[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,"test.nb.csv",row.names=FALSE)
library(pROC)
names(combined.train)
combined.train.glm=combined.train[,-1] # exclude country from being a predictor
attach(combined.train.glm)
names(combined.train.glm)
train.glm=glm(LeadStatus~.,data=combined.train.glm,family=binomial)
summary(train.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
#dev test
glm.probs.flag.dev=1-predict(train.glm,combined.dev,type="response")
glm.pred=rep("Passed",dim(combined.dev)[1])#create vector of predictions of length the same as dev dataset
glm.pred[glm.probs.flag.dev>0.2]="Flagged"#change relevant values to "Flagged" based on model-predicted value.
table(glm.pred,combined.dev$LeadStatus)
roc.dev=roc(LeadStatus~glm.probs.flag.dev,data=combined.dev)
plot(roc.dev,xlim=c(0,1),ylim=c(0,1))
#kaggle test
test=E_df
glm.probs=predict(train.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
names(test)
test.output.glm=as.data.frame(test$AssessmentiD)
test.output.glm$LeadStatus=glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names=FALSE)
plot(test.output.glm$LeadStatus,nb.predictions_df$Flagged,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
full.glm=glm(LeadStatus~.,data=combined_df[,-1],family=binomial)
summary(full.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
test=E_df
glm.probs=predict(full.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
test.full.glm=as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus=glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names=FALSE)
plot(test.full.glm$LeadStatus,test.output.glm$LeadStatus)
abline(0,1)
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
# str(E_df)
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus) # how many passed vs. not
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
# train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all) # visually check data frame
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
test.all = E_df # study E is the test set
to
tot
dim(combined.all)[1]
dim(combined_df)[1]
combined_df
unique(combined_df)
combined_df
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
default %>%
mutate(prob = ifelse(default == "Yes", 1, 0)) %>%
ggplot(aes(balance, prob)) +
geom_point(alpha = .15) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
ggtitle("Logistic regression model fit") +
xlab("Balance") +
ylab("Probability of Default")
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
default %>%
mutate(prob = ifelse(LeadStatus == "Passed", 1, 0)) %>%
ggplot(aes(balance, prob)) +
geom_point(alpha = .15) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
ggtitle("Logistic regression model fit") +
xlab("VisitDay") +
ylab("Probability of Passing")
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
LeadStatus %>%
mutate(prob = ifelse(LeadStatus == "Passed", 1, 0)) %>%
ggplot(aes(balance, prob)) +
geom_point(alpha = .15) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
ggtitle("Logistic regression model fit") +
xlab("VisitDay") +
ylab("Probability of Passing")
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(model1)
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(model1)
tidy(model1)
install.packages("tidyverse")
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(model1)
library(tidyverse)  # data manipulation and visualization
tidy(model1)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
library(dplyr)
library(ggplot2)
A_init_df=read.csv("Data/Study_A.csv")
B_init_df=read.csv("Data/Study_B.csv")
C_init_df=read.csv("Data/Study_C.csv")
D_init_df=read.csv("Data/Study_D.csv")
E_init_df=read.csv("Data/Study_E.csv")
#summary(E_init_df)
names(E_init_df)
dim(E_init_df)[1]
sample_submission_df = read.csv("Data/sample_submission_status.csv")
prediction.ids = sample_submission_df$AssessmentID # the AssessmentID #s we should use for Kaggle submission
length(unique(prediction.ids))
length(prediction.ids)
all(E_init_df$AssessmentiD==prediction.ids)
A_df = subset(A_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
B_df = subset(B_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
C_df = subset(C_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
D_df = subset(D_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
names(E_init_df)
E_df = subset(E_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,AssessmentiD))
names(E_df)
combined_df = rbind(A_df,B_df,C_df,D_df)
summary(combined_df)
names(combined_df)
dim(combined_df)[1]
combined_df=distinct(combined_df)
dim(combined_df)[1]
combined_df <- mutate_at(combined_df, vars(Country, TxGroup,LeadStatus), as.factor)
str(combined_df) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
str(E_df)
combined_df$LeadStatus[combined_df$LeadStatus!="Passed"]<-"Flagged"
combined_df$LeadStatus=factor(combined_df$LeadStatus)
table(combined_df$LeadStatus)
set.seed(1)
tot=1:dim(combined_df)[1] # total number of observations
train=sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train=combined_df[train,]
head(combined.train)
dev=tot[-train] # rest go into development set
combined.dev=combined_df[dev,]
test=E_df # study E is the test set
library(dplyr)
library(h2o)
library(caret)
library(corrplot)
Y.train="LeadStatus"
X.train=setdiff(names(combined.train),c(Y.train,"VisitDay"))
#names(X.train)
h2o.no_progress()
h2o.init()
combined.train.h2o <- combined.train %>%
mutate_if(is.factor, factor, ordered = FALSE) %>%
as.h2o()
str(combined.train.h2o)
train.nb <- h2o.naiveBayes(
x = X.train,
y = Y.train,
training_frame = combined.train.h2o,
nfolds = 10,
laplace = 0
)
# assess results on training data
cM.nb=h2o.confusionMatrix(train.nb)
accuracy.nb=(cM.nb[1,1]+cM.nb[2,2])/(cM.nb[3,1]+cM.nb[3,2])
print(cM.nb)
print(paste("Training accuracy: =",accuracy.nb))
# ROC curve on the development data
names(combined.dev)
combined.dev.h2o=combined.dev[,-3]#get rid of VisitDay
names(combined.dev.h2o)
combined.dev.h2o=combined.dev.h2o %>%
mutate_if(is.factor,factor,ordered=FALSE) %>%
as.h2o()
performance.train=h2o.performance(train.nb,xval=TRUE)
performance.dev=h2o.performance(train.nb,newdata=combined.dev.h2o)
logloss.train = h2o.logloss(performance.train,xval=TRUE)
logloss.dev=h2o.logloss(performance.dev,xval=TRUE)
auc.train <- h2o.auc(performance.train,xval=TRUE)
auc.dev <- h2o.auc(performance.dev)
fpr.dev <- h2o.fpr(performance.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(performance.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
ggplot(aes(fpr, tpr) ) +
geom_line() +
ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
# predict values with predict -->
names(test)
test.h2o=test[,-3]#get rid of VisitDay
#test.h2o=test.h2o[,-4]#get rid of AssessmentiD
test.h2o=test.h2o %>%
mutate_if(is.factor,factor,ordered=FALSE)%>%
as.h2o()
str(test.h2o)
nb.predictions=h2o.predict(train.nb,test.h2o)
nb.predictions_df=as.data.frame(nb.predictions)
test$LeadStatus=nb.predictions_df$Flagged
test.output=test[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,"test.nb.csv",row.names=FALSE)
library(pROC)
names(combined.train)
combined.train.glm=combined.train[,-1] # exclude country from being a predictor
attach(combined.train.glm)
names(combined.train.glm)
train.glm=glm(LeadStatus~.,data=combined.train.glm,family=binomial)
summary(train.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
#dev test
glm.probs.flag.dev=1-predict(train.glm,combined.dev,type="response")
glm.pred=rep("Passed",dim(combined.dev)[1])#create vector of predictions of length the same as dev dataset
glm.pred[glm.probs.flag.dev>0.2]="Flagged"#change relevant values to "Flagged" based on model-predicted value.
table(glm.pred,combined.dev$LeadStatus)
roc.dev=roc(LeadStatus~glm.probs.flag.dev,data=combined.dev)
plot(roc.dev,xlim=c(0,1),ylim=c(0,1))
#kaggle test
test=E_df
glm.probs=predict(train.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
names(test)
test.output.glm=as.data.frame(test$AssessmentiD)
test.output.glm$LeadStatus=glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names=FALSE)
plot(test.output.glm$LeadStatus,nb.predictions_df$Flagged,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
full.glm=glm(LeadStatus~.,data=combined_df[,-1],family=binomial)
summary(full.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
test=E_df
glm.probs=predict(full.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
test.full.glm=as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus=glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names=FALSE)
plot(test.full.glm$LeadStatus,test.output.glm$LeadStatus)
abline(0,1)
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
# str(E_df)
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus) # how many passed vs. not
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all) # visually check data frame
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
test.all = E_df # study E is the test set
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(model1)
?tidy
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model3)
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model1)
varimp.plot(model1)
varImp.plot(model1)
varImp.plot.train(model1)
library(broom)      # helps to tidy up model outputs
tidy(model1)
plot(caret::varImp(model1))
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model1)
varImpPlot(model1,type=2)
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model1)
caret:varImpPlot(model1,type=2)
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model1)
caret::varImpPlot(model1,type=2)
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model1)
caret::varImpPlot(model1,type=2)
library(broom)      # helps to tidy up model outputs
tidy(model1)
sort(caret::varImp(model1))
caret::varImp(model1)
?sort
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model1)
dev.log.all.pred = predict(model1, newdata = combined.dev.all)
table(combined.dev.all$LeadStatus, dev.log.all.pred$class) %>% prop.table() %>% round(3)
summary(dev.log.all.pred)
dev.log.all.pred
str(dev.log.all.pred)
dev.log.all.pred = predict(model1, newdata = combined.dev.all, type="response")
table(combined.dev.all$LeadStatus, dev.log.all.pred$class) %>% prop.table() %>% round(3)
dev.log.all.pred
str(dev.log.all.pred)
dev.log.all.pred = predict(model1, newdata = combined.dev.all, type="response")
table(combined.dev.all$LeadStatus, dev.log.all.pred) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.log.all.preds == combined.dev.all$LeadStatus)
dev.log.all.pred = predict(model1, newdata = combined.dev.all, type="response")
table(combined.dev.all$LeadStatus, dev.log.all.pred) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.log.all.pred == combined.dev.all$LeadStatus)
# error rate
mean(dev.log.all.pred != combined.dev.all$LeadStatus)
install.packages(c("clValid", "factoextra", "FactoMineR", "NbClust"))
