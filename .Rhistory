test = E_df
glm.probs = predict(full.glm,test,type = "response") #compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag = 1 - glm.probs#probabililty of being flagged for all.
test.full.glm = as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus = glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names = FALSE)
plot(test.output.glm$LeadStatus,test.full.glm$LeadStatus)
abline(0,1)
# create dataframe that has all individual PANSS scores
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object
# fix up LeadStatus column for the purposes of part 4
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus) # how many passed vs. not
# split into training and dev set
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all) # visually check data frame
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
# create test set that has all individual scores
E_df_tmp = subset(E_init_df,select = setdiff(names(E_init_df),c("Country","Study","PatientID","RaterID","PANSS_Total","SiteID")))
E_df_tmp = mutate_at(E_df_tmp,vars(TxGroup,AssessmentiD),as.factor)
test.all = E_df_tmp # study E is the test set
train.all.glm <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(train.all.glm)
library(broom)      # helps to tidy up model outputs
tidy(train.all.glm)
caret::varImp(train.all.glm)
#dev test
glm.probs.flag.dev = 1 - predict(train.all.glm, combined.dev.all, type = "response")
glm.pred = rep("Passed", dim(combined.dev.all)[1])
glm.pred[glm.probs.flag.dev > 0.2] = "Flagged"
table(glm.pred,combined.dev.all$LeadStatus)
roc.dev = roc(LeadStatus~glm.probs.flag.dev,data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test = test.all
glm.probs = predict(train.all.glm, test, type = "response")
glm.probs.flag = 1 - glm.probs
names(test)
test.all.glm = as.data.frame(test$AssessmentiD)
test.all.glm$LeadStatus = glm.probs.flag
colnames(test.all.glm)[colnames(test.all.glm)=="test$AssessmentiD"] <- "AssessmentID"
write.csv(test.all.glm,"Predictions/test.all.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,test.all.glm$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
library(glmnet)
#convert training data to matrix format
x <- model.matrix(LeadStatus~., combined.train.all)
#convert class to numerical variable
y <- ifelse(combined.train.all$LeadStatus=="Passed",1,0)
#perform grid search to find optimal value of lambda
#family= binomial => logistic regression, alpha=1 => lasso
# check docs to explore other type.measure options
cv.out <- cv.glmnet(x,y, alpha=1, family="binomial",type.measure="auc")
#plot result
plot(cv.out)
#min value of lambda
lambda_min <- cv.out$lambda.min
#best value of lambda
lambda_1se <- cv.out$lambda.1se
#regression coefficients
coef(cv.out,s=lambda_1se)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all)
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
str(x_test1)
str(x_test2)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
#write.csv(test.all.glm,"Predictions/test.lasso.glm.csv",row.names = FALSE)
# compare to original logistic regression
#plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,0.5),ylim=c(0,0.5))
#abline(0,1)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
write.csv(test.all.glm,"Predictions/test.lasso.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
write.csv(lasso_prob,"Predictions/test.lasso.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
write.csv(lasso_prob,"Predictions/test.lasso.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,1),ylim=c(0,1))
abline(0,1)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
library(dplyr)
library(ggplot2)
A_init_df=read.csv("Data/Study_A.csv")
B_init_df=read.csv("Data/Study_B.csv")
C_init_df=read.csv("Data/Study_C.csv")
D_init_df=read.csv("Data/Study_D.csv")
E_init_df=read.csv("Data/Study_E.csv")
#summary(E_init_df)
names(E_init_df)
dim(E_init_df)[1]
sample_submission_df = read.csv("Data/sample_submission_status.csv")
prediction.ids = sample_submission_df$AssessmentID # the AssessmentID #s we should use for Kaggle submission
length(unique(prediction.ids))
length(prediction.ids)
all(E_init_df$AssessmentiD==prediction.ids)
A_df = subset(A_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
B_df = subset(B_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
C_df = subset(C_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
D_df = subset(D_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
names(E_init_df)
E_df = subset(E_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,AssessmentiD))
names(E_df)
combined_df = rbind(A_df,B_df,C_df,D_df)
summary(combined_df)
names(combined_df)
dim(combined_df)[1]
combined_df=distinct(combined_df)
dim(combined_df)[1]
combined_df <- mutate_at(combined_df, vars(Country, TxGroup,LeadStatus), as.factor)
str(combined_df) # compactly display structure of the object
E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
str(E_df)
combined_df$LeadStatus[combined_df$LeadStatus!="Passed"]<-"Flagged"
combined_df$LeadStatus=factor(combined_df$LeadStatus)
table(combined_df$LeadStatus)
set.seed(1)
tot=1:dim(combined_df)[1] # total number of observations
train=sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train=combined_df[train,]
head(combined.train)
dev=tot[-train] # rest go into development set
combined.dev=combined_df[dev,]
test=E_df # study E is the test set
library(dplyr)
library(h2o)
library(caret)
library(corrplot)
Y.train="LeadStatus"
X.train=setdiff(names(combined.train),c(Y.train,"VisitDay"))
# h2o.no_progress()
h2o.init()
combined.train.h2o <- combined.train %>%
mutate_if(is.factor, factor, ordered = FALSE) %>%
as.h2o()
str(combined.train.h2o)
train.nb <- h2o.naiveBayes(
x = X.train,
y = Y.train,
training_frame = combined.train.h2o,
nfolds = 10,
laplace = 0
)
# assess results on training data
cM.nb=h2o.confusionMatrix(train.nb)
accuracy.nb=(cM.nb[1,1]+cM.nb[2,2])/(cM.nb[3,1]+cM.nb[3,2])
print(cM.nb)
print(paste("Training accuracy: =",accuracy.nb))
# ROC curve on the development data
names(combined.dev)
combined.dev.h2o=combined.dev[,-3]#get rid of VisitDay
names(combined.dev.h2o)
combined.dev.h2o=combined.dev.h2o %>%
mutate_if(is.factor,factor,ordered=FALSE) %>%
as.h2o()
performance.train=h2o.performance(train.nb,xval=TRUE)
performance.dev=h2o.performance(train.nb,newdata=combined.dev.h2o)
logloss.train = h2o.logloss(performance.train,xval=TRUE)
logloss.dev=h2o.logloss(performance.dev,xval=TRUE)
auc.train <- h2o.auc(performance.train,xval=TRUE)
auc.dev <- h2o.auc(performance.dev)
fpr.dev <- h2o.fpr(performance.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(performance.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
ggplot(aes(fpr, tpr) ) +
geom_line() +
ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
# predict values with predict -->
names(test)
test.h2o=test[,-3]#get rid of VisitDay
#test.h2o=test.h2o[,-4]#get rid of AssessmentiD
test.h2o=test.h2o %>%
mutate_if(is.factor,factor,ordered=FALSE)%>%
as.h2o()
str(test.h2o)
nb.predictions=h2o.predict(train.nb,test.h2o)
nb.predictions_df=as.data.frame(nb.predictions)
test$LeadStatus=nb.predictions_df$Flagged
test.output=test[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,"test.nb.csv",row.names=FALSE)
library(pROC)
names(combined.train)
combined.train.glm = combined.train[,-1] # exclude country from being a predictor
attach(combined.train.glm)
names(combined.train.glm)
train.glm = glm(LeadStatus~.,data = combined.train.glm,family = binomial)
summary(train.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
#dev test
glm.probs.flag.dev = 1-predict(train.glm,combined.dev,type = "response")
glm.pred = rep("Passed",dim(combined.dev)[1])#create vector of predictions of length the same as dev dataset
glm.pred[glm.probs.flag.dev>0.2] = "Flagged"#change relevant values to "Flagged" based on model-predicted value.
table(glm.pred,combined.dev$LeadStatus)
roc.dev = roc(LeadStatus~glm.probs.flag.dev,data = combined.dev)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test = E_df
glm.probs = predict(train.glm,test,type = "response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag = 1-glm.probs#probabililty of being flagged for all.
names(test)
test.output.glm = as.data.frame(test$AssessmentiD)
test.output.glm$LeadStatus = glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names = FALSE)
plot(test.output.glm$LeadStatus,nb.predictions_df$Flagged,xlim = c(0,0.5),ylim = c(0,0.5))
abline(0,1)
full.glm = glm(LeadStatus~.,data = combined_df[,-1],family = binomial)
summary(full.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
test = E_df
glm.probs = predict(full.glm,test,type = "response") #compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag = 1 - glm.probs#probabililty of being flagged for all.
test.full.glm = as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus = glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names = FALSE)
plot(test.output.glm$LeadStatus,test.full.glm$LeadStatus)
abline(0,1)
# create dataframe that has all individual PANSS scores
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object
# fix up LeadStatus column for the purposes of part 4
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus) # how many passed vs. not
# split into training and dev set
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all) # visually check data frame
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
# create test set that has all individual scores
E_df_tmp = subset(E_init_df,select = setdiff(names(E_init_df),c("Country","Study","PatientID","RaterID","PANSS_Total","SiteID")))
E_df_tmp = mutate_at(E_df_tmp,vars(TxGroup,AssessmentiD),as.factor)
test.all = E_df_tmp # study E is the test set
train.all.glm <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(train.all.glm)
library(broom)      # helps to tidy up model outputs
tidy(train.all.glm)
caret::varImp(train.all.glm)
#dev test
glm.probs.flag.dev = 1 - predict(train.all.glm, combined.dev.all, type = "response")
glm.pred = rep("Passed", dim(combined.dev.all)[1])
glm.pred[glm.probs.flag.dev > 0.2] = "Flagged"
table(glm.pred,combined.dev.all$LeadStatus)
roc.dev = roc(LeadStatus~glm.probs.flag.dev,data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test = test.all
glm.probs = predict(train.all.glm, test, type = "response")
glm.probs.flag = 1 - glm.probs
names(test)
test.all.glm = as.data.frame(test$AssessmentiD)
test.all.glm$LeadStatus = glm.probs.flag
colnames(test.all.glm)[colnames(test.all.glm)=="test$AssessmentiD"] <- "AssessmentID"
write.csv(test.all.glm,"Predictions/test.all.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,test.all.glm$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
library(glmnet)
#convert training data to matrix format
x <- model.matrix(LeadStatus~., combined.train.all)
#convert class to numerical variable
y <- ifelse(combined.train.all$LeadStatus=="Passed",0,1)
#perform grid search to find optimal value of lambda
#family= binomial => logistic regression, alpha=1 => lasso
# check docs to explore other type.measure options
cv.out <- cv.glmnet(x,y, alpha=1, family="binomial",type.measure="auc")
#plot result
plot(cv.out)
#min value of lambda
lambda_min <- cv.out$lambda.min
#best value of lambda
lambda_1se <- cv.out$lambda.1se
#regression coefficients
coef(cv.out,s=lambda_1se)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
write.csv(lasso_prob,"Predictions/test.lasso.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,1),ylim=c(0,1))
abline(0,1)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
write.csv(lasso_prob,"Predictions/test.lasso.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
lasso_prob
str(lasso_prob)
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)
#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")
roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))
#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
test.all$LeadStatus = lasso_prob
test.all = subset(test.all, select = c(AssessmentiD,LeadStatus))
write.csv(test.all,"Predictions/test.lasso.glm.csv",row.names = FALSE)
# compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(hexbin)
library(RColorBrewer)
library(ggrepel)
library(ggfortify)
library(extrafont)
library(factoextra)
library("FactoMineR")
library(NbClust)
library(cluster)
library(clValid)
library(ggfortify)
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(A_df)
A_df = subset(A_df, VisitDay==0)
B_df = subset(B_df, VisitDay==0)
C_df = subset(C_df, VisitDay==0)
D_df = subset(D_df, VisitDay==0)
E_df = subset(E_df, VisitDay==0)
A_sub = A_df[ , -which(names(A_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
B_sub = B_df[ , -which(names(B_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
C_sub = C_df[ , -which(names(C_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
D_sub = D_df[ , -which(names(D_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
E_sub = E_df[ , -which(names(E_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
names(A_sub)
combined_df = rbind(A_sub,B_sub,C_sub,D_sub,E_sub)
A_scale = scale(A_sub)
B_scale = scale(B_sub)
C_scale = scale(C_sub)
D_scale = scale(D_sub)
E_scale = scale(E_sub)
scaled_df = scale(combined_df)
summary(scaled_df)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8, clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
# set.seed(1)
# gc()
# fviz_nbclust(scaled_df, kmeans,k.max = 8,iter.max=30,nstart = 25,method="gap_stat",nboot = 50)+
#   labs(subtitle = "Gap statistic method")
# NbClust(data = scaled_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans");
set.seed(1)
chosen_k = 2
km.out = kmeans(scaled_df, chosen_k, nstart = 50)
km.clusters =km.out$cluster
# stats
km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
# visualize
fviz_cluster(km.out, scaled_df,geom = c("point")) + theme_minimal() + labs(title ="")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
# set.seed(1)
# fviz_nbclust(scaled_df, k.max = 10, pam,method="gap_stat",nboot = 50)+
#  labs(subtitle = "Gap statistic method")
pam.res <- pam(scaled_df, chosen_k)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point") + theme_minimal() + labs(title ="")  + theme_minimal() + labs(title ="")
pca.out = prcomp(scaled_df, scale=TRUE)
ggplot2::autoplot(pca.out, label = FALSE, loadings.label = TRUE)
pcaCharts <- function(x) {
x.var <- x$sdev ^ 2
x.pvar <- x.var/sum(x.var)
print("proportions of variance:")
print(x.pvar)
par(mfrow=c(2,2))
plot(x.pvar,xlab="Principal component",
ylab="Proportion of variance explained", ylim=c(0,1), type='b')
plot(cumsum(x.pvar),xlab="Principal component",
ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
screeplot(x)
screeplot(x,type="l")
par(mfrow=c(1,1))
}
# check proportion of variance explained by each component
pcaCharts(pca.out)
pca.out$rotation[,1:3]
res.pca = PCA(scaled_df, graph = FALSE)
print(res.pca)
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
# Graph of variables: default plot
fviz_pca_var(res.pca, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
fviz_pca_ind(res.pca, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
label = FALSE,
repel = TRUE # Avoid text overlapping (slow if many points)
)
pca.out = prcomp(scaled_df, scale=TRUE)
ggplot2::autoplot(pca.out, label = FALSE, loadings.label = TRUE, repel=TRUE) + theme_minimal()
