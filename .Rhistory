abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/lda-prediction.csv",row.names=FALSE)
(lda.full = lda(LeadStatus~., data = combined_df[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(lda.full)
test=E_df
test.lda.pred = predict(lda.full, newdata = test)
test$LeadStatus = test.lda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/lda-full-prediction.csv",row.names=FALSE)
(qda.m1 = qda(LeadStatus~., data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
dev.qda.pred = predict(qda.m1, newdata = combined.dev)
table(combined.dev$LeadStatus, dev.qda.pred$class) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.qda.pred$class == combined.dev$LeadStatus)
# error rate
mean(dev.qda.pred$class != combined.dev$LeadStatus)
# ROC curves
library(ROCR)
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
performance(measure = "auc") %>%
.@y.values
# Cross-entropy
prediction(dev.qda.pred$posterior[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
performance(measure = "mxe") %>%
.@y.values
test=E_df
test.qda.pred = predict(qda.m1, newdata = test)
test$LeadStatus = test.qda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/qda-prediction.csv",row.names=FALSE)
library(e1071)        # SVM methodology
set.seed(1)
svmfit <- svm(LeadStatus~., kernel = "linear",data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(svmfit)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
# # find optimal cost of misclassification
# tune.out <- tune(svm, LeadStatus~., kernel = "linear", data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)), probability=TRUE)
# # extract the best model
# summary(tune.out)
# (bestmod <- tune.out$best.model)
# summary(bestmod)
dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE)
table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.svm.pred == combined.dev$LeadStatus)
# error rate
mean(dev.svm.pred != combined.dev$LeadStatus)
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "auc") %>%
.@y.values
# Cross-entropy
prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
performance(measure = "mxe") %>%
.@y.values
test=E_df
test.svm.pred = predict(svmfit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/svm-prediction.csv",row.names=FALSE)
set.seed(1)
svmfit <- svm(LeadStatus~., kernel = "linear",data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(svmfit)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
# # find optimal cost of misclassification
# tune.out <- tune(svm, LeadStatus~., kernel = "linear", data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)), probability=TRUE)
# # extract the best model
# summary(tune.out)
# (bestmod <- tune.out$best.model)
# summary(bestmod)
dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE)
table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.svm.pred == combined.dev$LeadStatus)
# error rate
mean(dev.svm.pred != combined.dev$LeadStatus)
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "auc") %>%
.@y.values
# Cross-entropy
prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
performance(measure = "mxe") %>%
.@y.values
test=E_df
test.svm.pred = predict(svmfit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/svm-all-pred-prediction.csv",row.names=FALSE)
set.seed(1)
svmfit <- svm(LeadStatus~., kernel = "radial",data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(svmfit)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
# # find optimal cost of misclassification
# tune.out <- tune(svm, LeadStatus~., kernel = "radial", data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000),
# gamma=c(0.5,1,2,3,4)), probability=TRUE)
# # extract the best model
# summary(tune.out)
# (bestmod <- tune.out$best.model)
# summary(bestmod)
dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE)
table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.svm.pred == combined.dev$LeadStatus)
# error rate
mean(dev.svm.pred != combined.dev$LeadStatus)
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "auc") %>%
.@y.values
# Cross-entropy
prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
performance(measure = "mxe") %>%
.@y.values
test=E_df
test.svm.pred = predict(svmfit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/svm-radial-prediction.csv",row.names=FALSE)
set.seed(1)
svmfit <- svm(LeadStatus~., kernel = "poly",degree=2,data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(svmfit)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
# # find optimal cost of misclassification
# tune.out <- tune(svm, LeadStatus~., kernel = "poly", degree=2, data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000),
# gamma=c(0.5,1,2,3,4)), probability=TRUE)
# # extract the best model
# summary(tune.out)
# (bestmod <- tune.out$best.model)
# summary(bestmod)
dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE)
table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.svm.pred == combined.dev$LeadStatus)
# error rate
mean(dev.svm.pred != combined.dev$LeadStatus)
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
performance(measure = "auc") %>%
.@y.values
# Cross-entropy
prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
performance(measure = "mxe") %>%
.@y.values
test=E_df
test.svm.pred = predict(svmfit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/svm-poly2-prediction.csv",row.names=FALSE)
#h2o.no_progress()
h2o.removeAll()
h2o.init(max_mem_size="4g")
Y.train = "LeadStatus"
X.train = setdiff(names(combined.train),c(Y.train,"Country"))
combined.train.h2o.gbm = as.h2o(combined.train)
combined.dev.h2o.gbm = as.h2o(combined.dev)
h2o.gbm.fit1 = h2o.gbm(x = X.train,y = Y.train,training_frame = combined.train.h2o.gbm, nfolds = 6)
h2o.gbm.fit1
#define function to easily plot ROC curve each time
getROC.h2o<-function(h2o.fit,dev.h2o.data){
pfm.gbm.dev = h2o.performance(h2o.fit,newdata = dev.h2o.data)
logloss.dev = h2o.logloss(pfm.gbm.dev)
auc.dev <- h2o.auc(pfm.gbm.dev)
fpr.dev <- h2o.fpr(pfm.gbm.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(pfm.gbm.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
ggplot(aes(fpr, tpr) ) +
geom_line() +
ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
}
#plot ROC curve
getROC.h2o(h2o.gbm.fit1,combined.dev.h2o.gbm)
h2o.gbm.fit2 = h2o.gbm(x=X.train,y=Y.train,training_frame=combined.train.h2o.gbm,nfolds=6,ntrees=1000,stopping_rounds=10,stopping_tolerance=0,seed=123)
h2o.gbm.fit2
getROC.h2o(h2o.gbm.fit2,combined.dev.h2o.gbm)
hyper_grid=list(
max_depth = c(3, 5, 6), # 2 < and < 7
min_rows = c(1, 10, 20, 30),
learn_rate = c(0.005, 0.01, 0.1, 0.5), # > 0.001 and < 1
learn_rate_annealing = c(1), # 1 is best
sample_rate = c(.6, .7,.8,.9), # < 1
col_sample_rate = c(0.7,.8, .9) # < 1
)
search_criteria=list(
strategy="RandomDiscrete",
stopping_metric="logloss",
stopping_tolerance=0.005,
stopping_rounds=10,
max_runtime_secs=60*30
)
grid = h2o.grid(
algorithm = "gbm",
grid_id = "gbm_random_discrete",
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
validation_frame = combined.dev.h2o.gbm,
hyper_params = hyper_grid,
search_criteria = search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
grid_perf= h2o.getGrid(grid_id="gbm_random_discrete",sort_by="logloss",decreasing=FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model.random.discrete = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model.random.discrete,combined.dev.h2o.gbm)
# predict values with predict , make a function to use easily every time
getPredict = function(model.h2o,test_df,file.output){
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
pred = h2o.predict(model.h2o,test.h2o)
pred_df = as.data.frame(pred)
test_df$LeadStatus = pred_df$Flagged
test.output = test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names = FALSE)
}
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
class(LeadStatus)
hyper_grid=list(
max_depth = c(3, 4, 5), # 2 < and < 6
min_rows = c(10, 20, 30, 40),
learn_rate = c(0.0025, 0.005, 0.01, 0.05), # > 0.001 and < 0.1
learn_rate_annealing = c(1), # 1 is best
sample_rate = c(.65, .7,0.75,.8,.85), # > 0.6 and < 0.9
col_sample_rate = c(0.6, 0.7,.8, .9) # > 0.6 and < 1
)
search_criteria=list(
strategy="RandomDiscrete",
stopping_metric="logloss",
stopping_tolerance=0.005,
stopping_rounds=10,
max_runtime_secs=60*30
)
grid = h2o.grid(
algorithm = "gbm",
grid_id = "gbm_random_discrete",
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
validation_frame = combined.dev.h2o.gbm,
hyper_params = hyper_grid,
search_criteria = search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
grid_perf= h2o.getGrid(grid_id="gbm_random_discrete",sort_by="logloss",decreasing=FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model.random.discrete = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model.random.discrete,combined.dev.h2o.gbm)
#h2o.no_progress()
h2o.removeAll()
h2o.init(max_mem_size="4g")
Y.train = "LeadStatus"
X.train = setdiff(names(combined.train),c(Y.train,"Country"))
combined.train.h2o.gbm = as.h2o(combined.train)
combined.dev.h2o.gbm = as.h2o(combined.dev)
h2o.gbm.fit1 = h2o.gbm(x = X.train,y = Y.train,training_frame = combined.train.h2o.gbm, nfolds = 6)
h2o.gbm.fit1
#define function to easily plot ROC curve each time
getROC.h2o<-function(h2o.fit,dev.h2o.data){
pfm.gbm.dev = h2o.performance(h2o.fit,newdata = dev.h2o.data)
logloss.dev = h2o.logloss(pfm.gbm.dev)
auc.dev <- h2o.auc(pfm.gbm.dev)
fpr.dev <- h2o.fpr(pfm.gbm.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(pfm.gbm.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
ggplot(aes(fpr, tpr) ) +
geom_line() +
ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
}
#plot ROC curve
getROC.h2o(h2o.gbm.fit1,combined.dev.h2o.gbm)
h2o.gbm.fit2 = h2o.gbm(x=X.train,y=Y.train,training_frame=combined.train.h2o.gbm,nfolds=6,ntrees=1000,stopping_rounds=10,stopping_tolerance=0,seed=123)
h2o.gbm.fit2
getROC.h2o(h2o.gbm.fit2,combined.dev.h2o.gbm)
hyper_grid=list(
max_depth = c(3, 4, 5), # 2 < and < 6
min_rows = c(10, 20, 30, 40),
learn_rate = c(0.0025, 0.005, 0.01, 0.05), # > 0.001 and < 0.1
learn_rate_annealing = c(1), # 1 is best
sample_rate = c(.65, .7,0.75,.8,.85), # > 0.6 and < 0.9
col_sample_rate = c(0.6, 0.7,.8, .9) # > 0.6 and < 1
)
search_criteria=list(
strategy="RandomDiscrete",
stopping_metric="logloss",
stopping_tolerance=0.005,
stopping_rounds=10,
max_runtime_secs=60*30
)
grid = h2o.grid(
algorithm = "gbm",
grid_id = "gbm_random_discrete",
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
validation_frame = combined.dev.h2o.gbm,
hyper_params = hyper_grid,
search_criteria = search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
grid_perf= h2o.getGrid(grid_id="gbm_random_discrete",sort_by="logloss",decreasing=FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model.random.discrete = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model.random.discrete,combined.dev.h2o.gbm)
# predict values with predict , make a function to use easily every time
getPredict = function(model.h2o,test_df,file.output){
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
pred = h2o.predict(model.h2o,test.h2o)
pred_df = as.data.frame(pred)
test_df$LeadStatus = pred_df$Flagged
test.output = test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names = FALSE)
}
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
best_model.random.discrete$LeadStatus
test$LeadStatus
class(test$LeadStatus)
test.h2o = test
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
test.h2o = test %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
pred = h2o.predict(model.h2o,test.h2o)
model.h2o = best_model.random.discrete
pred = h2o.predict(model.h2o,test.h2o)
test.h2o$LeadStatus
class(test.h2o$LeadStatus)
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
pred = h2o.predict(model.h2o,test.h2o)
pred_df = as.data.frame(pred)
test_df$LeadStatus = pred_df$Flagged
test.output = test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names = FALSE)
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
str(test.h2o)
class(train.nb)
# predict values with predict , make a function to use easily every time
getPredict = function(model.h2o,test_df,file.output){
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
str(test.h2o)
pred = h2o.predict(model.h2o,test.h2o)
pred_df = as.data.frame(pred)
test_df$LeadStatus = pred_df$Flagged
test.output = test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names = FALSE)
}
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
# predict values with predict , make a function to use easily every time
getPredict = function(model.h2o,test_df,file.output){
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
#str(test.h2o)
pred = h2o.predict(model.h2o,test.h2o)
pred_df = as.data.frame(pred)
test_df$LeadStatus = pred_df$Flagged
test.output = test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names = FALSE)
}
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
model.h2o = best_model.random.discrete
test = test_df
test_df = test
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
#str(test.h2o)
pred = h2o.predict(model.h2o,test.h2o)
pred_df = as.data.frame(pred)
test_df$LeadStatus = pred_df$Flagged
test.output = test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names = FALSE)
pred = h2o.predict(model.h2o,test.h2o)
?h2o.predict
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
test.h2o
model.h2o
model.h2o$LeadStatus
test_df
test=E_df # study E is the test set
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = combined.train.h2o.gbm,
ntrees = 10000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 4,
min_rows = 30,
sample_rate = 0.75,
col_sample_rate = 0.9,
stopping_rounds = 10,
seed = 1
)
# train final model
h2o.final <- h2o.gbm(
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
ntrees = 10000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 4,
min_rows = 30,
sample_rate = 0.75,
col_sample_rate = 0.9,
stopping_rounds = 10,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final, num_of_features = 5)
# train final model
h2o.final <- h2o.gbm(
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
ntrees = 20000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 4,
min_rows = 30,
sample_rate = 0.75,
col_sample_rate = 0.9,
stopping_rounds = 10,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final, num_of_features = 5)
summary(h2o.final)
