prediction[[1]]
as.vector(prediction)
length(as.vector(prediction))
prediction = predict(best_model, test_df.h2o)
plot(as.vector(prediction), test_df$PANSS_Total)
abline(0,1) # line with y-intercept 0 and slope 1
h2o.mse(best_model_perf)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
library(h2o)          # a java-based platform
library(plyr)
library(ggplot2)
rm(list = ls()) # clear global environment
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
# # check that there are in fact duplicates
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
#
# # check disregarding assessment id
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
#
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values
#n_distinct(prediction.patients)   # gives same result
# number.visits = count(E_df, vars = "PatientID")
#
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
#A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
A_df = subset(A_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
for (i in 1:dim(combined_df)[1]) {
id = combined_df[i,"PatientID"]
patient_df = subset(combined_df,PatientID == id)
final.day = max(patient_df$VisitDay)
#if (final.day==0){ # several patients must have dropped out immediately
#  print(combined_df[i,])
#}
combined_df[i,"FinalDay"] = final.day
}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data
library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[length(names(test_df))] = "PANSS_Total"
dim(test_df)
#test_df = subset(test_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
test_df = subset(test_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
forecast_df = test_df
#forecast_df = subset(forecast_df, select = c(PatientID, Country, TxGroup, VisitDay, Study))
forecast_df = subset(forecast_df, select = c(PatientID, Country, VisitDay, Study))
forecast_df$VisitDay = forecast_df$VisitDay + 7
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
dim(combined_df)
combined_df = anti_join(combined_df, test_df)
dim(combined_df)
training_df = distinct(combined_df)
dim(training_df)[1]
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[length(names(training_df))] = "PANSS_Total"
dim(training_df)
#training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
training_df = subset(training_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
library(randomForest) # basic implementation
# for reproduciblity
set.seed(1)
# default RF model
m1 <- randomForest(
formula = PANSS_Total ~ .,
data    = training_df[,-"PatientID"],
mtry = 2
)
m1
plot(m1)
# number of trees with lowest MSE
which.min(m1$mse)
# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])
require(tidyr)
require(dplyr)
# create training and validation data
set.seed(1)
# split data
training_rows = sample(1:nrow(training_df), floor(nrow(training_df)*0.8))
train_v2 = training_df[training_rows,]
valid = training_df[-training_rows,]
x_test = valid
y_test = valid$PANSS_Total
rf_oob_comp <- randomForest(
formula = PANSS_Total ~ .,
data    = train_v2[,-"PatientID"],
xtest   = x_test[,-c("PatientID","PANSS_Total")],
ytest   = y_test
)
# extract OOB & validation errors
oob <- rf_oob_comp$mse
validation <- rf_oob_comp$test$mse
# compare error rates
tibble::tibble(
`Out of Bag Error` = oob,
`Test error` = validation,
ntrees = 1:rf_oob_comp$ntree
) %>%
gather(Metric, MSE, -ntrees) %>%
ggplot(aes(ntrees, MSE, color = Metric)) +
geom_line() +
xlab("Number of trees")
# start up h2o
h2o.init(max_mem_size = "6g")
set.seed(1)
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df[,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df[,-"PatientID"])
# only train on study E
#x <- setdiff(names(training_df[,-c("PatientID","Study")]), y)
#train.h2o <- as.h2o(subset(training_df, Study=="E",select=c(Country, TxGroup, VisitDay, PANSS_Total)))
# second hypergrid
hyper_grid.h2o <- list(
ntrees      = seq(300, 550, by = 50),
mtries      = 2,
max_depth   = seq(20, 40, by = 5),
min_rows    = seq(8, 10, by = 1),
nbins       = seq(5, 30, by = 5),
sample_rate = c(0.5,0.55,.6,.65,.7)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
# build grid search
random_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid2",
x = x,
y = y,
training_frame = train.h2o,
hyper_params = hyper_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
grid_id = "rf_grid2",
sort_by = "mse",
decreasing = FALSE
)
print(grid_perf2)
# first grid
hyper_grid.h2o <- list(
ntrees      = seq(200, 500, by = 150), # best all had 350 min so set 350 as new min
mtries      = seq(2,4, by = 1), # best all have 2 so set this identically to 2
max_depth   = seq(20, 40, by = 5),
min_rows    = seq(1, 5, by = 2), # best all have 5 (so set 5 as min)
nbins       = seq(10, 30, by = 5),
sample_rate = c(.55, .632, .75) # best all have 0.55 so vary around this
) # best model from this one has test MSE of 131.2987
# second hypergrid
hyper_grid.h2o <- list(
ntrees      = seq(350, 500, by = 75), # none of the top 5 use 500
mtries      = 2,
max_depth   = seq(20, 40, by = 5), # none of top 5 use 40
min_rows    = seq(5, 10, by = 2), # none of the top 5 models use 5
nbins       = seq(10, 30, by = 5), # none of top 5 use 10
sample_rate = c(0.45,.55, .65) # none of the top 5 models use sample_rate of 0.65
) # best model from this has test MSE of 131.1589. best model while using study E gave 130.52. Latter scored 121.37919 on the public leaderboard. second time I ran this I had test MSE of 131.2212. Scored almost the same on Kaggle than when I dropped everything but Study E.
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.varimp_plot(best_model)
# Now letâ€™s evaluate the model performance on a test set
test_df.h2o <- as.h2o(test_df)
best_model_perf <- h2o.performance(model = best_model, newdata = test_df.h2o)
# View prediction
prediction = predict(best_model, test_df.h2o)
plot(as.vector(prediction), test_df$PANSS_Total)
abline(0,1) # line with y-intercept 0 and slope 1
# RMSE of best model
h2o.mse(best_model_perf)
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(forecast_df)
forecast.h2o$PANSS_Total = predict(best_model, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'rf-prediction.csv',force=TRUE)
plot(as.vector(prediction), test_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
summary(A_df)
summary(C_df)
summary(A_df)
summary(E_df)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
library(h2o)          # a java-based platform
library(plyr)
library(ggplot2)
rm(list = ls()) # clear global environment
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
# # check that there are in fact duplicates
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
#
# # check disregarding assessment id
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
#
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values
#n_distinct(prediction.patients)   # gives same result
# number.visits = count(E_df, vars = "PatientID")
#
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
#A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
A_df = subset(A_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
for (i in 1:dim(combined_df)[1]) {
id = combined_df[i,"PatientID"]
patient_df = subset(combined_df,PatientID == id)
final.day = max(patient_df$VisitDay)
#if (final.day==0){ # several patients must have dropped out immediately
#  print(combined_df[i,])
#}
combined_df[i,"FinalDay"] = final.day
}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data
library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[length(names(test_df))] = "PANSS_Total"
dim(test_df)
#test_df = subset(test_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
test_df = subset(test_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
forecast_df = test_df
#forecast_df = subset(forecast_df, select = c(PatientID, Country, TxGroup, VisitDay, Study))
forecast_df = subset(forecast_df, select = c(PatientID, Country, VisitDay, Study))
forecast_df$VisitDay = forecast_df$VisitDay + 7
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
dim(combined_df)
combined_df = anti_join(combined_df, test_df)
dim(combined_df)
training_df = distinct(combined_df)
dim(training_df)[1]
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[length(names(training_df))] = "PANSS_Total"
dim(training_df)
#training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
training_df = subset(training_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
library(randomForest) # basic implementation
# for reproduciblity
set.seed(1)
# default RF model
m1 <- randomForest(
formula = PANSS_Total ~ .,
data    = training_df[,-"PatientID"],
mtry = 2
)
m1
plot(m1)
# number of trees with lowest MSE
which.min(m1$mse)
# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])
require(tidyr)
require(dplyr)
# create training and validation data
set.seed(1)
# split data
training_rows = sample(1:nrow(training_df), floor(nrow(training_df)*0.8))
train_v2 = training_df[training_rows,]
valid = training_df[-training_rows,]
x_test = valid
y_test = valid$PANSS_Total
rf_oob_comp <- randomForest(
formula = PANSS_Total ~ .,
data    = train_v2[,-"PatientID"],
xtest   = x_test[,-c("PatientID","PANSS_Total")],
ytest   = y_test
)
# extract OOB & validation errors
oob <- rf_oob_comp$mse
validation <- rf_oob_comp$test$mse
# compare error rates
tibble::tibble(
`Out of Bag Error` = oob,
`Test error` = validation,
ntrees = 1:rf_oob_comp$ntree
) %>%
gather(Metric, MSE, -ntrees) %>%
ggplot(aes(ntrees, MSE, color = Metric)) +
geom_line() +
xlab("Number of trees")
# start up h2o
h2o.init(max_mem_size = "6g")
set.seed(1)
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df[,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df[,-"PatientID"])
# only train on study E
#x <- setdiff(names(training_df[,-c("PatientID","Study")]), y)
#train.h2o <- as.h2o(subset(training_df, Study=="E",select=c(Country, TxGroup, VisitDay, PANSS_Total)))
# second hypergrid
hyper_grid.h2o <- list(
ntrees      = seq(300, 550, by = 50),
mtries      = 2,
max_depth   = seq(15, 45, by = 5),
min_rows    = seq(7, 11, by = 1),
nbins       = seq(5, 25, by = 5),
sample_rate = c(0.4,0.45,0.5,0.55,.6,.65,.7)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
# build grid search
random_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid2",
x = x,
y = y,
training_frame = train.h2o,
hyper_params = hyper_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
grid_id = "rf_grid2",
sort_by = "mse",
decreasing = FALSE
)
print(grid_perf2)
# first grid
hyper_grid.h2o <- list(
ntrees      = seq(200, 500, by = 150), # best all had 350 min so set 350 as new min
mtries      = seq(2,4, by = 1), # best all have 2 so set this identically to 2
max_depth   = seq(20, 40, by = 5),
min_rows    = seq(1, 5, by = 2), # best all have 5 (so set 5 as min)
nbins       = seq(10, 30, by = 5),
sample_rate = c(.55, .632, .75) # best all have 0.55 so vary around this
) # best model from this one has test MSE of 131.2987
# second hypergrid
hyper_grid.h2o <- list(
ntrees      = seq(350, 500, by = 75), # none of the top 5 use 500
mtries      = 2,
max_depth   = seq(20, 40, by = 5), # none of top 5 use 40
min_rows    = seq(5, 10, by = 2), # none of the top 5 models use 5
nbins       = seq(10, 30, by = 5), # none of top 5 use 10
sample_rate = c(0.45,.55, .65) # none of the top 5 models use sample_rate of 0.65
) # best model from this has test MSE of 131.1589. best model while using study E gave 130.52. Latter scored 121.37919 on the public leaderboard. second time I ran this I had test MSE of 131.2212. Scored almost the same on Kaggle than when I dropped everything but Study E.
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.varimp_plot(best_model)
# Now letâ€™s evaluate the model performance on a test set
test_df.h2o <- as.h2o(test_df)
best_model_perf <- h2o.performance(model = best_model, newdata = test_df.h2o)
# View prediction
prediction = predict(best_model, test_df.h2o)
plot(as.vector(prediction), test_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
abline(0,1) # line with y-intercept 0 and slope 1
# RMSE of best model
h2o.mse(best_model_perf)
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(forecast_df)
forecast.h2o$PANSS_Total = predict(best_model, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'rf-prediction.csv',force=TRUE)
