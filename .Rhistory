}
}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data
library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[7] = "PANSS_Total"
dim(test_df)
dim(combined_df)
combined_df = anti_join(combined_df, test_df)
dim(combined_df)
training_df = distinct(combined_df)
dim(training_df)[1]
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[7] = "PANSS_Total"
dim(training_df)
set.seed(1)
library(gbm)          # basic implementation
# train GBM model
gbm.fit <- gbm(
formula = PANSS_Total ~ .,
distribution = "gaussian",
data = training_df,
n.trees = 1000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 10,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
# print results
print(gbm.fit)
## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train,
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001,
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.
# get MSE and compute RMSE
min(gbm.fit$cv.error)
sqrt(min(gbm.fit$cv.error))
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
library(h2o)          # a java-based platform
h2o.no_progress()
h2o.init(max_mem_size = "5g")
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df)
# training basic GBM model with defaults
h2o.fit1 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 5
)
# assess model results
h2o.fit1
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 1
)
# model stopped after xx trees
h2o.fit2@parameters$ntrees
# cross validated RMSE
h2o.rmse(h2o.fit2, xval = TRUE)
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1, 3, 5),
min_rows = c(1, 5, 10),
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(.99, 1),
sample_rate = c(.4, .6, .8, 1),
col_sample_rate = c(.8, .9, 1)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
# perform grid search
grid <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid2",
x = x,
y = y,
training_frame = train,
validation_frame = valid,
hyper_params = hyper_grid,
search_criteria = search_criteria, # add search criteria
ntrees = 1000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
grid_id = "gbm_grid2",
sort_by = "mse",
decreasing = FALSE
)
grid_perf
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1, 3, 5),
min_rows = c(1, 5, 10),
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(.99, 1),
sample_rate = c(.4, .6, .8, 1),
col_sample_rate = c(.8, .9, 1)
)
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 5,
ntrees = 10000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 3,
min_rows = 10,
sample_rate = 0.75,
col_sample_rate = 1,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
rm(list = ls()) # clear global environment
library(plyr)
library(ggplot2)
rm(list = ls()) # clear global environment
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
# # check that there are in fact duplicates
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
#
# # check disregarding assessment id
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
#
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values
#n_distinct(prediction.patients)   # gives same result
# number.visits = count(E_df, vars = "PatientID")
#
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
for (i in 1:dim(combined_df)[1]) {
id = combined_df[i,"PatientID"]
patient_df = subset(combined_df,PatientID == id)
final.day = max(patient_df$VisitDay)
#if (final.day==0){ # several patients must have dropped out immediately
#  print(combined_df[i,])
#}
combined_df[i,"FinalDay"] = final.day
}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data
library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[7] = "PANSS_Total"
dim(test_df)
test_df = subset(test_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = test_df$VisitDay + 7
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
# dim(combined_df)
# combined_df = anti_join(combined_df, test_df)
# dim(combined_df)
training_df = distinct(combined_df)
dim(training_df)[1]
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[7] = "PANSS_Total"
dim(training_df)
training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
library(h2o)          # a java-based platform
h2o.no_progress()
h2o.init(max_mem_size = "10g") # have 16g total
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df)
# training basic GBM model with defaults
h2o.fit1 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10
)
# assess model results
h2o.fit1
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 1,
max_runtime_secs = 60*5
)
# model stopped after xx trees
h2o.fit2@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.fit2, xval = TRUE)^2
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1,5,10),
min_rows = c(1, 5, 10), # minimum observations in a terminal node
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(1),
sample_rate = c(.4, .6, .8, 1),
col_sample_rate = c(.8, .9, 1)
)
# # perform grid search
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x,
#   y = y,
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 5000,
#   stopping_rounds = 10,
#   stopping_tolerance = 0,
#   seed = 123
#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1,5,10),
min_rows = c(1, 5, 10), # minimum observations in a terminal node
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(1),
sample_rate = c(.4, .6, .8, 1),
col_sample_rate = c(.8, .9, 1)
)
# # perform grid search
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x,
#   y = y,
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 5000,
#   stopping_rounds = 10,
#   stopping_tolerance = 0,
#   seed = 123
#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement
max_runtime_secs = 60*5 # limit how long it runs
)
# perform grid search
grid <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid2",
x = x,
y = y,
training_frame = train,
validation_frame = valid,
hyper_params = hyper_grid,
search_criteria = search_criteria, # add search criteria
ntrees = 1000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
grid_id = "gbm_grid2",
sort_by = "mse",
decreasing = FALSE
)
grid_perf
nrows(search_criteria)
nrows(hyper_grid)
nrow(hyper_grid)
nrow(expand.grid(
max_depth = c(1,5,10),
min_rows = c(1, 5, 10), # minimum observations in a terminal node
learn_rate = c(0.05, 0.1, 0.3),
learn_rate_annealing = c(1),
sample_rate = c(.4, .6, .8, 1),
col_sample_rate = c(.8, .9, 1)
))
nrow(expand.grid(hyper_grid))
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
# Now letâ€™s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1,5,10),
min_rows = c(1), # minimum observations in a terminal node
learn_rate = c(0.05, 0.1, 0.3),
learn_rate_annealing = c(1),
sample_rate = c(.4, .6, .8, 1),
col_sample_rate = c(.8, .9, 1)
)
# number of combinations
nrow(expand.grid(hyper_grid))
# # perform grid search
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x,
#   y = y,
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 5000,
#   stopping_rounds = 10,
#   stopping_tolerance = 0,
#   seed = 123
#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 50,
min_rows = 1,
sample_rate = 0.8,
col_sample_rate = 0.8,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
h2o.varimp_plot(h2o.final, num_of_features = 10)
h2o.varimp_plot(h2o.final, num_of_features = 5)
# convert test set to h2o object
test.h2o <- as.h2o(test_df)
# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)
# predict with h2o.predict
h2o.predict(h2o.final, newdata = test.h2o)
# predict values with predict
predict(h2o.final, test.h2o)
# write to csv for Kaggle submission
write.csv(pred,'healthstudy2.csv')
test.h2o
prediction = predict(h2o.final, test.h2o)
summary(prediction)
prediction
predictionv1 = h2o.predict(h2o.final, newdata = test.h2o)
predictionv1
summary(test_df)
test_df
test_df$prediction = predict(h2o.final, test.h2o) # gives same result as above
class(prediction)
prediction = predict(h2o.final, test.h2o)
class(prediction)
test.h2o
test.h2o$prediction = predict(h2o.final, test.h2o) # gives same result as above
test.h2o
write.csv(test.h2o[,PatientID,prediction],'h2o-prediction.csv')
write.csv(test.h2o[,"PatientID"],'h2o-prediction.csv')
write.csv(test.h2o,'h2o-prediction.csv')
h2o.exportFile(test.h2o,'h2o-prediction.csv')
h2o.exportFile(test.h2o[,PatientID],'h2o-prediction.csv')
h2o.exportFile(test.h2o[,"PatientID"],'h2o-prediction.csv')
h2o.exportFile(test.h2o[,"PatientID"],'h2o-prediction.csv',force=TRUE)
