if (n<3) {
naive_df[naive_df$PatientID == id,"ThirdToLastDay"] = NA
}else{
naive_df[naive_df$PatientID == id,"ThirdToLastDay"] = sort(x,partial=n-2)[n-2]
}
}
# find third to last score
naive_df$ThirdToLastScore = as.numeric(naive_df$FinalScore)
for (id in naive_df$PatientID) { # for each unique id
day = as.integer(naive_df[naive_df$PatientID == id,"ThirdToLastDay"])
if (is.na(day)){
naive_df[naive_df$PatientID == id,"ThirdToLastScore"] = NA
}else{
sub_df = subset(E_df, PatientID==id & VisitDay==day)
if (dim(sub_df)[1] > 1){ # take simple average in this case
naive_df[naive_df$PatientID == id,"ThirdToLastScore"] = mean(sub_df$PANSS_Total)
}else{
naive_df[naive_df$PatientID == id,"ThirdToLastScore"] = sub_df$PANSS_Total
}
}
}
naive_df$PANSS_Total = 0*naive_df$PANSS_Total
for (id in naive_df$PatientID) { # for each unique id
day = as.integer(naive_df[naive_df$PatientID == id,"SecondToLastDay"])
if (is.na(day)){
naive_df[naive_df$PatientID == id,"PANSS_Total"] = naive_df[naive_df$PatientID == id,"FinalScore"]
}else{
day2 = as.integer(naive_df[naive_df$PatientID == id,"ThirdToLastDay"])
if (is.na(day2)){
alpha = 0.9
naive_df[naive_df$PatientID == id,"PANSS_Total"] = alpha*naive_df[naive_df$PatientID == id,"FinalScore"] + alpha*(1-alpha)*naive_df[naive_df$PatientID == id,"SecondToLastScore"]
}else{
alpha = 0.8
naive_df[naive_df$PatientID == id,"PANSS_Total"] = alpha*naive_df[naive_df$PatientID == id,"FinalScore"] + alpha*(1-alpha)*naive_df[naive_df$PatientID == id,"SecondToLastScore"]  + alpha*(1-alpha)^2*naive_df[naive_df$PatientID == id,"ThirdToLastScore"]
}
}
}
# create submission script
write.csv(naive_df[,c("PatientID","PANSS_Total")],'Predictions/less-naive-forecast.csv',row.names=FALSE)
naive_df$PANSS_Total = 0*naive_df$PANSS_Total
for (id in naive_df$PatientID) { # for each unique id
day = as.integer(naive_df[naive_df$PatientID == id,"SecondToLastDay"])
if (is.na(day)){
naive_df[naive_df$PatientID == id,"PANSS_Total"] = naive_df[naive_df$PatientID == id,"FinalScore"]
}else{
day2 = as.integer(naive_df[naive_df$PatientID == id,"ThirdToLastDay"])
alpha = 0.9
naive_df[naive_df$PatientID == id,"PANSS_Total"] = alpha*naive_df[naive_df$PatientID == id,"FinalScore"] + alpha*(1-alpha)*naive_df[naive_df$PatientID == id,"SecondToLastScore"]
}
}
# create submission script
write.csv(naive_df[,c("PatientID","PANSS_Total")],'Predictions/two-prev-days-forecast.csv',row.names=FALSE)
# find fourth to last visit day
for (id in naive_df$PatientID) { # for each unique id
sub_df = subset(E_df, PatientID==id)
x = sub_df$VisitDay
n <- length(x)
if (n<4) {
naive_df[naive_df$PatientID == id,"FourthToLastDay"] = NA
}else{
naive_df[naive_df$PatientID == id,"FourthToLastDay"] = sort(x,partial=n-3)[n-3]
}
}
# find fourth to last score
naive_df$FourthToLastScore = as.numeric(naive_df$FinalScore)
for (id in naive_df$PatientID) { # for each unique id
day = as.integer(naive_df[naive_df$PatientID == id,"FourthToLastDay"])
if (is.na(day)){
naive_df[naive_df$PatientID == id,"FourthToLastScore"] = NA
}else{
sub_df = subset(E_df, PatientID==id & VisitDay==day)
if (dim(sub_df)[1] > 1){ # take simple average in this case
naive_df[naive_df$PatientID == id,"FourthToLastScore"] = mean(sub_df$PANSS_Total)
}else{
naive_df[naive_df$PatientID == id,"FourthToLastScore"] = sub_df$PANSS_Total
}
}
}
naive_df$PANSS_Total = 0*naive_df$PANSS_Total
for (id in naive_df$PatientID) { # for each unique id
day = as.integer(naive_df[naive_df$PatientID == id,"SecondToLastDay"])
if (is.na(day)){
naive_df[naive_df$PatientID == id,"PANSS_Total"] = naive_df[naive_df$PatientID == id,"FinalScore"]
}else{
day2 = as.integer(naive_df[naive_df$PatientID == id,"ThirdToLastDay"])
if (is.na(day2)){
alpha = 0.9
naive_df[naive_df$PatientID == id,"PANSS_Total"] = alpha*naive_df[naive_df$PatientID == id,"FinalScore"] + alpha*(1-alpha)*naive_df[naive_df$PatientID == id,"SecondToLastScore"]
}else{
day3 = as.integer(naive_df[naive_df$PatientID == id,"FourthToLastDay"])
if (is.na(day3)){
alpha = 0.8
naive_df[naive_df$PatientID == id,"PANSS_Total"] = alpha*naive_df[naive_df$PatientID == id,"FinalScore"] + alpha*(1-alpha)*naive_df[naive_df$PatientID == id,"SecondToLastScore"]  + alpha*(1-alpha)^2*naive_df[naive_df$PatientID == id,"ThirdToLastScore"]
}else{
alpha = 0.7
naive_df[naive_df$PatientID == id,"PANSS_Total"] =     alpha*naive_df[naive_df$PatientID == id,"FinalScore"] + alpha*(1-alpha)*naive_df[naive_df$PatientID == id,"SecondToLastScore"]  + alpha*(1-alpha)^2*naive_df[naive_df$PatientID == id,"ThirdToLastScore"] + alpha*(1-alpha)^3*naive_df[naive_df$PatientID == id,"FourthToLastScore"]
}
}
}
}
# create submission script
write.csv(naive_df[,c("PatientID","PANSS_Total")],'Predictions/4-day-naive-forecast.csv',row.names=FALSE)
dim(combined_df)
combined_df = anti_join(combined_df, select_patients_df)
dim(combined_df)
training_df = distinct(combined_df)
dim(training_df)[1]
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[length(names(training_df))] = "PANSS_Total"
dim(training_df)
training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#training_df = subset(training_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
#select_patients_df$PatientID = scale(select_patients_df$PatientID)
#select_patients_df$VisitDay = scale(select_patients_df$VisitDay)
#select_patients_df$PANSS_Total = scale(select_patients_df$PANSS_Total)
h2o.no_progress()
h2o.init(max_mem_size = "6g") # have 16g ram total
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df[,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df[,-"PatientID"])
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
max_runtime_secs = 60*10,
seed = 1
)
# model stopped after xx trees
h2o.fit2@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.fit2, xval = TRUE)^2
# assess model results
h2o.fit2
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
rm(hyper_grid)
hyper_grid = list(
max_depth = c(1,3,4), # depth of each tree
min_rows = c(5,10,20), # minimum observations in a terminal node
learn_rate = c(0.005, 0.01, 0.05, 0.1),
learn_rate_annealing = c(1), # 1 tends to always beat 0.99
sample_rate = c(.65, 0.7, 0.75, 0.8), # row sample rate. better to have less than 1 it seems
col_sample_rate = c(0.7, .8, .9) # always better to have less than 1 here
)
# number of combinations
nrow(expand.grid(hyper_grid))
# # perform grid search
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x,
#   y = y,
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 10000,
#   stopping_rounds = 10,
#   #stopping_tolerance = 0,
#   seed = 1
#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005, # MSE tolerance
stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement
max_runtime_secs = 60*1 # limit how long it runs when debugging
)
# perform grid search
gbm_grid2 <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid2",
x = x,
y = y,
training_frame = train,
validation_frame = valid,
hyper_params = hyper_grid,
search_criteria = search_criteria, # add search criteria
ntrees = 10000,
#stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that
#stopping_tolerance = 0,
seed = 1
)
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
grid_id = "gbm_grid2",
sort_by = "mse",
decreasing = FALSE
)
grid_perf
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
# Now letâ€™s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
# train final model
h2o.final <- h2o.gbm(
x = x,
y = y,
training_frame = train.h2o,
nfolds = 10,
ntrees = 10000,
learn_rate = 0.1,
learn_rate_annealing = 1,
max_depth = 4,
min_rows = 10,
sample_rate = 0.75,
col_sample_rate = 0.9,
stopping_rounds = 10,
#stopping_tolerance = 0.005,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
h2o.varimp_plot(h2o.final, num_of_features = 5)
# convert test set to h2o object
test.h2o <- as.h2o(select_patients_df)
# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)
# predict values with predict
h2o.predict(h2o.final, newdata = test.h2o) # predict with h2o.predict
test.h2o$prediction = predict(h2o.final, test.h2o) # gives same result as above
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(test_df)
forecast.h2o$PANSS_Total = predict(h2o.final, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'h2o-prediction.csv',force=TRUE)
library(randomForest) # basic implementation
# for reproduciblity
set.seed(1)
# default RF model
m1 <- randomForest(
formula = PANSS_Total ~ .,
data    = training_df[,-"PatientID"],
mtry = 2
)
m1
plot(m1)
# number of trees with lowest MSE
which.min(m1$mse)
# MSE of this optimal random forest
m1$mse[which.min(m1$mse)]
require(tidyr)
require(dplyr)
# create training and validation data
set.seed(1)
# split data
training_rows = sample(1:nrow(training_df), floor(nrow(training_df)*0.8))
train_v2 = training_df[training_rows,]
valid = training_df[-training_rows,]
x_test = valid
y_test = valid$PANSS_Total
rf_oob_comp <- randomForest(
formula = PANSS_Total ~ .,
data    = train_v2[,-"PatientID"],
xtest   = x_test[,-c("PatientID","PANSS_Total")],
ytest   = y_test
)
# extract OOB & validation errors
oob <- rf_oob_comp$mse
validation <- rf_oob_comp$test$mse
# compare error rates
tibble::tibble(
`Out of Bag Error` = oob,
`Test error` = validation,
ntrees = 1:rf_oob_comp$ntree
) %>%
gather(Metric, MSE, -ntrees) %>%
ggplot(aes(ntrees, MSE, color = Metric)) +
geom_line() +
xlab("Number of trees")
# start up h2o
h2o.init(max_mem_size = "6g")
set.seed(1)
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df[,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df[,-"PatientID"])
# only train on study E
#x <- setdiff(names(training_df[,-c("PatientID","Study")]), y)
#train.h2o <- as.h2o(subset(training_df, Study=="E",select=c(Country, TxGroup, VisitDay, PANSS_Total)))
# second hypergrid
hyper_grid.h2o <- list(
ntrees      = seq(300, 550, by = 50),
mtries      = 2,
max_depth   = seq(15, 45, by = 5),
min_rows    = seq(7, 11, by = 1),
nbins       = seq(5, 25, by = 5),
sample_rate = c(0.4,0.45,0.5,0.55,.6,.65,.7)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*1 # run for a short time we debugging script
)
# build grid search
random_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid2",
x = x,
y = y,
training_frame = train.h2o,
hyper_params = hyper_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
grid_id = "rf_grid2",
sort_by = "mse",
decreasing = FALSE
)
print(grid_perf2)
# first grid
hyper_grid.h2o <- list(
ntrees      = seq(200, 500, by = 150), # best all had 350 min so set 350 as new min
mtries      = seq(2,4, by = 1), # best all have 2 so set this identically to 2
max_depth   = seq(20, 40, by = 5),
min_rows    = seq(1, 5, by = 2), # best all have 5 (so set 5 as min)
nbins       = seq(10, 30, by = 5),
sample_rate = c(.55, .632, .75) # best all have 0.55 so vary around this
) # best model from this one has test MSE of 131.2987
# second hypergrid
hyper_grid.h2o <- list(
ntrees      = seq(350, 500, by = 75), # none of the top 5 use 500
mtries      = 2,
max_depth   = seq(20, 40, by = 5), # none of top 5 use 40
min_rows    = seq(5, 10, by = 2), # none of the top 5 models use 5
nbins       = seq(10, 30, by = 5), # none of top 5 use 10
sample_rate = c(0.45,.55, .65) # none of the top 5 models use sample_rate of 0.65
) # best model from this has test MSE of 131.1589. best model while using study E gave 130.52. Latter scored 121.37919 on the public leaderboard. second time I ran this I had test MSE of 131.2212. Scored almost the same on Kaggle than when I dropped everything but Study E.
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.varimp_plot(best_model)
# Now letâ€™s evaluate the model performance on a test set
select_patients_df.h2o <- as.h2o(select_patients_df)
best_model_perf <- h2o.performance(model = best_model, newdata = select_patients_df.h2o)
# View prediction
prediction = predict(best_model, select_patients_df.h2o)
plot(as.vector(prediction), select_patients_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
abline(0,1) # line with y-intercept 0 and slope 1
# RMSE of best model
h2o.mse(best_model_perf)
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(test_df)
forecast.h2o$PANSS_Total = predict(best_model, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'rf-prediction.csv',force=TRUE)
training_df = subset(training_df, Study=="E", select = c(PatientID, TxGroup, VisitDay, Study, PANSS_Total))
training_df = subset(training_df, select = c(PatientID, TxGroup, VisitDay, PANSS_Total))
dev_df = subset(dev_df, Study=="E", select = c(PatientID, TxGroup, VisitDay, Study, PANSS_Total))
dev_df = subset(dev_df, select = c(PatientID, TxGroup, VisitDay, PANSS_Total))
linear.mod = lm(PANSS_Total ~., data=training_df)
summary(linear.mod)
# Calculate test MSE
mean((dev_df$PANSS_Total - predict(linear.mod, dev_df))^2)
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatientID, data = dev_df)
# adding exp(-VisitDay) didn't seem to help much
# Ridge regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
ridge.cv.out = cv.out
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - dev_df$PANSS_Total)^2)
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
library(broom)
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
library(glmnet)
set.seed(1)
# Lasso regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1,lambda=grid, thresh=1e-12)
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1)
lasso.cv.out = cv.out
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Calculate test MSE
lasso.pred=predict(lasso.mod,s=bestlam,newx=test.mat)
mean((lasso.pred-dev_df$PANSS_Total)^2)
predict(cv.out, s = bestlam, type = "coefficients")
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize lasso results
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
abline(v = log(cv.out$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
# most influential variables
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# minimum Ridge MSE
min(ridge.cv.out$cvm)
plot(ridge.pred,dev_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
# minimum Lasso MSE
min(lasso.cv.out$cvm)
plot(lasso.pred,dev_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
library(earth)     # fit MARS models
# Fit a basic MARS model
mars1 <- earth(
PANSS_Total ~ .,
data = training_df[,-"PatientID"]
)
# Print model summary
print(mars1)
summary(mars1) %>% .$coefficients %>% head(10)
plot(mars1, which = 1)
# Fit a basic MARS model
mars2 <- earth(
PANSS_Total ~ .,
data = training_df[,-"PatientID"],
degree = 3
)
# check out the first 10 coefficient terms
print(mars2)
summary(mars2) %>% .$coefficients %>% head(10)
plot(mars2, which = 1)
# create a tuning grid
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(1, 16, by = 2)
)
head(hyper_grid)
library(caret)
set.seed(1)
# cross validated model
tuned_mars <- train(
x = subset(training_df[,-"PatientID"], select = -PANSS_Total),
y = training_df$PANSS_Total,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
# best model
tuned_mars$bestTune
summary(tuned_mars)
# plot results
ggplot(tuned_mars)
library(vip)       # variable importance
# variable importance plots
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "gcv") + ggtitle("GCV")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p1, p2, ncol = 2)
test_predict = predict(tuned_mars, dev_df)
mean((dev_df$PANSS_Total - test_predict)^2)
plot(as.vector(test_predict), dev_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
abline(0,1)
test_df$PANSS_Total = predict(tuned_mars, test_df)
write.csv(test_df[,c("PatientID","PANSS_Total")],'mars-forecast.csv',row.names=FALSE)
