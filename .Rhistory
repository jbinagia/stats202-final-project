library(extrafont)
library(factoextra)
library("FactoMineR")
library(NbClust)
library(cluster)
library(clValid)
library(ggfortify)
cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:6, clMethods = c("kmeans","pam"), validation = "internal")
summary(cvalid.out)
pam.res <- pam(scaled_df, 6)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
pam.res <- pam(scaled_df, 3)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
pam.res <- pam(scaled_df, 2)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
pam.res <- pam(scaled_df, 3)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
pam.res <- pam(scaled_df, 4)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
pam.res <- pam(scaled_df, 2)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
summary(cvalid.out)
pam.res <- pam(scaled_df, 6)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
set.seed(1)
chosen_k = 6
km.out = kmeans(scaled_df, chosen_k, nstart =50)
km.clusters =km.out$cluster
km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
#km.out$centers # cluster means
set.seed(1)
chosen_k = 6
km.out = kmeans(scaled_df, chosen_k, nstart =50)
km.clusters =km.out$cluster
# stats
km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
# visualize
fviz_cluster(km.out, scaled_df,geom = c("point"))
set.seed(1)
chosen_k = 2
km.out = kmeans(scaled_df, chosen_k, nstart =50)
km.clusters =km.out$cluster
# stats
km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
# visualize
fviz_cluster(km.out, scaled_df,geom = c("point"))
pam.res <- pam(scaled_df, chosen_k)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
![clValid results](cluster-results.png)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
![clValid results](clustering-results.png)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
!['optional caption text'](clustering-results.png)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
![](clustering-results.png)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
![clValid clustering results](clustering-results.png)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
![](clustering-results.png)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(hexbin)
library(RColorBrewer)
library(ggrepel)
library(ggfortify)
library(extrafont)
library(factoextra)
library("FactoMineR")
library(NbClust)
library(cluster)
library(clValid)
library(ggfortify)
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(A_df)
A_df = subset(A_df, VisitDay==0)
B_df = subset(B_df, VisitDay==0)
C_df = subset(C_df, VisitDay==0)
D_df = subset(D_df, VisitDay==0)
E_df = subset(E_df, VisitDay==0)
A_sub = A_df[ , -which(names(A_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
B_sub = B_df[ , -which(names(B_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
C_sub = C_df[ , -which(names(C_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
D_sub = D_df[ , -which(names(D_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
E_sub = E_df[ , -which(names(E_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
names(A_sub)
combined_df = rbind(A_sub,B_sub,C_sub,D_sub,E_sub)
A_scale = scale(A_sub)
B_scale = scale(B_sub)
C_scale = scale(C_sub)
D_scale = scale(D_sub)
E_scale = scale(E_sub)
scaled_df = scale(combined_df)
summary(scaled_df)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
#set.seed(1)
#fviz_nbclust(scaled_df, kmeans,k.max = 10,iter.max=30,nstart = 25,method="gap_stat",nboot = 50)+
#  labs(subtitle = "Gap statistic method")
#NbClust(data = scaled_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans");
set.seed(1)
chosen_k = 2
km.out = kmeans(scaled_df, chosen_k, nstart =50)
km.clusters =km.out$cluster
# stats
km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
# visualize
fviz_cluster(km.out, scaled_df,geom = c("point"))
pam.res <- pam(scaled_df, chosen_k)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point")
pca.out = prcomp(scaled_df, scale=TRUE)
ggplot2::autoplot(pca.out, label = FALSE, loadings.label = TRUE)
pcaCharts <- function(x) {
x.var <- x$sdev ^ 2
x.pvar <- x.var/sum(x.var)
print("proportions of variance:")
print(x.pvar)
par(mfrow=c(2,2))
plot(x.pvar,xlab="Principal component",
ylab="Proportion of variance explained", ylim=c(0,1), type='b')
plot(cumsum(x.pvar),xlab="Principal component",
ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
screeplot(x)
screeplot(x,type="l")
par(mfrow=c(1,1))
}
# check proportion of variance explained by each component
pcaCharts(pca.out)
pca.out$rotation[,1:3]
res.pca = PCA(scaled_df, graph = FALSE)
print(res.pca)
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
# Graph of variables: default plot
fviz_pca_var(res.pca, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
#keep country this time
A_sub_hc=A_df[,-which(names(A_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD",
"TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
B_sub_hc=B_df[,-which(names(B_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD",
"TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
C_sub_hc=C_df[,-which(names(C_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD",
"TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
D_sub_hc=D_df[,-which(names(D_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD",
"TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
E_sub_hc=E_df[,-which(names(E_df) %in% c("Study","PatientID","SiteID","RaterID","AssessmentiD",
"TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
names(A_sub)
combined_df_hc = rbind(A_sub_hc,B_sub_hc,C_sub_hc,D_sub_hc,E_sub_hc)
combined.data=combined_df_hc[,2:31]
combined.labs=combined_df_hc[,1]
scaled_data = scale(combined.data)
summary(scaled_data)
summary(combined.labs)
combined_df$Country=combined.labs
combined_df$Cluster=km.out$cluster
head(combined_df)
clust1_df=combined_df[which(combined_df$Cluster==1),]
clust2_df=combined_df[which(combined_df$Cluster==2),]
clust3_df=combined_df[which(combined_df$Cluster==3),]
clust4_df=combined_df[which(combined_df$Cluster==4),]
par(mfrow=c(2,2))
hist.c1=barplot(prop.table(table(as.factor(clust1_df$Country))),las=2)
hist.c2=barplot(prop.table(table(as.factor(clust2_df$Country))),las=2)
hist.c3=barplot(prop.table(table(as.factor(clust3_df$Country))),las=2)
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, kmeans,k.max = 10,iter.max=30,nstart = 25,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
#set.seed(1)
#fviz_nbclust(scaled_df, kmeans,k.max = 10,iter.max=30,nstart = 25,method="gap_stat",nboot = 50)+
#  labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
#set.seed(1)
#fviz_nbclust(scaled_df, pam,iter.max=30,nstart = 25,method="gap_stat",nboot = 50)+
#  labs(subtitle = "Gap statistic method")
knitr::opts_chunk$set(echo = TRUE)
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
library(ggplot2)
library(hexbin)
library(RColorBrewer)
library(ggrepel)
library(ggfortify)
library(extrafont)
library(factoextra)
library("FactoMineR")
library(NbClust)
library(cluster)
library(clValid)
library(ggfortify)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(hexbin)
library(RColorBrewer)
library(ggrepel)
library(ggfortify)
library(extrafont)
library(factoextra)
library("FactoMineR")
library(NbClust)
library(cluster)
library(clValid)
library(ggfortify)
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(A_df)
A_df = subset(A_df, VisitDay==0)
B_df = subset(B_df, VisitDay==0)
C_df = subset(C_df, VisitDay==0)
D_df = subset(D_df, VisitDay==0)
E_df = subset(E_df, VisitDay==0)
A_sub = A_df[ , -which(names(A_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
B_sub = B_df[ , -which(names(B_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
C_sub = C_df[ , -which(names(C_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
D_sub = D_df[ , -which(names(D_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
E_sub = E_df[ , -which(names(E_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
names(A_sub)
combined_df = rbind(A_sub,B_sub,C_sub,D_sub,E_sub)
A_scale = scale(A_sub)
B_scale = scale(B_sub)
C_scale = scale(C_sub)
D_scale = scale(D_sub)
E_scale = scale(E_sub)
scaled_df = scale(combined_df)
summary(scaled_df)
#cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8,
#                     clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
#summary(cvalid.out)
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
#set.seed(1)
#fviz_nbclust(scaled_df, kmeans,k.max = 10,iter.max=30,nstart = 25,method="gap_stat",nboot = 500)+
#  labs(subtitle = "Gap statistic method")
#NbClust(data = scaled_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans");
set.seed(1)
chosen_k = 2
km.out = kmeans(scaled_df, chosen_k, nstart =50)
km.clusters =km.out$cluster
# stats
km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
# visualize
fviz_cluster(km.out, scaled_df,geom = c("point"))
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, pam,iter.max=30,nstart = 25,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, pam, nstart = 25,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, pam,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, k.max = 6, pam,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, k.max = 6, pam,method="gap_stat",nboot = 100)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
#set.seed(1)
#fviz_nbclust(scaled_df, k.max = 6, pam,method="gap_stat",nboot = 100)+
#  labs(subtitle = "Gap statistic method")
# max.nc - maximum number of clusters
NbClust(data = scaled_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "pam");
NbClust(data = scaled_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans");
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, kmeans,k.max = 6,iter.max=30,nstart = 25,method="gap_stat",nboot = 100)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, k.max = 6, pam,method="gap_stat",nboot = 100)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
gc()
fviz_nbclust(scaled_df, kmeans,k.max = 10,iter.max=30,nstart = 25,method="gap_stat",nboot = 500)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, k.max = 6, pam,method="gap_stat",nboot = 10)+
labs(subtitle = "Gap statistic method")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, k.max = 10, pam,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
