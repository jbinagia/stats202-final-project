# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - test_df$PANSS_Total)^2)
min(ridge.mod$cvm)       # minimum MSE
ridge.mod$lambda.min     # lambda for this min MSE
ridge.mod$cvm[ridge.mod$lambda == ridge.mod$lambda.1se]  # 1 st.error of min MSE
ridge.mod$lambda.1se  # lambda for this MSE
min(cv.out$cvm)       # minimum MSE
cv.outlambda.min     # lambda for this min MSE
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(ames_ridge$lambda.1se), col = "red", lty = "dashed")
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ ., data = training_df)
test.mat = model.matrix(PANSS_Total ~ ., data = test_df)
# adding exp(-VisitDay) didn't seem to help much
# Ridge regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = log(training_df$PANSS_Total), alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - test_df$PANSS_Total)^2)
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
library(broom)
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
training_df = subset(training_df, select = c(PatientID, TxGroup, VisitDay, Study,PANSS_Total))
test_df = subset(test_df, select = c(PatientID, TxGroup, VisitDay, Study,PANSS_Total))
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ ., data = training_df)
test.mat = model.matrix(PANSS_Total ~ ., data = test_df)
# adding exp(-VisitDay) didn't seem to help much
# Ridge regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - test_df$PANSS_Total)^2)
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
library(broom)
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatiendID, data = test_df)
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-"PatientID", data = training_df)
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatiendID, data = test_df)
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatientID, data = test_df)
# adding exp(-VisitDay) didn't seem to help much
# Ridge regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - test_df$PANSS_Total)^2)
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
library(broom)
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
library(glmnet)
set.seed(1)
# Lasso regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
# Find lambda through cross-validation
cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred-test_df$PANSS_Total)^2)
predict(cv.out, s = bestlam, type = "coefficients")
library(glmnet)
set.seed(1)
# Lasso regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred-test_df$PANSS_Total)^2)
predict(cv.out, s = bestlam, type = "coefficients")
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
## [1] 0.02562055
cv.out$lambda.1se  # lambda for this MSE
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize lasso results
ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(ridge.mod, xvar = "lambda")
abline(v = log(cv.out$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize lasso results
ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(ridge.mod, xvar = "lambda")
abline(v = log(cv.out$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
# most influential variables
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatientID, data = test_df)
# adding exp(-VisitDay) didn't seem to help much
# Ridge regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
ridge.cv.out = cv.out
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - test_df$PANSS_Total)^2)
library(glmnet)
set.seed(1)
# Lasso regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1)
lasso.cv.out = cv.out
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred-test_df$PANSS_Total)^2)
predict(cv.out, s = bestlam, type = "coefficients")
# minimum Ridge MSE
min(ridge.cv.out$cvm)
# minimum Lasso MSE
min(lasso.cv.out$cvm)
ridge.pred
summary(ridge.pred)
summary(test_df$PANSS_Total)
plot(ridge.pred,test_df$PANSS_Total)
plot(lasso.pred,test_df$PANSS_Total)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
library(h2o)          # a java-based platform
library(plyr)
library(ggplot2)
rm(list = ls()) # clear global environment
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
# # check that there are in fact duplicates
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
#
# # check disregarding assessment id
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
#
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
#
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values
#n_distinct(prediction.patients)   # gives same result
# number.visits = count(E_df, vars = "PatientID")
#
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
# A_df = subset(A_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# B_df = subset(B_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# C_df = subset(C_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# D_df = subset(D_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# E_df = subset(E_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
for (i in 1:dim(combined_df)[1]) {
id = combined_df[i,"PatientID"]
patient_df = subset(combined_df,PatientID == id)
final.day = max(patient_df$VisitDay)
#if (final.day==0){ # several patients must have dropped out immediately
#  print(combined_df[i,])
#}
combined_df[i,"FinalDay"] = final.day
}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
for (id in unique(test_df$PatientID)) { # for each unique id
sub_df = subset(test_df, PatientID==id)
if (dim(sub_df)[1]>1){
print(sub_df)
}
}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data
library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[length(names(test_df))] = "PANSS_Total"
dim(test_df)
test_df = subset(test_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#test_df = subset(test_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
forecast_df = test_df
forecast_df = subset(forecast_df, select = c(PatientID, Country, TxGroup, VisitDay, Study))
#forecast_df = subset(forecast_df, select = c(PatientID, Country, VisitDay, Study))
forecast_df$VisitDay = forecast_df$VisitDay + 7
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
# create "Naive" submission
write.csv(test_df[,c("PatientID","PANSS_Total")],'naive-forecast.csv',row.names=FALSE)
dim(combined_df)
combined_df = anti_join(combined_df, test_df)
dim(combined_df)
training_df = distinct(combined_df)
dim(training_df)[1]
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[length(names(training_df))] = "PANSS_Total"
dim(training_df)
training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#training_df = subset(training_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
training_df = subset(training_df, select = c(PatientID, TxGroup, VisitDay, Study,PANSS_Total))
test_df = subset(test_df, select = c(PatientID, TxGroup, VisitDay, Study,PANSS_Total))
linear.mod = lm(PANSS_Total ~., data=training_df)
summary(linear.mod)
# Calculate test MSE
mean((test_df$PANSS_Total - predict(linear.mod, test_df))^2)
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatientID, data = test_df)
# adding exp(-VisitDay) didn't seem to help much
# Ridge regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
ridge.cv.out = cv.out
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - test_df$PANSS_Total)^2)
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
library(broom)
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
library(glmnet)
set.seed(1)
# Lasso regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1,lambda=grid, thresh=1e-12)
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1)
lasso.cv.out = cv.out
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Calculate test MSE
lasso.pred=predict(lasso.mod,s=bestlam,newx=test.mat)
mean((lasso.pred-test_df$PANSS_Total)^2)
predict(cv.out, s = bestlam, type = "coefficients")
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize lasso results
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
abline(v = log(cv.out$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
# most influential variables
coef(cv.out, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# minimum Ridge MSE
min(ridge.cv.out$cvm)
# minimum Lasso MSE
min(lasso.cv.out$cvm)
plot(ridge.pred,test_df$PANSS_Total)
plot(lasso.pred,test_df$PANSS_Total)
# minimum Ridge MSE
min(ridge.cv.out$cvm)
plot(ridge.pred,test_df$PANSS_Total)
# minimum Lasso MSE
min(lasso.cv.out$cvm)
lot(lasso.pred,test_df$PANSS_Total)
# minimum Ridge MSE
min(ridge.cv.out$cvm)
plot(ridge.pred,test_df$PANSS_Total)
# minimum Lasso MSE
min(lasso.cv.out$cvm)
plot(lasso.pred,test_df$PANSS_Total)
# minimum Ridge MSE
min(ridge.cv.out$cvm)
plot(ridge.pred,test_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
# minimum Lasso MSE
min(lasso.cv.out$cvm)
plot(lasso.pred,test_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
