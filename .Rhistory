#kaggle test
test = E_df
glm.probs = predict(train.glm,test,type = "response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag = 1-glm.probs#probabililty of being flagged for all.
names(test)
test.output.glm = as.data.frame(test$AssessmentiD)
test.output.glm$LeadStatus = glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names = FALSE)
plot(test.output.glm$LeadStatus,nb.predictions_df$Flagged,xlim = c(0,0.5),ylim = c(0,0.5))
abline(0,1)
full.glm = glm(LeadStatus~.,data = combined_df[,-1],family = binomial)
summary(full.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
test = E_df
glm.probs = predict(full.glm,test,type = "response") #compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag = 1 - glm.probs#probabililty of being flagged for all.
test.full.glm = as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus = glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names = FALSE)
plot(test.output.glm$LeadStatus,test.full.glm$LeadStatus)
abline(0,1)
# create dataframe that has all individual PANSS scores
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object
# fix up LeadStatus column for the purposes of part 4
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus) # how many passed vs. not
# split into training and dev set
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations
train = sample(tot,length(tot)*0.7) # put 70% of observations into training set
combined.train.all = combined.all[train,]
head(combined.train.all) # visually check data frame
dev = tot[-train] # rest go into development set
combined.dev.all = combined.all[dev,]
# create test set that has all individual scores
E_df_tmp = subset(E_init_df,select = setdiff(names(E_init_df),c("Country","Study","PatientID","RaterID","PANSS_Total","SiteID")))
E_df_tmp = mutate_at(E_df_tmp,vars(TxGroup,AssessmentiD),as.factor)
test.all = E_df_tmp # study E is the test set
model1 <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(model1)
library(broom)      # helps to tidy up model outputs
tidy(model1)
caret::varImp(model1)
library(MASS)       # provides LDA & QDA model functions
(lda.m1 = lda(LeadStatus~., data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(lda.m1)
dev.lda.pred = predict(lda.m1, newdata = combined.dev)
table(combined.dev$LeadStatus, dev.lda.pred$class) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.lda.pred$class == combined.dev$LeadStatus)
# error rate
mean(dev.lda.pred$class != combined.dev$LeadStatus)
# ROC curves
library(ROCR)
prediction(dev.lda.pred$posterior[,2], combined.dev$LeadStatus) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC
prediction(dev.lda.pred$posterior[,2], combined.dev$LeadStatus) %>%
performance(measure = "auc") %>%
.@y.values
# Cross-entropy
prediction(dev.lda.pred$posterior[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
performance(measure = "mxe") %>%
.@y.values
test=E_df
test.lda.pred = predict(lda.m1, newdata = test)
test$LeadStatus = test.lda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/lda-prediction.csv",row.names=FALSE)
(lda.full = lda(LeadStatus~., data = combined_df[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(lda.full)
test=E_df
test.lda.pred = predict(lda.full, newdata = test)
test$LeadStatus = test.lda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/lda-full-prediction.csv",row.names=FALSE)
(qda.m1 = qda(LeadStatus~., data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
dev.qda.pred = predict(qda.m1, newdata = combined.dev)
table(combined.dev$LeadStatus, dev.qda.pred$class) %>% prop.table() %>% round(3)
# accuracy rate
mean(dev.qda.pred$class == combined.dev$LeadStatus)
# error rate
mean(dev.qda.pred$class != combined.dev$LeadStatus)
# ROC curves
library(ROCR)
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
performance(measure = "auc") %>%
.@y.values
# Cross-entropy
prediction(dev.qda.pred$posterior[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
performance(measure = "mxe") %>%
.@y.values
test=E_df
test.qda.pred = predict(qda.m1, newdata = test)
test$LeadStatus = test.qda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]
# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
# write for kaggle prediction
write.csv(test,"Predictions/qda-prediction.csv",row.names=FALSE)
#h2o.no_progress()
h2o.removeAll()
h2o.init(max_mem_size="6g")
Y.train = "LeadStatus"
X.train = setdiff(names(combined.train),c(Y.train,"Country"))
combined.train.h2o.gbm = as.h2o(combined.train)
combined.dev.h2o.gbm = as.h2o(combined.dev)
h2o.gbm.fit1 = h2o.gbm(x = X.train,y = Y.train,training_frame = combined.train.h2o.gbm, nfolds = 6)
h2o.gbm.fit1
#define function to easily plot ROC curve each time
getROC.h2o<-function(h2o.fit,dev.h2o.data){
pfm.gbm.dev = h2o.performance(h2o.fit,newdata = dev.h2o.data)
logloss.dev = h2o.logloss(pfm.gbm.dev)
auc.dev <- h2o.auc(pfm.gbm.dev)
fpr.dev <- h2o.fpr(pfm.gbm.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(pfm.gbm.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
ggplot(aes(fpr, tpr) ) +
geom_line() +
ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
}
#plot ROC curve
getROC.h2o(h2o.gbm.fit1,combined.dev.h2o.gbm)
h2o.gbm.fit2 = h2o.gbm(x=X.train,y=Y.train,training_frame=combined.train.h2o.gbm,nfolds=6,ntrees=1000,stopping_rounds=10,stopping_tolerance=0,seed=123)
h2o.gbm.fit2
getROC.h2o(h2o.gbm.fit2,combined.dev.h2o.gbm)
hyper_grid=list(
max_depth = c(3, 4, 5), # 2 < and < 6
min_rows = c(10, 20, 30, 40),
learn_rate = c(0.0025, 0.005, 0.01, 0.05), # > 0.001 and < 0.1
learn_rate_annealing = c(1), # 1 is best
sample_rate = c(.65, .7,0.75,.8,.85), # > 0.6 and < 0.9
col_sample_rate = c(0.6, 0.7,.8, .9) # > 0.6 and < 1
)
search_criteria=list(
strategy="RandomDiscrete",
stopping_metric="logloss",
stopping_tolerance=0.005,
stopping_rounds=10,
max_runtime_secs=60*1
)
grid = h2o.grid(
algorithm = "gbm",
grid_id = "gbm_random_discrete",
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
validation_frame = combined.dev.h2o.gbm,
hyper_params = hyper_grid,
search_criteria = search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
grid_perf= h2o.getGrid(grid_id="gbm_random_discrete",sort_by="logloss",decreasing=FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model.random.discrete = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model.random.discrete,combined.dev.h2o.gbm)
# train final model
h2o.final <- h2o.gbm(
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
ntrees = 20000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 4,
min_rows = 30,
sample_rate = 0.75,
col_sample_rate = 0.9,
stopping_rounds = 10,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final, num_of_features = 5)
# predict values with predict , make a function to use easily every time
getPredict = function(model.h2o,test_df,file.output){
test.h2o = test_df %>%
mutate_if(is.factor,factor,ordered = FALSE)%>%
as.h2o()
#str(test.h2o)
pred = h2o.predict(model.h2o,test.h2o) #error!!!
pred_df = as.data.frame(pred)
test_df$LeadStatus = pred_df$Flagged
test.output = test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names = FALSE)
plot(test.output.glm$LeadStatus,test_df$LeadStatus,xlim = c(0,0.5),ylim = c(0,0.5))
abline(0,1)
}
test=E_df # study E is the test set
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
Y.train = "LeadStatus"
X.train = setdiff(names(combined.train.all),c(Y.train,"Country")) # the predictors
combined.train.h2o.gbm = as.h2o(combined.train.all)
combined.dev.h2o.gbm = as.h2o(combined.dev.all)
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1, 3, 5),
min_rows = c(1, 5, 10),
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(.99, 1),
sample_rate = c(.5, .75, 1),
col_sample_rate = c(.8, .9, 1)
)
search_criteria = list(
strategy = "RandomDiscrete",
stopping_metric = "logloss",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*1 # covers 36 models in 15 min
)
grid = h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_all",
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
validation_frame = combined.dev.h2o.gbm,
hyper_params = hyper_grid,
search_criteria = search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
grid_perf = h2o.getGrid(grid_id = "gbm_grid_all",sort_by = "logloss",decreasing = FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model,combined.dev.h2o.gbm)
train.h2o <- as.h2o(combined.all)
# train final model with more trees and on full data
h2o.final <- h2o.gbm(
x = X.train,
y = Y.train,
training_frame = train.h2o,
ntrees = 10000,
learn_rate = 0.01,
learn_rate_annealing = 1,
max_depth = 6,
min_rows = 30,
sample_rate = 0.8,
col_sample_rate = 0.8,
stopping_rounds = 10,
seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final)
getPredict(h2o.final,test.all,"Predictions/test.gbm.all.csv")
#library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
# for reproduciblity
set.seed(1)
# default RF model
m1 <- ranger(
formula = LeadStatus ~ .,
data    = combined.train.all,
probability = T
)
m1
# prediction
pred_randomForest <- predict(m1, test.all)
plot(test.output.glm$LeadStatus,pred_randomForest$predictions[,1],xlim=c(0,.5),ylim=c(0,.5))
abline(0,1)
set.seed(1)
# hypergrid
hyper_grid.h2o <- list(
ntrees      = seq(200, 500, by = 150),
mtries      = c(15,20,25,30,33),
max_depth   = seq(20, 40, by = 5),
min_rows    = seq(1, 5, by = 2),
nbins       = seq(10, 30, by = 5),
sample_rate = c(.55, .632, .75)
)
# total number of combinations
nrow(hyper_grid.h2o)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "logloss",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*15
)
# build grid search
random_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid2",
x = X.train,
y = Y.train,
training_frame = combined.train.h2o.gbm,
hyper_params = hyper_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
grid_id = "rf_grid2",
sort_by = "logloss",
decreasing = FALSE
)
summary(grid_perf2)
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
# Now let’s evaluate the model performance on a test set
h2o.varimp_plot(best_model)
getROC.h2o(best_model,combined.dev.h2o.gbm)
# train on full data set
train.h2o <- as.h2o(combined.all)
h2o.final <- h2o.randomForest(
x = X.train,
y = Y.train,
training_frame = train.h2o,
ntrees      = seq(200, 500, by = 150),
mtries      = seq(15, 35, by = 10),
max_depth   = seq(20, 40, by = 5),
min_rows    = seq(1, 5, by = 2),
nbins       = seq(10, 30, by = 5),
sample_rate = c(.55, .632, .75)
stopping_rounds = 10,
summary(grid_perf2)
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
# Now let’s evaluate the model performance on a test set
h2o.varimp_plot(best_model)
getROC.h2o(best_model,combined.dev.h2o.gbm)
# train on full data set
train.h2o <- as.h2o(combined.all)
h2o.final <- h2o.randomForest(
x = X.train,
y = Y.train,
training_frame = train.h2o,
ntrees      = 500,
mtries      = 15,
max_depth   = 25,
min_rows    = 3,
nbins       = 25,
sample_rate = 0.75,
stopping_rounds = 10,
seed = 1
)
# View prediction
getPredict(h2o.final,test.all,"Predictions/test.rf.all.csv")
4*5*5*4*6*3
7200/50*15
2160/60
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(hexbin)
library(RColorBrewer)
library(ggrepel)
library(ggfortify)
library(extrafont)
library(factoextra)
library("FactoMineR")
library(NbClust)
library(cluster)
library(clValid)
library(ggfortify)
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(A_df)
A_df = subset(A_df, VisitDay==0)
B_df = subset(B_df, VisitDay==0)
C_df = subset(C_df, VisitDay==0)
D_df = subset(D_df, VisitDay==0)
E_df = subset(E_df, VisitDay==0)
A_sub = A_df[ , -which(names(A_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
B_sub = B_df[ , -which(names(B_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
C_sub = C_df[ , -which(names(C_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
D_sub = D_df[ , -which(names(D_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
E_sub = E_df[ , -which(names(E_df) %in% c("Study","Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus"))]
names(A_sub)
combined_df = rbind(A_sub,B_sub,C_sub,D_sub,E_sub)
A_scale = scale(A_sub)
B_scale = scale(B_sub)
C_scale = scale(C_sub)
D_scale = scale(D_sub)
E_scale = scale(E_sub)
scaled_df = scale(combined_df)
summary(scaled_df)
cvalid.out = clValid(scaled_df, maxitems = 3000, nClust = 2:8, clMethods = c("kmeans","pam"), validation = c("internal", "stability"))
summary(cvalid.out)
# Elbow method
fviz_nbclust(scaled_df, kmeans, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
gc()
fviz_nbclust(scaled_df, kmeans,k.max = 8,iter.max=30,nstart = 25,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
NbClust(data = scaled_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans");
set.seed(1)
chosen_k = 2
km.out = kmeans(scaled_df, chosen_k, nstart = 50)
km.clusters =km.out$cluster
# stats
km.out$tot.withinss # total within-cluster sum of squares
km.out$withinss # within-cluster sum of squares
km.out$size # cluster size
# visualize
fviz_cluster(km.out, scaled_df,geom = c("point")) + theme_minimal() + labs(title ="")
# Elbow method
fviz_nbclust(scaled_df, pam, method = "wss",print.summary=TRUE) +
geom_vline(xintercept = 2, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_df, pam, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy. Number of Monte Carlo ("bootstrap") samples.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(1)
fviz_nbclust(scaled_df, k.max = 10, pam,method="gap_stat",nboot = 50)+
labs(subtitle = "Gap statistic method")
pam.res <- pam(scaled_df, chosen_k)
# Visualize pam clustering
fviz_cluster(pam.res, geom = "point") + theme_minimal() + labs(title ="")
pca.out = prcomp(scaled_df, scale=TRUE)
ggplot2::autoplot(pca.out, label = FALSE, loadings.label = TRUE)
pcaCharts <- function(x) {
x.var <- x$sdev ^ 2
x.pvar <- x.var/sum(x.var)
print("proportions of variance:")
print(x.pvar)
par(mfrow=c(2,2))
plot(x.pvar,xlab="Principal component",
ylab="Proportion of variance explained", ylim=c(0,1), type='b')
plot(cumsum(x.pvar),xlab="Principal component",
ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
screeplot(x)
screeplot(x,type="l")
par(mfrow=c(1,1))
}
# check proportion of variance explained by each component
pcaCharts(pca.out)
pca.out$rotation[,1:3]
res.pca = PCA(scaled_df, graph = FALSE)
print(res.pca)
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
# Graph of variables: default plot
fviz_pca_var(res.pca, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
omission_vector = c("Country","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus")
combined_all_col = rbind(A_df[ , -which(names(A_df) %in% omission_vector)],B_df[ , -which(names(A_df) %in% omission_vector)],C_df[ , -which(names(A_df) %in% omission_vector)],D_df[ , -which(names(A_df) %in% omission_vector)],E_df[ , -which(names(A_df) %in% omission_vector)])
res.pca <- prcomp(combined_all_col[,-1],  scale = TRUE)
fviz(res.pca, "ind", label = "none",habillage=combined_all_col$Study,addEllipses = "True",alpha=0.3,ellipse.type = "convex") # Individuals plot
res.pca <- prcomp(combined_all_col[,-1],  scale = TRUE)
fviz(res.pca, "ind", label = "none",habillage=combined_all_col$Study,addEllipses = "True",alpha=0.3) # Individuals plot
omission_vector = c("Study","PatientID","SiteID","RaterID",
"AssessmentiD","TxGroup","VisitDay","PANSS_Total","LeadStatus")
combined_all_col = rbind(A_df[ , -which(names(A_df) %in% omission_vector)],B_df[ , -which(names(A_df) %in% omission_vector)],C_df[ , -which(names(A_df) %in% omission_vector)],D_df[ , -which(names(A_df) %in% omission_vector)],E_df[ , -which(names(A_df) %in% omission_vector)])
library(plyr)
sort(table(combined_all_col$Country),decreasing=TRUE)
# consider only top 5 countries
countries = c("USA","China","Russia","Japan","Ukraine")
top_5_country_df = subset(combined_all_col, Country %in% countries)
sum(table(top_5_country_df$Country))/sum(table(combined_all_col$Country)) # these countries make up 80%
res.pca <- prcomp(top_5_country_df[,-1],  scale = TRUE)
fviz(res.pca, "ind", label = "none",habillage=top_5_country_df$Country,addEllipses = TRUE,alpha=0.5,ellipse.alpha = 0.25)
