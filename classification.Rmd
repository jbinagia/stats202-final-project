---
title: "Classification"
author: "Jeremy Binagia and Sai Gourisankar"
date: "7/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Notes/Assumptions
### Model 
  - LeadStatus is the response 
  - likely predictors include
    - Country 
    - TxGroup
    - VisitDay
    - SiteID
    - RaterID
    - PANSS_Total (maybe)
    - PatientID
    
  - we will keep AssessmentID in our dataframe as a "key" for each row/patient. 
  - exclude
    - individual scores: hard to see how to interpret if they impact the assessment status. We'll have total score anyway.
    - Study: no reason not to combine the data.
  - when it comes time to predict, we specify a assessment ID and get a probability of LeadStatus being NOT Passed (so Flagged or Assigned to CS)
  
### Data
  - We'll combine A-D into one dataset
  - The training dataset will be 70% randomly chosen assessment IDs from that combined dataset
  - The development dataset will be the other 30% of that.
  - The test dataset for kaggle will be study E, for which we have no LeadStatus. 
  

# Thoughts/Conclusions

# Setup
```{r Load Libraries, results="hide"}
rm(list = ls()) # clear global environment 
library(dplyr)
library(ggplot2)
```

```{r Load Data}
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
#summary(E_df)
names(E_df)
dim(E_df)[1]
```

The following creates a list of the assessments we should consider for the Kaggle submission. This should be the same as Study E. 
```{r sample submission}
sample_submission_df = read.csv("Data/sample_submission_status.csv")
prediction.ids = sample_submission_df$AssessmentID # the AssessmentID #s we should use for Kaggle submission 
length(unique(prediction.ids)) 
length(prediction.ids)
all(E_df$AssessmentiD==prediction.ids)
```
This is the same length as Study E. All the values are in both. 

## Data Cleaning
Remove columns not corresponding to our predictors and response.

```{r subset-df}
A_df = subset(A_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
B_df = subset(B_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
C_df = subset(C_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
D_df = subset(D_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
names(E_df)
E_df = subset(E_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,AssessmentiD))
names(E_df)
```

Combine studies
```{r combine-studies}
combined_df = rbind(A_df,B_df,C_df,D_df)
summary(combined_df)
names(combined_df)
```
Get rid of any duplicates, by AssessmentiD. 
```{r remove duplicates}
dim(combined_df)[1]
combined_df=distinct(combined_df)
dim(combined_df)[1]
```
We also need to make the ID numbers factors, not numeric data. Indeed, everything but VisitDay and PANSS_Total should be factorized. 
```{r factor}
combined_df <- mutate_at(combined_df, vars(Country, TxGroup,LeadStatus), as.factor)
str(combined_df) # compactly display structure of the object 

E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
str(E_df)
```

And since we just want probability of flagged OR assigned to CS, turn LeadStatus into a binary classification of Passed or Flagged. 
```{r reclassify}
combined_df$LeadStatus[combined_df$LeadStatus!="Passed"]<-"Flagged"
combined_df$LeadStatus=factor(combined_df$LeadStatus)
table(combined_df$LeadStatus)
```


Split data into training, development, and test sets. What we mean by test set here is just Study E, where we have no response whatsoever. 
```{r split data}
set.seed(1)
tot=1:dim(combined_df)[1] # total number of observations 

train=sample(tot,length(tot)*0.7) # put 70% of observations into training set 
combined.train=combined_df[train,]
head(combined.train)

dev=tot[-train] # rest go into development set 
combined.dev=combined_df[dev,]
test=E_df # study E is the test set 
```

# Classification 

## Naive Bayes
First, try a Naive Bayes Classifier. To do this, we have to assume each predictor (PatientID, Country, TxGroup, VisitDay,Study,PANSS_Total,SiteID,RaterID,AssesmentiD) is independent. We also have to get rid of VisitDay as a predictor since Naive Bayes assumes a normal distribution for all quantitative predictors, which would make little sense here. Finally, to predict on Study E, we have to get rid of all the IDs not present in Study E (the IDs unique to other studies)
```{r naive bayes classifier}
library(dplyr)
library(h2o)
library(caret)
library(corrplot)
Y.train="LeadStatus"
X.train=setdiff(names(combined.train),c(Y.train,"VisitDay"))

#names(X.train)
h2o.no_progress()
h2o.init()

combined.train.h2o <- combined.train %>%
  mutate_if(is.factor, factor, ordered = FALSE) %>%
  as.h2o()
str(combined.train.h2o)
train.nb <- h2o.naiveBayes(
  x = X.train,
  y = Y.train,
  training_frame = combined.train.h2o,
  nfolds = 10,
  laplace = 0
)

# assess results on training data
cM.nb=h2o.confusionMatrix(train.nb)
accuracy.nb=(cM.nb[1,1]+cM.nb[2,2])/(cM.nb[3,1]+cM.nb[3,2])
print(cM.nb)
print(paste("Training accuracy: =",accuracy.nb))


# ROC curve on the development data
names(combined.dev)
combined.dev.h2o=combined.dev[,-3]#get rid of VisitDay
names(combined.dev.h2o)
combined.dev.h2o=combined.dev.h2o %>%
  mutate_if(is.factor,factor,ordered=FALSE) %>%
  as.h2o()

performance.train=h2o.performance(train.nb,xval=TRUE)
performance.dev=h2o.performance(train.nb,newdata=combined.dev.h2o)

logloss.train = h2o.logloss(performance.train,xval=TRUE)
logloss.dev=h2o.logloss(performance.dev,xval=TRUE)
auc.train <- h2o.auc(performance.train,xval=TRUE)
auc.dev <- h2o.auc(performance.dev)
fpr.dev <- h2o.fpr(performance.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(performance.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
  ggplot(aes(fpr, tpr) ) +
  geom_line() +
  ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )

# predict values with predict -->
names(test)
test.h2o=test[,-3]#get rid of VisitDay
#test.h2o=test.h2o[,-4]#get rid of AssessmentiD
test.h2o=test.h2o %>%
  mutate_if(is.factor,factor,ordered=FALSE)%>%
  as.h2o()
str(test.h2o)

nb.predictions=h2o.predict(train.nb,test.h2o)
nb.predictions_df=as.data.frame(nb.predictions)
test$LeadStatus=nb.predictions_df$Flagged
test.output=test[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,"test.nb.csv",row.names=FALSE)

```

The naive classifier doesn't do great on our holdout test data. The total development AUC is $0.77$ but the false and true positive rates are high.

## Logistic 
### Training - validation split 
We might try a logistic regression. However, Study E has no country of UK, so we take out the country as a predictor. 
```{r logistic}
library(pROC)
names(combined.train)
combined.train.glm=combined.train[,-1] # exclude country from being a predictor 
attach(combined.train.glm)
names(combined.train.glm)
train.glm=glm(LeadStatus~.,data=combined.train.glm,family=binomial)
summary(train.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down

#dev test
glm.probs.flag.dev=1-predict(train.glm,combined.dev,type="response")
glm.pred=rep("Passed",dim(combined.dev)[1])#create vector of predictions of length the same as dev dataset
glm.pred[glm.probs.flag.dev>0.2]="Flagged"#change relevant values to "Flagged" based on model-predicted value.
table(glm.pred,combined.dev$LeadStatus)
roc.dev=roc(LeadStatus~glm.probs.flag.dev,data=combined.dev)
plot(roc.dev,xlim=c(0,1),ylim=c(0,1))

#kaggle test
test=E_df
glm.probs=predict(train.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
names(test)
test.output.glm=as.data.frame(test$AssessmentiD)
test.output.glm$LeadStatus=glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names=FALSE)

```
It looks like PANSS_Total, VisitDay, Study, TxGroup, and some countries are statistically significant predictors of Passed vs Flagged. 

```{r similarity}
plot(test.output.glm$LeadStatus,nb.predictions_df$Flagged,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
```
### Full training set 
```{r full-logistic}
full.glm=glm(LeadStatus~.,data=combined_df[,-1],family=binomial)
summary(full.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
```

```{r full-log-test-set}
test=E_df
glm.probs=predict(full.glm,test,type="response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag=1-glm.probs#probabililty of being flagged for all.
test.full.glm=as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus=glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names=FALSE)

```

```{r similarity-of-training-and-full}
plot(test.full.glm$LeadStatus,test.output.glm$LeadStatus)
abline(0,1)
```


## LDA
### Training - validation split 
Study E has no country of UK, so we take out the country as a predictor. 
```{r lda-start}
library(MASS)       # provides LDA & QDA model functions
(lda.m1 = lda(LeadStatus~., data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(lda.m1)
```

We can now use our LDA model to make a prediction on the development set
```{r lda-predict}
dev.lda.pred = predict(lda.m1, newdata = combined.dev)
table(combined.dev$LeadStatus, dev.lda.pred$class) %>% prop.table() %>% round(3)

# accuracy rate
mean(dev.lda.pred$class == combined.dev$LeadStatus)

# error rate
mean(dev.lda.pred$class != combined.dev$LeadStatus)
```

```{r roc-curve}
# ROC curves
library(ROCR)
prediction(dev.lda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(dev.lda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy 
prediction(dev.lda.pred$posterior[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r write-lda-result}
test=E_df
test.lda.pred = predict(lda.m1, newdata = test)
test$LeadStatus = test.lda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/lda-prediction.csv",row.names=FALSE)
```
### Use full training set  
```{r lda-full}
(lda.full = lda(LeadStatus~., data = combined_df[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(lda.full)
```

```{r write-lda-full-result}
test=E_df
test.lda.pred = predict(lda.full, newdata = test)
test$LeadStatus = test.lda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/lda-full-prediction.csv",row.names=FALSE)
```




## QDA
```{r qda-start}
(qda.m1 = qda(LeadStatus~., data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
```

We can now use our QDA model to make a prediction on the development set
```{r qda-predict}
dev.qda.pred = predict(qda.m1, newdata = combined.dev)
table(combined.dev$LeadStatus, dev.qda.pred$class) %>% prop.table() %>% round(3)

# accuracy rate
mean(dev.qda.pred$class == combined.dev$LeadStatus)

# error rate
mean(dev.qda.pred$class != combined.dev$LeadStatus)
```
```{r roc-curve-qda}
# ROC curves
library(ROCR)
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy 
prediction(dev.qda.pred$posterior[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r write-qda-result}
test=E_df
test.qda.pred = predict(qda.m1, newdata = test)
test$LeadStatus = test.qda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/qda-prediction.csv",row.names=FALSE)
```

## SVM
### Linear
```{r svm-intro}
library(e1071)        # SVM methodology
set.seed(1)

svmfit <- svm(LeadStatus~., kernel = "linear",data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(svmfit)
```

```{r plot-svm-model}
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
```
Points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. 

```{r svm-tune}
# # find optimal cost of misclassification
# tune.out <- tune(svm, LeadStatus~., kernel = "linear", data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)), probability=TRUE)
# # extract the best model
# summary(tune.out)
# (bestmod <- tune.out$best.model)
# summary(bestmod)
```

```{r svm-accuracy}
dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE)
table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3)

# accuracy rate
mean(dev.svm.pred == combined.dev$LeadStatus)

# error rate
mean(dev.svm.pred != combined.dev$LeadStatus)
```

```{r roc-curve-svm}
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy 
prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r write-svm-result}
test=E_df
test.svm.pred = predict(svmfit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/svm-prediction.csv",row.names=FALSE)
```

### Radial
```{r svm-radial}
set.seed(1)
svmfit <- svm(LeadStatus~., kernel = "radial",data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(svmfit)
```

```{r plot-svm-radial-model}
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
```
Points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. 

```{r svm-radial-tune}
# # find optimal cost of misclassification
# tune.out <- tune(svm, LeadStatus~., kernel = "radial", data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000),
# gamma=c(0.5,1,2,3,4)), probability=TRUE)
# # extract the best model
# summary(tune.out)
# (bestmod <- tune.out$best.model)
# summary(bestmod)
```

```{r svm-radial-accuracy}
dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE)
table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3)

# accuracy rate
mean(dev.svm.pred == combined.dev$LeadStatus)

# error rate
mean(dev.svm.pred != combined.dev$LeadStatus)
```

```{r roc-curve-svm-radial}
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy 
prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r write-svm-radial-result}
test=E_df
test.svm.pred = predict(svmfit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/svm-radial-prediction.csv",row.names=FALSE)
```

### Polynomial
```{r svm-poly}
set.seed(1)
svmfit <- svm(LeadStatus~., kernel = "poly",degree=2,data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(svmfit)
```

```{r plot-svm-poly-model}
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
```
Points that are represented by an “X” are the support vectors, or the points that directly affect the classification line.

```{r svm-poly-tune}
# # find optimal cost of misclassification
# tune.out <- tune(svm, LeadStatus~., kernel = "poly", degree=2, data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000),
# gamma=c(0.5,1,2,3,4)), probability=TRUE)
# # extract the best model
# summary(tune.out)
# (bestmod <- tune.out$best.model)
# summary(bestmod)
```

```{r svm-poly-accuracy}
dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE)
table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3)

# accuracy rate
mean(dev.svm.pred == combined.dev$LeadStatus)

# error rate
mean(dev.svm.pred != combined.dev$LeadStatus)
```

```{r roc-curve-svm-poly}
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(probabilities[,2], combined.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r write-svm-poly-result}
test=E_df
test.svm.pred = predict(svmfit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction
write.csv(test,"Predictions/svm-poly2-prediction.csv",row.names=FALSE)
```


## Gradient Boosting 
```{r h2o}
h2o.no_progress()
h2o.init(max_mem_size="4g")
Y.train="LeadStatus"
X.train=setdiff(names(combined.train),c(Y.train,"Country"))
combined.train.h2o.gbm=as.h2o(combined.train)
combined.dev.h2o.gbm=as.h2o(combined.dev)
h2o.gbm.fit1=h2o.gbm(x=X.train,y=Y.train,training_frame=combined.train.h2o.gbm,nfolds=6)
h2o.gbm.fit1

#define function to easily plot ROC curve each time
getROC.h2o<-function(h2o.fit,dev.h2o.data){
  pfm.gbm.dev=h2o.performance(h2o.fit,newdata=dev.h2o.data)
logloss.dev=h2o.logloss(pfm.gbm.dev)
auc.dev <- h2o.auc(pfm.gbm.dev)
fpr.dev <- h2o.fpr(pfm.gbm.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(pfm.gbm.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
  ggplot(aes(fpr, tpr) ) +
  geom_line() +
  ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )

}

#plot ROC curve
getROC.h2o(h2o.gbm.fit1,combined.dev.h2o.gbm)

```
The development logloss from a default GBM model with 6-fold CV is 0.49 - not bad. The default model has 50 trees. We can train for more trees (say up to 1000):
```{r more trees gbm}
h2o.gbm.fit2=h2o.gbm(x=X.train,y=Y.train,training_frame=combined.train.h2o.gbm,nfolds=6,ntrees=1000,stopping_rounds=10,stopping_tolerance=0,seed=123)
h2o.gbm.fit2

getROC.h2o(h2o.gbm.fit2,combined.dev.h2o.gbm)
```
Now the logloss gets slightly better. 
Let's try tuning the parameters: ntrees, max_depth, min_rows, learn_rate, learn_rate_annealing, sample_rate, col_sample_rate. To speed this up, we'll use the development set for validation: 

```{r tuning}
hyper_grid=list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)
search_criteria=list(
  strategy="RandomDiscrete",
  stopping_metric="logloss",
  stopping_tolerance=0.005,
  stopping_rounds=10,
  max_runtime_secs=60*60*5
)

grid=h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid_080819midight",
  x = X.train, 
  y = Y.train, 
  training_frame = combined.train.h2o.gbm,
  validation_frame = combined.dev.h2o.gbm,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )
grid_perf=h2o.getGrid(grid_id="gbm_grid_080819midight",sort_by="logloss",decreasing=FALSE)
best_model_id=grid_perf@model_ids[[1]]
best_model.080819midnight=h2o.getModel(best_model_id)
getROC.h2o(best_model.080819midnight,combined.dev.h2o.gbm)
```
We get a marginal worsening in logloss. Note that I only tuned for maximum of 1 minute here. Let's predict the Kaggle test set:
```{r test}

# predict values with predict , make a function to use easily every time
getPredict=function(model.h2o,test_df,file.output){
  test.h2o=test_df %>%
  mutate_if(is.factor,factor,ordered=FALSE)%>%
  as.h2o()

pred=h2o.predict(model.h2o,test.h2o)
pred_df=as.data.frame(pred)
test_df$LeadStatus=pred_df$Flagged
test.output=test_df[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,file.output,row.names=FALSE)
}
getPredict(best_model.080819midnight,test,"Predictions/test.gbm.csv")
```

Maybe including all the individual scores would help. 
```{r include scores}
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
combined.all = rbind(A_df,B_df,C_df,D_df)
combined.all=subset(combined.all,select=setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all=distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
str(combined.all) # compactly display structure of the object 

E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
str(E_df)
combined_df$LeadStatus[combined_df$LeadStatus!="Passed"]<-"Flagged"
combined_df$LeadStatus=factor(combined_df$LeadStatus)
table(combined_df$LeadStatus)

set.seed(1)
tot=1:dim(combined.all)[1] # total number of observations 

train=sample(tot,length(tot)*0.7) # put 70% of observations into training set 
combined.train.all=combined.all[train,]
head(combined.train.all)

dev=tot[-train] # rest go into development set 
combined.dev.all=combined.all[dev,]
test.all=E_df # study E is the test set 
```

```{r gbm with all}
Y.train="LeadStatus"
X.train=setdiff(names(combined.train.all),c(Y.train,"Country"))
combined.train.h2o.gbm=as.h2o(combined.train.all)
combined.dev.h2o.gbm=as.h2o(combined.dev.all)
search_criteria=list(
  strategy="RandomDiscrete",
  stopping_metric="logloss",
  stopping_tolerance=0.005,
  stopping_rounds=10,
  max_runtime_secs=60
)
grid=h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid_all",
  x = X.train, 
  y = Y.train, 
  training_frame = combined.train.h2o.gbm,
  validation_frame = combined.dev.h2o.gbm,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )
grid_perf=h2o.getGrid(grid_id="gbm_grid_all",sort_by="logloss",decreasing=FALSE)
best_model_id=grid_perf@model_ids[[1]]
best_model=h2o.getModel(best_model_id)

temp=h2o.gbm(x=X.train,y=Y.train,training_frame=combined.train.h2o.gbm,nfolds=6,ntrees=1000,stopping_rounds=10,stopping_tolerance=0,seed=123)

#getROC.h2o(temp,combined.dev.h2o.gbm)
temp_pm=h2o.performance(temp,newdata = combined.dev.h2o.gbm)
h2o.logloss(temp_pm)
```
With all the data, the log-loss is much worse: 0.54 now. 



