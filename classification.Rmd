---
title: "Classification"
author: "Jeremy Binagia and Sai Gourisankar"
date: "7/5/2019"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notes/Assumptions
### Model 
  - LeadStatus is the response 
  - likely predictors include
    - Country 
    - TxGroup
    - VisitDay
    - SiteID
    - RaterID
    - PANSS_Total 
    - PatientID
  - we will keep AssessmentID in our dataframe as a "key" for each row/patient. 
  - exclude
    - individual scores: hard to see how to interpret if they impact the assessment status. We'll have total score anyway. Sometimes we will use individual scores for predictors at which point we will exclude the total PANSS score. 
    - Study: no observation in study E contains the response (LeadStatus). 
  - when it comes time to predict, we specify a assessment ID and get a probability of LeadStatus being NOT Passed (so Flagged or Assigned to CS)
  
### Data
  - We'll combine A-D into one dataset
  - The training dataset will be 70% randomly chosen assessment IDs from that combined dataset
  - The development dataset will be the other 30% of that.
  - The test dataset for kaggle will be study E, for which we have no LeadStatus. 
  
### Notes
  - random forests vs gbm
    - https://stats.stackexchange.com/questions/173390/gradient-boosting-tree-vs-random-forest
    - https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80
    - http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/
    - random forests easier to tune but usually on par if not slightly worse than gbm

## Thoughts/Conclusions
- we started off with Naive Bayes, and our test logloss was middle of the Kaggle board (70)
- then logistic, then LDA which as expected did similar, QDA was worse, as expected - so maybe linear decision boundary. 
  - decreasing bias by assuming different variances for each predictor in QDA wasn't a good tradeoff with increasing variance. 
  - both were better than naive bayes. 
- then we tried SVM, which didn't do better than logistic even with extensive tuning, 
- then gbm and rf which also did not perform well. BUT gbm and SVM had lower development set loglosses, so we'll submit anyway.
- perhaps they didn't they do as well because decision boundary truly is linear, which is why logistic regression worked so well. 




## Setup
```{r Load Libraries, results="hide"}
rm(list = ls()) # clear global environment 
library(dplyr)
library(ggplot2)
```

```{r Load Data}
A_init_df=read.csv("Data/Study_A.csv")
B_init_df=read.csv("Data/Study_B.csv")
C_init_df=read.csv("Data/Study_C.csv")
D_init_df=read.csv("Data/Study_D.csv")
E_init_df=read.csv("Data/Study_E.csv")
#summary(E_init_df)
names(E_init_df)
dim(E_init_df)[1]
```

The following creates a list of the assessments we should consider for the Kaggle submission. This should be the same as Study E. 
```{r sample submission}
sample_submission_df = read.csv("Data/sample_submission_status.csv")
prediction.ids = sample_submission_df$AssessmentID # the AssessmentID #s we should use for Kaggle submission 
length(unique(prediction.ids)) 
length(prediction.ids)
all(E_init_df$AssessmentiD==prediction.ids)
```
This is the same length as Study E. All the values are in both. 

### Data Cleaning
Remove columns not corresponding to our predictors and response.

```{r subset-df}
A_df = subset(A_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
B_df = subset(B_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
C_df = subset(C_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
D_df = subset(D_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
names(E_init_df)
E_df = subset(E_init_df, select = c(Country,TxGroup,VisitDay,PANSS_Total,AssessmentiD))
names(E_df)
```

Combine the studies into one dataframe. 
```{r combine-studies}
combined_df = rbind(A_df,B_df,C_df,D_df)
summary(combined_df)
names(combined_df)
```
Get rid of any duplicates, by AssessmentiD. 
```{r remove duplicates}
dim(combined_df)[1]
combined_df=distinct(combined_df)
dim(combined_df)[1]
```
We also need to make the ID numbers factors, not numeric data. Indeed, everything but VisitDay and PANSS_Total should be factorized. 
```{r factor}
combined_df <- mutate_at(combined_df, vars(Country, TxGroup,LeadStatus), as.factor)
str(combined_df) # compactly display structure of the object 

E_df<-mutate_at(E_df,vars(Country, TxGroup,AssessmentiD),as.factor)
str(E_df)
```

And since we just want probability of flagged OR assigned to CS, turn LeadStatus into a binary classification of Passed or Flagged. 
```{r reclassify}
combined_df$LeadStatus[combined_df$LeadStatus!="Passed"]<-"Flagged"
combined_df$LeadStatus=factor(combined_df$LeadStatus)
table(combined_df$LeadStatus)
```


Split data into training, development, and test sets. What we mean by test set here is just Study E, where we have no response whatsoever. 
```{r split data}
set.seed(1)
tot=1:dim(combined_df)[1] # total number of observations 

train=sample(tot,length(tot)*0.7) # put 70% of observations into training set 
combined.train=combined_df[train,]
head(combined.train)

dev=tot[-train] # rest go into development set 
combined.dev=combined_df[dev,]
test=E_df # study E is the test set 
```

## Naive Bayes
First, try a Naive Bayes Classifier. To do this, we have to assume each predictor (PatientID, Country, TxGroup, VisitDay,Study,PANSS_Total,SiteID,RaterID,AssesmentiD) is independent. We also have to get rid of VisitDay as a predictor since Naive Bayes assumes a normal distribution for all quantitative predictors, which would make little sense here. Finally, to predict on Study E, we have to get rid of all the IDs not present in Study E (the IDs unique to other studies)
```{r naive bayes classifier}
library(dplyr)
library(h2o)
library(caret)
library(corrplot)
Y.train="LeadStatus"
X.train=setdiff(names(combined.train),c(Y.train,"VisitDay"))

# h2o.no_progress()
h2o.init()

combined.train.h2o <- combined.train %>%
  mutate_if(is.factor, factor, ordered = FALSE) %>%
  as.h2o()
str(combined.train.h2o)
train.nb <- h2o.naiveBayes(
  x = X.train,
  y = Y.train,
  training_frame = combined.train.h2o,
  nfolds = 10,
  laplace = 0
)

# assess results on training data
cM.nb=h2o.confusionMatrix(train.nb)
accuracy.nb=(cM.nb[1,1]+cM.nb[2,2])/(cM.nb[3,1]+cM.nb[3,2])
print(cM.nb)
print(paste("Training accuracy: =",accuracy.nb))


# ROC curve on the development data
names(combined.dev)
combined.dev.h2o=combined.dev[,-3]#get rid of VisitDay
names(combined.dev.h2o)
combined.dev.h2o=combined.dev.h2o %>%
  mutate_if(is.factor,factor,ordered=FALSE) %>%
  as.h2o()

performance.train=h2o.performance(train.nb,xval=TRUE)
performance.dev=h2o.performance(train.nb,newdata=combined.dev.h2o)

logloss.train = h2o.logloss(performance.train,xval=TRUE)
logloss.dev=h2o.logloss(performance.dev,xval=TRUE)
auc.train <- h2o.auc(performance.train,xval=TRUE)
auc.dev <- h2o.auc(performance.dev)
fpr.dev <- h2o.fpr(performance.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(performance.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
  ggplot(aes(fpr, tpr) ) +
  geom_line() +
  ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )

# predict values with predict -->
names(test)
test.h2o=test[,-3]#get rid of VisitDay
#test.h2o=test.h2o[,-4]#get rid of AssessmentiD
test.h2o=test.h2o %>%
  mutate_if(is.factor,factor,ordered=FALSE)%>%
  as.h2o()
str(test.h2o)

nb.predictions=h2o.predict(train.nb,test.h2o)
nb.predictions_df=as.data.frame(nb.predictions)
test$LeadStatus=nb.predictions_df$Flagged
test.output=test[,c("AssessmentiD","LeadStatus")]
write.csv(test.output,"test.nb.csv",row.names=FALSE)

```

The naive classifier doesn't do great on our holdout test data. The total development AUC is $0.77$ but the false and true positive rates are high.

## Logistic 
### Training - validation split 
We might try a logistic regression. However, Study E has no country of UK, so we take out the country as a predictor. 
```{r logistic}
library(pROC)
names(combined.train)
combined.train.glm = combined.train[,-1] # exclude country from being a predictor 
attach(combined.train.glm)
names(combined.train.glm)
train.glm = glm(LeadStatus~.,data = combined.train.glm,family = binomial)
summary(train.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down

#dev test
glm.probs.flag.dev = 1-predict(train.glm,combined.dev,type = "response")
glm.pred = rep("Passed",dim(combined.dev)[1])#create vector of predictions of length the same as dev dataset
glm.pred[glm.probs.flag.dev>0.2] = "Flagged"#change relevant values to "Flagged" based on model-predicted value.
table(glm.pred,combined.dev$LeadStatus)
roc.dev = roc(LeadStatus~glm.probs.flag.dev,data = combined.dev)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))

#kaggle test
test = E_df
glm.probs = predict(train.glm,test,type = "response")#compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag = 1-glm.probs#probabililty of being flagged for all.
names(test)
test.output.glm = as.data.frame(test$AssessmentiD)
test.output.glm$LeadStatus = glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names = FALSE)

```
It looks like PANSS_Total, VisitDay, Study, TxGroup, and some countries are statistically significant predictors of Passed vs Flagged. 

```{r similarity}
plot(test.output.glm$LeadStatus,nb.predictions_df$Flagged,xlim = c(0,0.5),ylim = c(0,0.5))
abline(0,1)
```

### Full training set 
```{r full-logistic}
full.glm = glm(LeadStatus~.,data = combined_df[,-1],family = binomial)
summary(full.glm)
contrasts(LeadStatus)#check dummy encoding for Up/Down
```

```{r full-log-test-set}
test = E_df
glm.probs = predict(full.glm,test,type = "response") #compute predictions based on fit for each observation; 1 corresponds to Passed
glm.probs.flag = 1 - glm.probs#probabililty of being flagged for all.
test.full.glm = as.data.frame(test$AssessmentiD)
test.full.glm$LeadStatus = glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentiD"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names = FALSE)
```

```{r similarity-of-training-and-full}
plot(test.output.glm$LeadStatus,test.full.glm$LeadStatus)
abline(0,1)
```
### Logistic regression (all individual scores + lasso)
```{r include scores}
# create dataframe that has all individual PANSS scores
combined.all = rbind(A_init_df, B_init_df, C_init_df, D_init_df)
combined.all = subset(combined.all,select = setdiff(names(combined.all),c("Country","Study","PatientID","RaterID","AssessmentiD","PANSS_Total","SiteID")))
names(combined.all)
combined.all = distinct(combined.all)
combined.all <- mutate_at(combined.all, vars(TxGroup,LeadStatus), as.factor)
# str(combined.all) # compactly display structure of the object 


# fix up LeadStatus column for the purposes of part 4 
combined.all$LeadStatus[combined.all$LeadStatus!="Passed"]<-"Flagged"
combined.all$LeadStatus = factor(combined.all$LeadStatus)
table(combined.all$LeadStatus) # how many passed vs. not 


# split into training and dev set
set.seed(1)
tot = 1:dim(combined.all)[1] # total number of observations 

train = sample(tot,length(tot)*0.7) # put 70% of observations into training set 
combined.train.all = combined.all[train,]
head(combined.train.all) # visually check data frame 

dev = tot[-train] # rest go into development set 
combined.dev.all = combined.all[dev,]


# create test set that has all individual scores
E_df_tmp = subset(E_init_df,select = setdiff(names(E_init_df),c("Country","Study","PatientID","RaterID","PANSS_Total","SiteID")))
E_df_tmp = mutate_at(E_df_tmp,vars(TxGroup,AssessmentiD),as.factor)
test.all = E_df_tmp # study E is the test set 
```

```{r logistic-all-predictors}
train.all.glm <- glm(LeadStatus ~., family = "binomial", data = combined.train.all)
summary(train.all.glm)
```

```{r more-log-all-pred-evaluation}
library(broom)      # helps to tidy up model outputs
tidy(train.all.glm)
caret::varImp(train.all.glm)

#dev test
glm.probs.flag.dev = 1 - predict(train.all.glm, combined.dev.all, type = "response")
glm.pred = rep("Passed", dim(combined.dev.all)[1])
glm.pred[glm.probs.flag.dev > 0.2] = "Flagged"
table(glm.pred,combined.dev.all$LeadStatus)
roc.dev = roc(LeadStatus~glm.probs.flag.dev,data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))

#kaggle test
test = test.all
glm.probs = predict(train.all.glm, test, type = "response")
glm.probs.flag = 1 - glm.probs
names(test)
test.all.glm = as.data.frame(test$AssessmentiD)
test.all.glm$LeadStatus = glm.probs.flag
colnames(test.all.glm)[colnames(test.all.glm)=="test$AssessmentiD"] <- "AssessmentID"
write.csv(test.all.glm,"Predictions/test.all.glm.csv",row.names = FALSE)

# compare to original logistic regression
plot(test.output.glm$LeadStatus,test.all.glm$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
```

### Logistic with lasso
```{r log-lasso}
library(glmnet)

#convert training data to matrix format
x <- model.matrix(LeadStatus~., combined.train.all)

#convert class to numerical variable
y <- ifelse(combined.train.all$LeadStatus=="Passed",0,1)

#perform grid search to find optimal value of lambda
#family= binomial => logistic regression, alpha=1 => lasso
# check docs to explore other type.measure options
cv.out <- cv.glmnet(x,y, alpha=1, family="binomial",type.measure="auc")
#plot result
plot(cv.out)
```
```{r find-optimal-lambda}
#min value of lambda
lambda_min <- cv.out$lambda.min
#best value of lambda
lambda_1se <- cv.out$lambda.1se
#regression coefficients
coef(cv.out,s=lambda_1se)
```

```{r lasso-performance}
#get dev set data
x_test1 <- model.matrix(LeadStatus~., combined.dev.all)

#predict class, type=”class”
lasso_prob <- predict(cv.out, newx = x_test1, s=lambda_1se, type="response")

roc.dev = roc(LeadStatus~lasso_prob, data = combined.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))

#kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentiD")])
lasso_prob <- predict(cv.out, newx = x_test2, s=lambda_1se, type="response")
test.all$LeadStatus = lasso_prob
test.lasso = subset(test.all, select = c(AssessmentiD,LeadStatus))
write.csv(test.lasso,"Predictions/test.lasso.glm.csv",row.names = FALSE)

# compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)
```

## LDA
### Training - validation split 
Study E has no country of UK, so we take out the country as a predictor. 
```{r lda-start}
library(MASS)       # provides LDA & QDA model functions
(lda.m1 = lda(LeadStatus~., data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(lda.m1)
```

We can now use our LDA model to make a prediction on the development set
```{r lda-predict}
dev.lda.pred = predict(lda.m1, newdata = combined.dev)
table(combined.dev$LeadStatus, dev.lda.pred$class) %>% prop.table() %>% round(3)

# accuracy rate
mean(dev.lda.pred$class == combined.dev$LeadStatus)

# error rate
mean(dev.lda.pred$class != combined.dev$LeadStatus)
```

```{r roc-curve}
# ROC curves
library(ROCR)
prediction(dev.lda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(dev.lda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy 
prediction(dev.lda.pred$posterior[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r write-lda-result}
test=E_df
test.lda.pred = predict(lda.m1, newdata = test)
test$LeadStatus = test.lda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/lda-prediction.csv",row.names=FALSE)
```
### Use full training set  
```{r lda-full}
(lda.full = lda(LeadStatus~., data = combined_df[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(lda.full)
```

```{r write-lda-full-result}
test=E_df
test.lda.pred = predict(lda.full, newdata = test)
test$LeadStatus = test.lda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/lda-full-prediction.csv",row.names=FALSE)
```




### Use all predictors

## QDA
```{r qda-start}
(qda.m1 = qda(LeadStatus~., data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
```

We can now use our QDA model to make a prediction on the development set
```{r qda-predict}
dev.qda.pred = predict(qda.m1, newdata = combined.dev)
table(combined.dev$LeadStatus, dev.qda.pred$class) %>% prop.table() %>% round(3)

# accuracy rate
mean(dev.qda.pred$class == combined.dev$LeadStatus)

# error rate
mean(dev.qda.pred$class != combined.dev$LeadStatus)
```
```{r roc-curve-qda}
# ROC curves
library(ROCR)
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(dev.qda.pred$posterior[,2], combined.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy 
prediction(dev.qda.pred$posterior[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r write-qda-result}
test=E_df
test.qda.pred = predict(qda.m1, newdata = test)
test$LeadStatus = test.qda.pred$posterior[,1]
test = test[,c("AssessmentiD","LeadStatus")]

# compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# write for kaggle prediction 
write.csv(test,"Predictions/qda-prediction.csv",row.names=FALSE)
```

## SVM
<!-- ### Linear -->
<!-- ```{r svm-intro232213213} -->
<!-- library(e1071)        # SVM methodology -->
<!-- set.seed(1) -->

<!-- svmfit <- svm(LeadStatus~., kernel = "linear",data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE) -->
<!-- summary(svmfit) -->
<!-- ``` -->

<!-- ```{r plot-svm-model12321} -->
<!-- plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE) -->
<!-- plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE) -->
<!-- ``` -->
<!-- Points that are represented by an “X” are the support vectors, or the points that directly affect the classification line.  -->

<!-- ```{r svm-tune123213} -->
<!-- # # find optimal cost of misclassification -->
<!-- # tune.out <- tune(svm, LeadStatus~., kernel = "linear", data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)), probability=TRUE) -->
<!-- # # extract the best model -->
<!-- # summary(tune.out) -->
<!-- # (bestmod <- tune.out$best.model) -->
<!-- # summary(bestmod) -->
<!-- ``` -->

<!-- ```{r svm-accuracy123213} -->
<!-- dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE) -->
<!-- table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3) -->

<!-- # accuracy rate -->
<!-- mean(dev.svm.pred == combined.dev$LeadStatus) -->

<!-- # error rate -->
<!-- mean(dev.svm.pred != combined.dev$LeadStatus) -->
<!-- ``` -->

<!-- ```{r roc-curve-svm123213} -->
<!-- # ROC curves -->
<!-- probabilities = attr(dev.svm.pred, "probabilities") -->
<!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% -->
<!--   performance(measure = "tpr", x.measure = "fpr") %>% -->
<!--   plot() -->

<!-- # AUC -->
<!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% -->
<!--   performance(measure = "auc") %>% -->
<!--   .@y.values -->

<!-- # Cross-entropy  -->
<!-- prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>% -->
<!--   performance(measure = "mxe") %>% -->
<!--   .@y.values -->
<!-- ``` -->

<!-- ```{r write-svm-result123213} -->
<!-- test=E_df -->
<!-- test.svm.pred = predict(svmfit, newdata = test, probability=TRUE) -->
<!-- probabilities = attr(test.svm.pred, "probabilities") -->
<!-- test$LeadStatus = probabilities[,1] -->
<!-- test = test[,c("AssessmentiD","LeadStatus")] -->

<!-- # compare to logistic regression -->
<!-- plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5)) -->
<!-- abline(0,1) -->

<!-- # write for kaggle prediction  -->
<!-- write.csv(test,"Predictions/svm-prediction.csv",row.names=FALSE) -->
<!-- ``` -->

<!-- ### Linear (all PANSS scores) -->
<!-- <!-- ```{r svm-intro} --> -->
<!-- <!-- library(e1071)        # SVM methodology --> -->
<!-- <!-- set.seed(1) --> -->
<!-- <!-- svmfit <- svm(LeadStatus~., kernel = "linear",data = combined.train.all, scale=TRUE, probability=TRUE) --> -->
<!-- <!-- summary(svmfit) --> -->
<!-- <!-- ``` --> -->
<!-- <!-- This is quite a lengthy computation for svm. Doesn't complete within 10 minutes on my desktop computer.  --> -->

<!-- <!-- ```{r svm-tune} --> -->
<!-- <!-- # # find optimal cost of misclassification --> -->
<!-- <!-- # tune.out <- tune(svm, LeadStatus~., kernel = "linear", data = combined.train.all, ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)), probability=TRUE) --> -->
<!-- <!-- # # extract the best model --> -->
<!-- <!-- # summary(tune.out) --> -->
<!-- <!-- # (bestmod <- tune.out$best.model) --> -->
<!-- <!-- # summary(bestmod) --> -->
<!-- <!-- ``` --> -->

<!-- <!-- ```{r svm-accuracy} --> -->
<!-- <!-- dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE) --> -->
<!-- <!-- table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3) --> -->

<!-- <!-- # accuracy rate --> -->
<!-- <!-- mean(dev.svm.pred == combined.dev$LeadStatus) --> -->

<!-- <!-- # error rate --> -->
<!-- <!-- mean(dev.svm.pred != combined.dev$LeadStatus) --> -->
<!-- <!-- ``` --> -->

<!-- <!-- ```{r roc-curve-svm} --> -->
<!-- <!-- # ROC curves --> -->
<!-- <!-- probabilities = attr(dev.svm.pred, "probabilities") --> -->
<!-- <!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% --> -->
<!-- <!--   performance(measure = "tpr", x.measure = "fpr") %>% --> -->
<!-- <!--   plot() --> -->

<!-- <!-- # AUC --> -->
<!-- <!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% --> -->
<!-- <!--   performance(measure = "auc") %>% --> -->
<!-- <!--   .@y.values --> -->

<!-- <!-- # Cross-entropy  --> -->
<!-- <!-- prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>% --> -->
<!-- <!--   performance(measure = "mxe") %>% --> -->
<!-- <!--   .@y.values --> -->
<!-- <!-- ``` --> -->

<!-- <!-- ```{r write-svm-result} --> -->
<!-- <!-- test=E_df --> -->
<!-- <!-- test.svm.pred = predict(svmfit, newdata = test, probability=TRUE) --> -->
<!-- <!-- probabilities = attr(test.svm.pred, "probabilities") --> -->
<!-- <!-- test$LeadStatus = probabilities[,1] --> -->
<!-- <!-- test = test[,c("AssessmentiD","LeadStatus")] --> -->

<!-- <!-- # compare to logistic regression --> -->
<!-- <!-- plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5)) --> -->
<!-- <!-- abline(0,1) --> -->

<!-- <!-- # write for kaggle prediction  --> -->
<!-- <!-- write.csv(test,"Predictions/svm-all-pred-prediction.csv",row.names=FALSE) --> -->
<!-- <!-- ``` --> -->


<!-- ### Radial -->
<!-- ```{r svm-radial} -->
<!-- set.seed(1) -->
<!-- svmfit <- svm(LeadStatus~., kernel = "radial",data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE) -->
<!-- summary(svmfit) -->
<!-- ``` -->

<!-- ```{r plot-svm-radial-model} -->
<!-- plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE) -->
<!-- plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE) -->
<!-- ``` -->
<!-- Points that are represented by an “X” are the support vectors, or the points that directly affect the classification line.  -->

<!-- ```{r svm-radial-tune} -->
<!-- # # find optimal cost of misclassification -->
<!-- # tune.out <- tune(svm, LeadStatus~., kernel = "radial", data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000), -->
<!-- # gamma=c(0.5,1,2,3,4)), probability=TRUE) -->
<!-- # # extract the best model -->
<!-- # summary(tune.out) -->
<!-- # (bestmod <- tune.out$best.model) -->
<!-- # summary(bestmod) -->
<!-- ``` -->

<!-- ```{r svm-radial-accuracy} -->
<!-- dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE) -->
<!-- table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3) -->

<!-- # accuracy rate -->
<!-- mean(dev.svm.pred == combined.dev$LeadStatus) -->

<!-- # error rate -->
<!-- mean(dev.svm.pred != combined.dev$LeadStatus) -->
<!-- ``` -->

<!-- ```{r roc-curve-svm-radial} -->
<!-- # ROC curves -->
<!-- probabilities = attr(dev.svm.pred, "probabilities") -->
<!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% -->
<!--   performance(measure = "tpr", x.measure = "fpr") %>% -->
<!--   plot() -->

<!-- # AUC -->
<!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% -->
<!--   performance(measure = "auc") %>% -->
<!--   .@y.values -->

<!-- # Cross-entropy  -->
<!-- prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>% -->
<!--   performance(measure = "mxe") %>% -->
<!--   .@y.values -->
<!-- ``` -->

<!-- ```{r write-svm-radial-result} -->
<!-- test=E_df -->
<!-- test.svm.pred = predict(svmfit, newdata = test, probability=TRUE) -->
<!-- probabilities = attr(test.svm.pred, "probabilities") -->
<!-- test$LeadStatus = probabilities[,1] -->
<!-- test = test[,c("AssessmentiD","LeadStatus")] -->

<!-- # compare to logistic regression -->
<!-- plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5)) -->
<!-- abline(0,1) -->

<!-- # write for kaggle prediction  -->
<!-- write.csv(test,"Predictions/svm-radial-prediction.csv",row.names=FALSE) -->
<!-- ``` -->

<!-- ### Polynomial -->
<!-- ```{r svm-poly} -->
<!-- set.seed(1) -->
<!-- svmfit <- svm(LeadStatus~., kernel = "poly",degree=2,data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE) -->
<!-- summary(svmfit) -->
<!-- ``` -->

<!-- ```{r plot-svm-poly-model} -->
<!-- plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE) -->
<!-- plot(svmfit,combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE) -->
<!-- ``` -->
<!-- Points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. -->

<!-- ```{r svm-poly-tune} -->
<!-- # # find optimal cost of misclassification -->
<!-- # tune.out <- tune(svm, LeadStatus~., kernel = "poly", degree=2, data = combined.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000), -->
<!-- # gamma=c(0.5,1,2,3,4)), probability=TRUE) -->
<!-- # # extract the best model -->
<!-- # summary(tune.out) -->
<!-- # (bestmod <- tune.out$best.model) -->
<!-- # summary(bestmod) -->
<!-- ``` -->

<!-- ```{r svm-poly-accuracy} -->
<!-- dev.svm.pred = predict(svmfit,newdata = combined.dev,probability=TRUE) -->
<!-- table(combined.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(3) -->

<!-- # accuracy rate -->
<!-- mean(dev.svm.pred == combined.dev$LeadStatus) -->

<!-- # error rate -->
<!-- mean(dev.svm.pred != combined.dev$LeadStatus) -->
<!-- ``` -->

<!-- ```{r roc-curve-svm-poly} -->
<!-- # ROC curves -->
<!-- probabilities = attr(dev.svm.pred, "probabilities") -->
<!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% -->
<!--   performance(measure = "tpr", x.measure = "fpr") %>% -->
<!--   plot() -->

<!-- # AUC -->
<!-- prediction(probabilities[,2], combined.dev$LeadStatus) %>% -->
<!--   performance(measure = "auc") %>% -->
<!--   .@y.values -->

<!-- # Cross-entropy -->
<!-- prediction(probabilities[,2], ifelse(combined.dev$LeadStatus=="Passed", 1, 0)) %>% -->
<!--   performance(measure = "mxe") %>% -->
<!--   .@y.values -->
<!-- ``` -->

<!-- ```{r write-svm-poly-result} -->
<!-- test=E_df -->
<!-- test.svm.pred = predict(svmfit, newdata = test, probability=TRUE) -->
<!-- probabilities = attr(test.svm.pred, "probabilities") -->
<!-- test$LeadStatus = probabilities[,1] -->
<!-- test = test[,c("AssessmentiD","LeadStatus")] -->

<!-- # compare to logistic regression -->
<!-- plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5)) -->
<!-- abline(0,1) -->

<!-- # write for kaggle prediction -->
<!-- write.csv(test,"Predictions/svm-poly2-prediction.csv",row.names=FALSE) -->
<!-- ``` -->


## Gradient Boosting
```{r h2o}
#h2o.no_progress()
h2o.removeAll()
h2o.init(max_mem_size="6g")
Y.train = "LeadStatus"
X.train = setdiff(names(combined.train),c(Y.train,"Country"))
combined.train.h2o.gbm = as.h2o(combined.train)
combined.dev.h2o.gbm = as.h2o(combined.dev)
h2o.gbm.fit1 = h2o.gbm(x = X.train,y = Y.train,training_frame = combined.train.h2o.gbm, nfolds = 6)
h2o.gbm.fit1

#define function to easily plot ROC curve each time
getROC.h2o<-function(h2o.fit,dev.h2o.data){
  pfm.gbm.dev = h2o.performance(h2o.fit,newdata = dev.h2o.data)
logloss.dev = h2o.logloss(pfm.gbm.dev)
auc.dev <- h2o.auc(pfm.gbm.dev)
fpr.dev <- h2o.fpr(pfm.gbm.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(pfm.gbm.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
  ggplot(aes(fpr, tpr) ) +
  geom_line() +
  ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )

}

#plot ROC curve
getROC.h2o(h2o.gbm.fit1,combined.dev.h2o.gbm)
```
The development logloss from a default GBM model with 6-fold CV is 0.49 - not bad. The default model has 50 trees. We can train for more trees (say up to 1000):
```{r more trees gbm}
h2o.gbm.fit2 = h2o.gbm(x=X.train,y=Y.train,training_frame=combined.train.h2o.gbm,nfolds=6,ntrees=1000,stopping_rounds=10,stopping_tolerance=0,seed=123)

h2o.gbm.fit2
getROC.h2o(h2o.gbm.fit2,combined.dev.h2o.gbm)
```
Now the logloss gets slightly better.

Let's try tuning the parameters: ntrees, max_depth, min_rows, learn_rate, learn_rate_annealing, sample_rate, col_sample_rate. To speed this up, we'll use the development set for validation:
```{r tuning}
hyper_grid=list(
  max_depth = c(3, 4, 5), # 2 < and < 6
  min_rows = c(10, 20, 30, 40),
  learn_rate = c(0.0025, 0.005, 0.01, 0.05), # > 0.001 and < 0.1
  learn_rate_annealing = c(1), # 1 is best
  sample_rate = c(.65, .7,0.75,.8,.85), # > 0.6 and < 0.9
  col_sample_rate = c(0.6, 0.7,.8, .9) # > 0.6 and < 1
)

search_criteria=list(
  strategy="RandomDiscrete",
  stopping_metric="logloss",
  stopping_tolerance=0.005,
  stopping_rounds=10,
  max_runtime_secs=60*1
)

grid = h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_random_discrete",
  x = X.train,
  y = Y.train,
  training_frame = combined.train.h2o.gbm,
  validation_frame = combined.dev.h2o.gbm,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )

grid_perf= h2o.getGrid(grid_id="gbm_random_discrete",sort_by="logloss",decreasing=FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model.random.discrete = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model.random.discrete,combined.dev.h2o.gbm)
```

```{r evaluate-gbm}
# train final model
h2o.final <- h2o.gbm(
  x = X.train,
  y = Y.train,
  training_frame = combined.train.h2o.gbm,
  ntrees = 20000,
  learn_rate = 0.01,
  learn_rate_annealing = 1,
  max_depth = 4,
  min_rows = 30,
  sample_rate = 0.75,
  col_sample_rate = 0.9,
  stopping_rounds = 10,
  seed = 1
)

# model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final, num_of_features = 5)
```

Let's predict the Kaggle test set:
```{r test}
# predict values with predict , make a function to use easily every time
getPredict = function(model.h2o,test_df,file.output){
  test.h2o = test_df %>%
  mutate_if(is.factor,factor,ordered = FALSE)%>%
  as.h2o()
  #str(test.h2o)

  pred = h2o.predict(model.h2o,test.h2o) #error!!!
  pred_df = as.data.frame(pred)
  test_df$LeadStatus = pred_df$Flagged
  test.output = test_df[,c("AssessmentiD","LeadStatus")]
  write.csv(test.output,file.output,row.names = FALSE)

  plot(test.output.glm$LeadStatus,test_df$LeadStatus,xlim = c(0,0.5),ylim = c(0,0.5))
  abline(0,1)
}
test=E_df # study E is the test set
getPredict(best_model.random.discrete,test,"Predictions/test.gbm.csv")
```


### GBM with all predictors
Maybe including all the individual scores would help. First create the h2o dataframes:
```{r h2o-dfs-gbm-all-pred}
Y.train = "LeadStatus"
X.train = setdiff(names(combined.train.all),c(Y.train,"Country")) # the predictors
combined.train.h2o.gbm = as.h2o(combined.train.all)
combined.dev.h2o.gbm = as.h2o(combined.dev.all)
```

Now we will again perform a random discrete grid search.
```{r gbm-all-pred-search}
# create hyperparameter grid
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)

search_criteria = list(
  strategy = "RandomDiscrete",
  stopping_metric = "logloss",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*1 # covers 36 models in 15 min
)

grid = h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid_all",
  x = X.train,
  y = Y.train,
  training_frame = combined.train.h2o.gbm,
  validation_frame = combined.dev.h2o.gbm,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )

grid_perf = h2o.getGrid(grid_id = "gbm_grid_all",sort_by = "logloss",decreasing = FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model,combined.dev.h2o.gbm)
```

```{r evaluate-gbm-all}
train.h2o <- as.h2o(combined.all)

# train final model with more trees and on full data
h2o.final <- h2o.gbm(
  x = X.train,
  y = Y.train,
  training_frame = train.h2o,
  ntrees = 10000,
  learn_rate = 0.01,
  learn_rate_annealing = 1,
  max_depth = 6,
  min_rows = 30,
  sample_rate = 0.8,
  col_sample_rate = 0.8,
  stopping_rounds = 10,
  seed = 1
)

# model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final)
```

```{r gbm-pred-all-predict}
getPredict(h2o.final,test.all,"Predictions/test.gbm.all.csv")
```



## Random forests 
```{r default-rf-model}
#library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest

# for reproduciblity
set.seed(1)

# default RF model
m1 <- ranger(
  formula = LeadStatus ~ .,
  data    = combined.train.all,
  probability = T
)

m1
```

```{r rf-error}
# prediction 
pred_randomForest <- predict(m1, test.all)
plot(test.output.glm$LeadStatus,pred_randomForest$predictions[,1],xlim=c(0,.5),ylim=c(0,.5))
abline(0,1)
```

### Tuning via h2o
```{r h2o-rf}
set.seed(1)

# hypergrid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 150),
  mtries      = c(15, 20, 25, 30),
  max_depth   = seq(20, 40, by = 5),
  min_rows    = seq(1, 5, by = 2),
  nbins       = seq(10, 30, by = 5),
  sample_rate = c(.55, .632, .75)
)

# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "logloss",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*60*5 # did about 50 in 15 min
  )

# build grid search
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = X.train, 
  y = Y.train, 
  training_frame = combined.train.h2o.gbm,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )

# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "logloss",
  decreasing = FALSE
  )

summary(grid_perf2)
```


```{r h2o-rf-evaluate}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now let’s evaluate the model performance on a test set
h2o.varimp_plot(best_model) 
getROC.h2o(best_model,combined.dev.h2o.gbm)

# train on full data set
train.h2o <- as.h2o(combined.all)
h2o.final <- h2o.randomForest(
  x = X.train,
  y = Y.train,
  training_frame = train.h2o,
  ntrees      = 500,
  mtries      = 15,
  max_depth   = 25,
  min_rows    = 3,
  nbins       = 25,
  sample_rate = 0.75,
  stopping_rounds = 10,
  seed = 1
)

# View prediction
getPredict(h2o.final,test.all,"Predictions/test.rf.all.csv")
```




