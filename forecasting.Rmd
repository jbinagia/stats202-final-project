---
title: "Forecasting"
author: "Jeremy Binagia and Sai Gourisankar"
date: "7/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notes/Assumptions
### General 
- predict "18th" week total PANSS score 
- i.e. create a csv file that contains the PatientID and the predicted 18th-week PANSS score
- can submit predictions here https://www.kaggle.com/c/stanford-stats-202-forecasting/data
- challenges
  - typically data in class ranges over time OR over individual. Here we have both. 
  - we can't average over individuals because we need distinct predictions for each individual given. 
  - we can't average over time because visitDay is likely a decently strong predictor for the level. 
- gradient descent reference http://uc-r.github.io/gbm_regression
  
### Model 
  - clearly PANSS total is the response 
  - likely predictors include
    - (maybe) study 
    - Country 
    - TxGroup
    - VisitDay
    - PatientID - this one is tricky. Technically setting a PatientID (I believe??) uniquely determines VisitDay, TxGroup, Country, and Study for our final prediction. HOWEVER, there could be a scenario where two patients completely align on all of these factors EXCEPT for the fact that they are different Patients (in which case including this predictor could be valuable). For example, perhaps patient 1 and patient 2 are identical in all ways except for over the course of the study the former consistently has lower scores. We want this information in our model. 
  - exclude
    - individual scores (if you had these, you could simply sum them to get the total score)
    - SiteID - we don't know where they will be evaluated 
    - RaterID - we don't know who will evaluate them (see for example https://groups.google.com/forum/#!searchin/stats202/model%7Csort:date/stats202/y7IxH2KYTk4/U-NfpWgGCQAJ)
  - when it comes time to predict, we specify a patient ID which then sets the above listed predictors except for VisitDay. *For the final predictions, let's assume VisitDay is one week after the given patient's last visit?*

### Splitting the data 
  - Following the suggested methodology found in slide 14 of the lecture 5 slides, we will split the data into 5 parts
    - test set: the 17th-week for the 379 patients in study E (since ultimately we will be evaluated on the 18th-week scores). Define "17"th-week as second-to-last visit here. We will use the test set to choose between different models. 
    - training set: half of the remaining observations, combined across datasets. 
    - development set: the other half of the remaining observations, combined across datasets.  
    - issues:
      - study E seems to be distinct in the segmentation plots from the other studies... could be a bias in that there is much more data from the other studies in our training set. 
  - following suggests combining studies A-D since they are essentially the same https://groups.google.com/forum/#!searchin/stats202/validation%7Csort:date/stats202/ULXRN3seaBE/5DFG-1LmEgAJs
  - suggests a 70/30 or 70/15/15 split https://www.researchgate.net/post/Is_there_an_ideal_ratio_between_a_training_set_and_validation_set_Which_trade-off_would_you_suggest
  - ultimately 10-fold CV is probably king for tuning hyper-parameters. 

## Thoughts/Conclusions

## Setup
```{r Load Libraries, results="hide"}
library(plyr)
library(ggplot2)
```

```{r Load Data}
rm(list = ls()) # clear global environment 
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
```
Note that most patients from study E are from USA or Russia (about an even split). 

We should remove true duplicates from the dataset. Note that assessment ID is a completely unique identifier (no two rows have the same assessment id from examing the raw data in Excel). 
```{r remove-true-duplicates}
# # check that there are in fact duplicates 
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
# 
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
# 
# # check disregarding assessment id 
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
# 
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
# 
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
```

The following creates a list of the patients we should consider for the Kaggle submission. 
```{r prediction-patients}
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission 
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values 
#n_distinct(prediction.patients)   # gives same result 
```

How many visits did each patient have?
```{r number-visits}
# number.visits = count(E_df, vars = "PatientID")
# 
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
```


## Data Cleaning
Remove columns not corresponding to our predictors and response (total PANSS score)
```{r subset-df}
A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
```

Combine studies
```{r combine-studies}
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
```

What was the final visit day for each patient? We need this info since our test set will the final observation for each of the 379 selected study E patients we will evaluate our models on. 
```{r final-visit-day}
for (i in 1:dim(combined_df)[1]) {
  id = combined_df[i,"PatientID"]
  patient_df = subset(combined_df,PatientID == id)
  final.day = max(patient_df$VisitDay)
  #if (final.day==0){ # several patients must have dropped out immediately 
  #  print(combined_df[i,])
  #}
  combined_df[i,"FinalDay"] = final.day
}
```

### Create test set 
Set aside the test set, i.e. the patient's whose score we will predict on Kaggle. 
```{r test-set}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1] 
```
Note that this subsetting does not produce 379 as expected. What is going on here? 
```{r explore-test-set}
for (id in unique(test_df$PatientID)) { # for each unique id
  sub_df = subset(test_df, PatientID==id)
  if (dim(sub_df)[1]>1){
    print(sub_df)
  }
}
```
We see that multiple patients were assessed multiple times on the final day (perhaps by different people, at different locations). We can remove such duplicates with the `distinct()` function.
```{r remove-duplicates}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
```
This still doesn't yield a dataset of size 379 since there are PANSS_Total scores that differ! Thus, there must be patients who were assessed multiple times in the same day by the same person and at the same location (for example, PatientID 50505). 
```{r explore-test-set-v2}
for (id in unique(test_df$PatientID)) { # for each unique id
  sub_df = subset(test_df, PatientID==id)
  if (dim(sub_df)[1]>1){
    print(sub_df)
  }
}
```
We see that in each of these cases the difference in PANSS_Total is a few points (except PatientID 50299). Simplest solution is to average over these values: 
```{r simple-average}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data 

library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[7] = "PANSS_Total"
dim(test_df)
```
Which returns the desired number of 379 rows. 

### Create training set
```{r remove-test-from-total}
dim(combined_df)
combined_df = anti_join(combined_df, test_df)
dim(combined_df)
```
This removes 410 elements as expected. We should also remove any duplicates from here as we did for the test set. 

```{r remove-training-dups}
training_df = distinct(combined_df)
dim(training_df)[1]
``` 
We should also average over cases where all is identical except for the total PANSS score: 
```{r simple-average-training}
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[7] = "PANSS_Total"
dim(training_df)
```

## Gradient Boosting (gbm)
Note that in this section we make frequent use of code snippets given by http://uc-r.github.io/gbm_regression. 
```{r gboost}
set.seed(1)
library(gbm)          # basic implementation

# train GBM model
gbm.fit <- gbm(
  formula = PANSS_Total ~ .,
  distribution = "gaussian",
  data = training_df,
  n.trees = 5000,
  interaction.depth = 1, 
  shrinkage = 0.01,
  cv.folds = 10,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

# print results
print(gbm.fit)
## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train, 
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.
```

```{r visualize-boosting}
# get MSE and compute RMSE
min(gbm.fit$cv.error)
sqrt(min(gbm.fit$cv.error))

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
```

### Tune boosting 

## Gradient Boosting (xgboost)



## Gradient Boosting (h2o)