---
title: "Forecasting"
author: "Jeremy Binagia and Sai Gourisankar"
date: "7/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls()) # clear global environment 
library(h2o)          # a java-based platform
library(plyr)
library(ggplot2)
```

## Notes/Assumptions
### General 
- predict "18th" week total PANSS score 
- i.e. create a csv file that contains the PatientID and the predicted 18th-week PANSS score
- can submit predictions here https://www.kaggle.com/c/stanford-stats-202-forecasting/data
- challenges
  - typically data in class ranges over time OR over individual. Here we have both. 
  - we can't average over individuals because we need distinct predictions for each individual given. 
  - we can't average over time because visitDay is likely a decently strong predictor for the level. 
- gradient descent reference http://uc-r.github.io/gbm_regression
- no need to standardize data if using decision trees!
  
### Model 
  - clearly PANSS total is the response 
  - likely predictors include
    - (maybe) study 
    - Country 
    - TxGroup
    - VisitDay
  - we will keep PatientID in our dataframe as a "key" for each row/patient. 
  - exclude
    - individual scores (if you had these, you could simply sum them to get the total score)
    - SiteID - we don't know where they will be evaluated 
    - RaterID - we don't know who will evaluate them (see for example https://groups.google.com/forum/#!searchin/stats202/model%7Csort:date/stats202/y7IxH2KYTk4/U-NfpWgGCQAJ)
  - when it comes time to predict, we specify a patient ID which then sets the above listed predictors except for VisitDay. *For the final predictions, let's assume VisitDay is one week after the given patient's last visit?*

### Splitting the data 
  - Following the suggested methodology found in slide 14 of the lecture 5 slides, we will split the data as follows:
    - test set: The 17th-week for the 379 patients in study E (where we define "17th-week" as our last recorded measurement from them). We will use the test set to choose between different models.  
    - prediction set: Take the test set and add 7 days to `VisitDay` as a proxy for "18th-week" visit. 
    - training/development set: the total aggregate of all of the studies minus the observations in the test set (note that we do not choose to set aside a validation/development set since ultimately we will use k-fold cross-validation for tuning hyper-parameters).   
    - issues:
      - study E seems to be distinct in the segmentation plots from the other studies... could be a bias in that there is much more data from the other studies in our training set. 
  - following suggests combining studies A-D since they are essentially the same https://groups.google.com/forum/#!searchin/stats202/validation%7Csort:date/stats202/ULXRN3seaBE/5DFG-1LmEgAJs
  - suggests a 70/30 or 70/15/15 split for training vs. validation https://www.researchgate.net/post/Is_there_an_ideal_ratio_between_a_training_set_and_validation_set_Which_trade-off_would_you_suggest
  
### Algorithms
- xgboost strictly better than using gbm. h2o is a software that has a wrapper for xgboost. 
- don't use forecast methods like exponential smoothing since we don't have a single timeseries (like the stock market price for example) 
  

## Thoughts/Conclusions
- decision trees are likely ill-suited for this dataset https://stats.stackexchange.com/questions/235189/random-forest-regression-not-predicting-higher-than-training-data

## Setup
```{r Load Data}
rm(list = ls()) # clear global environment 
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
```
Note that most patients from study E are from USA or Russia (about an even split). 

We should remove true duplicates from the dataset. Note that assessment ID is a completely unique identifier (no two rows have the same assessment id from examing the raw data in Excel). 
```{r remove-true-duplicates}
# # check that there are in fact duplicates 
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
# 
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
# 
# # check disregarding assessment id 
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
# 
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
# 
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
```

The following creates a list of the patients we should consider for the Kaggle submission. 
```{r prediction-patients}
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission 
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values 
#n_distinct(prediction.patients)   # gives same result 
```

How many visits did each patient have?
```{r number-visits}
# number.visits = count(E_df, vars = "PatientID")
# 
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
```


## Data Cleaning
Remove columns not corresponding to our predictors and response (total PANSS score)
```{r subset-df}
A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
# A_df = subset(A_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# B_df = subset(B_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# C_df = subset(C_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# D_df = subset(D_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
# E_df = subset(E_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
```

Combine studies
```{r combine-studies}
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
```

What was the final visit day for each patient? We need this info since our test set will the final observation for each of the 379 selected study E patients we will evaluate our models on. 
```{r final-visit-day}
for (i in 1:dim(combined_df)[1]) {
  id = combined_df[i,"PatientID"]
  patient_df = subset(combined_df,PatientID == id)
  final.day = max(patient_df$VisitDay)
  #if (final.day==0){ # several patients must have dropped out immediately 
  #  print(combined_df[i,])
  #}
  combined_df[i,"FinalDay"] = final.day
}
```

### Create test set 
Set aside the test set, i.e. the patient's whose score we will predict on Kaggle. 
```{r test-set}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1] 
```
Note that this subsetting does not produce 379 as expected. What is going on here? 
```{r explore-test-set}
for (id in unique(test_df$PatientID)) { # for each unique id
  sub_df = subset(test_df, PatientID==id)
  if (dim(sub_df)[1]>1){
    print(sub_df)
  }
}
```
We see that multiple patients were assessed multiple times on the final day (perhaps by different people, at different locations). We can remove such duplicates with the `distinct()` function.
```{r remove-duplicates}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
```
This still doesn't yield a dataset of size 379 since there are PANSS_Total scores that differ! Thus, there must be patients who were assessed multiple times in the same day by the same person and at the same location (for example, PatientID 50505). 
```{r explore-test-set-v2}
for (id in unique(test_df$PatientID)) { # for each unique id
  sub_df = subset(test_df, PatientID==id)
  if (dim(sub_df)[1]>1){
    print(sub_df)
  }
}
```
We see that in each of these cases the difference in PANSS_Total is a few points (except PatientID 50299). Simplest solution is to average over these values: 
```{r simple-average}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data 

library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[length(names(test_df))] = "PANSS_Total"
dim(test_df)
```
Which returns the desired number of 379 rows. 

Finally, we would like our actually forecasting set to best reflect the "18-th week" visit. To do so, we imagine each patient going back for assessment one week after whenever their final week was. So we add a value of 7 days to their `VisitDay`. We also drop the `FinalDay` column at this point. Also potentially scale the data here. 
```{r create-test-set}
test_df = subset(test_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#test_df = subset(test_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))

forecast_df = test_df
forecast_df = subset(forecast_df, select = c(PatientID, Country, TxGroup, VisitDay, Study))
#forecast_df = subset(forecast_df, select = c(PatientID, Country, VisitDay, Study))
forecast_df$VisitDay = forecast_df$VisitDay + 7 
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)

# create "Naive" submission 
write.csv(test_df[,c("PatientID","PANSS_Total")],'naive-forecast.csv',row.names=FALSE)
```

### Create training set
```{r remove-test-from-total}
dim(combined_df)
combined_df = anti_join(combined_df, test_df)
dim(combined_df)
```
This removes 410 elements as expected. We should also remove any duplicates from here as we did for the test set. Note that I ended up not removing these 410 observations since I considered them distinct from the test set (the test set has VisitDay shifted upwards by a value of 7).  

```{r remove-training-dups}
training_df = distinct(combined_df)
dim(training_df)[1]
``` 
We should also average over cases where all is identical except for the total PANSS score: 
```{r simple-average-training}
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[length(names(training_df))] = "PANSS_Total"
dim(training_df)
training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#training_df = subset(training_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
```

While we could scale some variables, scaling does not matter for decision trees! 
```{r training-scale}
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
```

## Gradient Boosting (gbm)
<!-- Note that in this section we make frequent use of code snippets given by http://uc-r.github.io/gbm_regression.  -->
<!-- ```{r gboost} -->
<!-- set.seed(1) -->
<!-- library(gbm)          # basic implementation -->

<!-- # train GBM model -->
<!-- gbm.fit <- gbm( -->
<!--   formula = PANSS_Total ~ ., -->
<!--   distribution = "gaussian", -->
<!--   data = training_df, -->
<!--   n.trees = 5000, -->
<!--   interaction.depth = 5, # 3 is optimal. 1 led to decrease.  -->
<!--   shrinkage = 0.1, -->
<!--   cv.folds = 10, -->
<!--   n.cores = NULL, # will use all cores by default -->
<!--   verbose = FALSE -->
<!--   )   -->

<!-- # print results -->
<!-- print(gbm.fit) -->
<!-- ## gbm(formula = PANSS_Total ~ ., distribution = "gaussian", data = training_df,  -->
<!-- ##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001,  -->
<!-- ##     cv.folds = 5, verbose = FALSE, n.cores = NULL) -->
<!-- ## A gradient boosted model with gaussian loss function. -->
<!-- ## 10000 iterations were performed. -->
<!-- ## The best cross-validation iteration was 10000. -->
<!-- ## There were 80 predictors of which 45 had non-zero influence. -->
<!-- ``` -->
<!-- Displayed values are best so far, as found via manual tuning.  -->

<!-- ```{r visualize-boosting} -->
<!-- # get MSE -->
<!-- min(gbm.fit$cv.error) -->

<!-- # plot loss function as a result of n trees added to the ensemble -->
<!-- gbm.perf(gbm.fit, method = "cv") -->
<!-- ``` -->

<!-- ### Tune boosting  -->
<!-- ```{r gbm-tuning} -->
<!-- # create hyperparameter grid -->
<!-- hyper_grid <- expand.grid( -->
<!--   shrinkage = c(.01, .1, .3), -->
<!--   interaction.depth = c(1, 3, 5), -->
<!--   n.minobsinnode = c(5, 10, 15), -->
<!--   bag.fraction = c(.65, .8, 1),    # introduce stochastic gradient descent by allowing bag.fraction < 1 -->
<!--   optimal_trees = 0,               # a place to dump results -->
<!--   min_RMSE = 0                     # a place to dump results -->
<!-- ) -->

<!-- # total number of combinations -->
<!-- nrow(hyper_grid) -->


<!-- # randomize data -->
<!-- random_index <- sample(1:nrow(training_df), nrow(training_df)) -->
<!-- random_training_df <- training_df[random_index, ] -->

<!-- # grid search  -->
<!-- for(i in 1:nrow(hyper_grid)) { -->

<!--   # reproducibility -->
<!--   set.seed(123) -->

<!--   # train model -->
<!--   gbm.tune <- gbm( -->
<!--     formula = PANSS_Total ~ ., -->
<!--     distribution = "gaussian", -->
<!--     data = random_training_df, -->
<!--     n.trees = 5000, -->
<!--     interaction.depth = hyper_grid$interaction.depth[i], -->
<!--     shrinkage = hyper_grid$shrinkage[i], -->
<!--     n.minobsinnode = hyper_grid$n.minobsinnode[i], -->
<!--     bag.fraction = hyper_grid$bag.fraction[i], -->
<!--     train.fraction = .75, -->
<!--     n.cores = NULL, # will use all cores by default -->
<!--     verbose = FALSE -->
<!--   ) -->

<!--   # add min training error and trees to grid -->
<!--   hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error) -->
<!--   hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error)) -->
<!-- } -->

<!-- hyper_grid %>%  -->
<!--   dplyr::arrange(min_RMSE) %>% -->
<!--   head(10) -->
<!-- ``` -->

<!-- ```{r train-top-gbm-model} -->
<!-- set.seed(1) -->

<!-- # train GBM model -->
<!-- gbm.fit.final <- gbm( -->
<!--   formula = PANSS_Total ~ ., -->
<!--   distribution = "gaussian", -->
<!--   data = training_df, -->
<!--   n.trees = 5000, -->
<!--   interaction.depth = 3, -->
<!--   shrinkage = 0.1, -->
<!--   n.minobsinnode = 5, -->
<!--   bag.fraction = .65,  -->
<!--   train.fraction = 1, -->
<!--   n.cores = NULL, # will use all cores by default -->
<!--   verbose = FALSE -->
<!--   ) -->
<!-- ``` -->

<!-- ### Visualize -->
<!-- ```{r gbm-visualize} -->
<!-- par(mar = c(5, 8, 1, 1)) -->
<!-- summary( -->
<!--   gbm.fit.final,  -->
<!--   cBars = 10, -->
<!--   method = relative.influence, # also can use permutation.test.gbm -->
<!--   las = 2 -->
<!--   ) -->
<!-- ``` -->

<!-- ### Prediction -->
<!-- ```{r gbm-prediction} -->
<!-- # predict values for test data -->
<!-- pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, test_df) -->

<!-- # results -->
<!-- caret::RMSE(pred, test_df$PANSS_Total) -->

<!-- # write to csv for Kaggle submission -->
<!-- write.csv(pred,'healthstudy2.csv') -->
<!-- ``` -->

## Gradient Boosting (xgboost)
<!-- ### xgboost setup -->
<!-- ```{r xgboost-setup} -->
<!-- # variable names -->
<!-- features <- setdiff(names(training_df), "PANSS_Total") -->

<!-- # Create the treatment plan from the training data -->
<!-- treatplan <- vtreat::designTreatmentsZ(training_df, features, verbose = FALSE) -->

<!-- # Get the "clean" variable names from the scoreFrame -->
<!-- new_vars <- treatplan %>% -->
<!--   magrittr::use_series(scoreFrame) %>%         -->
<!--   dplyr::filter(code %in% c("clean", "lev")) %>%  -->
<!--   magrittr::use_series(varName)      -->

<!-- # Prepare the training data -->
<!-- features_train <- vtreat::prepare(treatplan, training_df, varRestriction = new_vars) %>% as.matrix() -->
<!-- response_train <- training_df$PANSS_Total -->

<!-- # Prepare the test data -->
<!-- features_test <- vtreat::prepare(treatplan, test_df, varRestriction = new_vars) %>% as.matrix() -->
<!-- response_test <- test_df$PANSS_Total -->

<!-- # dimensions of one-hot encoded data -->
<!-- dim(features_train) -->
<!-- dim(features_test) -->
<!-- ``` -->

<!-- ```{r sample-xgboost-fit} -->
<!-- # reproducibility -->
<!-- set.seed(123) -->

<!-- xgb.fit1 <- xgb.cv( -->
<!--   data = features_train, -->
<!--   label = response_train, -->
<!--   nrounds = 1000, -->
<!--   nfold = 5, -->
<!--   objective = "reg:linear",  # for regression models -->
<!--   verbose = 0,               # silent, -->
<!--   early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r xgboost-visualize} -->
<!-- # get number of trees that minimize error -->
<!-- xgb.fit1$evaluation_log %>% -->
<!--   dplyr::summarise( -->
<!--     ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1], -->
<!--     rmse.train   = min(train_rmse_mean), -->
<!--     ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1], -->
<!--     rmse.test   = min(test_rmse_mean), -->
<!--   ) -->
<!-- ##   ntrees.train rmse.train ntrees.test rmse.test -->
<!-- ## 1          965  0.5022836          60  27572.31 -->

<!-- # plot error vs number trees -->
<!-- ggplot(xgb.fit1$evaluation_log) + -->
<!--   geom_line(aes(iter, train_rmse_mean), color = "red") + -->
<!--   geom_line(aes(iter, test_rmse_mean), color = "blue") -->
<!-- ``` -->

<!-- ### xgboost Tuning  -->
<!-- ```{r xgboost-grid} -->
<!-- # create hyperparameter grid -->
<!-- hyper_grid <- expand.grid( -->
<!--   eta = c(.01, .05, .1, .3), -->
<!--   max_depth = c(1, 3, 5, 7), -->
<!--   min_child_weight = c(1, 3, 5, 7), -->
<!--   subsample = c(.65, .8, 1),  -->
<!--   colsample_bytree = c(.8, .9, 1), -->
<!--   optimal_trees = 0,               # a place to dump results -->
<!--   min_RMSE = 0                     # a place to dump results -->
<!-- ) -->

<!-- nrow(hyper_grid) -->
<!-- ``` -->

<!-- ```{r xgboost-grid-search} -->
<!-- # grid search  -->
<!-- for(i in 1:nrow(hyper_grid)) { -->

<!--   # create parameter list -->
<!--   params <- list( -->
<!--     eta = hyper_grid$eta[i], -->
<!--     max_depth = hyper_grid$max_depth[i], -->
<!--     min_child_weight = hyper_grid$min_child_weight[i], -->
<!--     subsample = hyper_grid$subsample[i], -->
<!--     colsample_bytree = hyper_grid$colsample_bytree[i] -->
<!--   ) -->

<!--   # reproducibility -->
<!--   set.seed(123) -->

<!--   # train model -->
<!--   xgb.tune <- xgb.cv( -->
<!--     params = params, -->
<!--     data = features_train, -->
<!--     label = response_train, -->
<!--     nrounds = 5000, -->
<!--     nfold = 5, -->
<!--     objective = "reg:linear",  # for regression models -->
<!--     verbose = 0,               # silent, -->
<!--     early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees -->
<!--   ) -->

<!--   # add min training error and trees to grid -->
<!--   hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean) -->
<!--   hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean) -->
<!-- } -->

<!-- hyper_grid %>% -->
<!--   dplyr::arrange(min_RMSE) %>% -->
<!--   head(10) -->
<!-- ``` -->

<!-- ```{r xgboost-fit-final} -->
<!-- # parameter list -->
<!-- params <- list( -->
<!--   eta = 0.01, -->
<!--   max_depth = 5, -->
<!--   min_child_weight = 5, -->
<!--   subsample = 0.65, -->
<!--   colsample_bytree = 1 -->
<!-- ) -->

<!-- # train final model -->
<!-- xgb.fit.final <- xgboost( -->
<!--   params = params, -->
<!--   data = features_train, -->
<!--   label = response_train, -->
<!--   nrounds = 1576, -->
<!--   objective = "reg:linear", -->
<!--   verbose = 0 -->
<!-- ) -->
<!-- ``` -->

<!-- ### Visualization  -->

<!-- ### Prediciton -->
<!-- ```{r xgboost-prediction} -->
<!-- # predict values for test data -->
<!-- pred <- predict(xgb.fit.final, test_df) -->

<!-- # results -->
<!-- caret::RMSE(pred, response_test) -->
<!-- ``` -->


## Gradient Boosting (h2o)
<!-- ```{r h20-startup} -->
<!-- h2o.no_progress() -->
<!-- h2o.init(max_mem_size = "6g") # have 16g ram total  -->
<!-- ``` -->


<!-- ```{r h20-automated-stopping} -->
<!-- # create feature names -->
<!-- y <- "PANSS_Total" -->
<!-- x <- setdiff(names(training_df[,-"PatientID"]), y) -->

<!-- # turn training set into h2o object -->
<!-- train.h2o <- as.h2o(training_df[,-"PatientID"]) -->

<!-- # training basic GBM model with defaults -->
<!-- h2o.fit2 <- h2o.gbm( -->
<!--   x = x, -->
<!--   y = y, -->
<!--   training_frame = train.h2o, -->
<!--   nfolds = 10, -->
<!--   ntrees = 5000, -->
<!--   stopping_rounds = 10, -->
<!--   stopping_tolerance = 0, -->
<!--   max_runtime_secs = 60*10, -->
<!--   seed = 1 -->
<!-- ) -->

<!-- # model stopped after xx trees -->
<!-- h2o.fit2@parameters$ntrees -->

<!-- # cross validated MSE -->
<!-- h2o.rmse(h2o.fit2, xval = TRUE)^2 -->

<!-- # assess model results -->
<!-- h2o.fit2 -->
<!-- ``` -->

<!-- ### Full grid search -->
<!-- ```{r h20-full-grid} -->
<!-- # create training & validation sets -->
<!-- split <- h2o.splitFrame(train.h2o, ratios = 0.75) -->
<!-- train <- split[[1]] -->
<!-- valid <- split[[2]] -->

<!-- # create hyperparameter grid -->
<!-- rm(hyper_grid) -->
<!-- hyper_grid = list( -->
<!--   max_depth = c(1,3,4), # depth of each tree -->
<!--   min_rows = c(5,10,20), # minimum observations in a terminal node -->
<!--   learn_rate = c(0.005, 0.01, 0.05, 0.1), -->
<!--   learn_rate_annealing = c(1), # 1 tends to always beat 0.99 -->
<!--   sample_rate = c(.65, 0.7, 0.75, 0.8), # row sample rate. better to have less than 1 it seems -->
<!--   col_sample_rate = c(0.7, .8, .9) # always better to have less than 1 here -->
<!-- ) -->

<!-- # number of combinations -->
<!-- nrow(expand.grid(hyper_grid)) -->

<!-- # # perform grid search  -->
<!-- # grid <- h2o.grid( -->
<!-- #   algorithm = "gbm", -->
<!-- #   grid_id = "gbm_grid1", -->
<!-- #   x = x,  -->
<!-- #   y = y,  -->
<!-- #   training_frame = train, -->
<!-- #   validation_frame = valid, -->
<!-- #   hyper_params = hyper_grid, -->
<!-- #   ntrees = 10000, -->
<!-- #   stopping_rounds = 10, -->
<!-- #   #stopping_tolerance = 0, -->
<!-- #   seed = 1 -->
<!-- #   ) -->
<!-- #  -->
<!-- # # collect the results and sort by our model performance metric of choice -->
<!-- # grid_perf <- h2o.getGrid( -->
<!-- #   grid_id = "gbm_grid1",  -->
<!-- #   sort_by = "mse",  -->
<!-- #   decreasing = FALSE -->
<!-- #   ) -->
<!-- # grid_perf -->
<!-- ``` -->

<!-- ### Random discrete grid search  -->
<!-- ```{r random-discrete-grid} -->
<!-- # random grid search criteria -->
<!-- search_criteria <- list( -->
<!--   strategy = "RandomDiscrete", -->
<!--   stopping_metric = "mse", -->
<!--   stopping_tolerance = 0.005, # MSE tolerance -->
<!--   stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement  -->
<!--   max_runtime_secs = 60*60 # limit how long it runs -->
<!--   ) -->

<!-- # perform grid search  -->
<!-- gbm_grid2 <- h2o.grid( -->
<!--   algorithm = "gbm", -->
<!--   grid_id = "gbm_grid2", -->
<!--   x = x,  -->
<!--   y = y,  -->
<!--   training_frame = train, -->
<!--   validation_frame = valid, -->
<!--   hyper_params = hyper_grid, -->
<!--   search_criteria = search_criteria, # add search criteria -->
<!--   ntrees = 10000, -->
<!--   #stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that  -->
<!--   #stopping_tolerance = 0, -->
<!--   seed = 1 -->
<!--   ) -->

<!-- # collect the results and sort by our model performance metric of choice -->
<!-- grid_perf <- h2o.getGrid( -->
<!--   grid_id = "gbm_grid2",  -->
<!--   sort_by = "mse",  -->
<!--   decreasing = FALSE -->
<!--   ) -->
<!-- grid_perf -->
<!-- ``` -->

<!-- Top 5 models all have `max_depth` of 5 when the options are 1, 3, and 5. Same if the options are 1, 5, 10. Chooses 4 if options are 3,4,5,6,7 so let's keep it below 4. If I set max_depth to 1,2,3,4 it still always chooses 4 and sometimes 3. They also always have `learn_rate_annealing` of 1. Best column sample rates are never 1, always 0.8 or 0.9. Best models tend to use lowest learning rate of 0.01 so far. All use at least min_rows of 5.  All have sample rate of at least 0.65.  -->

<!-- ```{r h20-performance} -->
<!-- # Grab the model_id for the top model, chosen by validation error -->
<!-- best_model_id <- grid_perf@model_ids[[1]] -->
<!-- best_model <- h2o.getModel(best_model_id) -->

<!-- # Now let’s get performance metrics on the best model -->
<!-- h2o.performance(model = best_model, valid = TRUE) -->
<!-- ``` -->

<!-- ```{r h20-train-final-best} -->
<!-- # train final model -->
<!-- h2o.final <- h2o.gbm( -->
<!--   x = x, -->
<!--   y = y, -->
<!--   training_frame = train.h2o, -->
<!--   nfolds = 10, -->
<!--   ntrees = 10000, -->
<!--   learn_rate = 0.1, -->
<!--   learn_rate_annealing = 1, -->
<!--   max_depth = 4, -->
<!--   min_rows = 10, -->
<!--   sample_rate = 0.75, -->
<!--   col_sample_rate = 0.9, -->
<!--   stopping_rounds = 10, -->
<!--   #stopping_tolerance = 0.005, -->
<!--   seed = 1 -->
<!-- ) -->

<!-- # model stopped after xx trees -->
<!-- h2o.final@parameters$ntrees -->

<!-- # cross validated MSE -->
<!-- h2o.rmse(h2o.final, xval = TRUE)^2 -->
<!-- ``` -->

<!-- ### Visualization -->
<!-- ```{r variable-importance} -->
<!-- h2o.varimp_plot(h2o.final, num_of_features = 5) -->
<!-- ``` -->

<!-- ### Prediction -->
<!-- ```{r h20-prediction} -->
<!-- # convert test set to h2o object -->
<!-- test.h2o <- as.h2o(test_df) -->

<!-- # evaluate performance on new data -->
<!-- h2o.performance(model = h2o.final, newdata = test.h2o) -->

<!-- # predict values with predict -->
<!-- h2o.predict(h2o.final, newdata = test.h2o) # predict with h2o.predict -->
<!-- test.h2o$prediction = predict(h2o.final, test.h2o) # gives same result as above -->

<!-- # write to csv for Kaggle submission -->
<!-- forecast.h2o <- as.h2o(forecast_df) -->
<!-- forecast.h2o$PANSS_Total = predict(h2o.final, forecast.h2o)  -->
<!-- h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'h2o-prediction.csv',force=TRUE) -->
<!-- ``` -->

## Random Forests
<!-- Let's given random forests a shot, at least to see how the test error compares to the other methods we use (as a gut check). Following guide from http://uc-r.github.io/random_forests. -->
<!-- ```{r default-rf-model} -->
<!-- library(randomForest) # basic implementation -->

<!-- # for reproduciblity -->
<!-- set.seed(1) -->

<!-- # default RF model -->
<!-- m1 <- randomForest( -->
<!--   formula = PANSS_Total ~ ., -->
<!--   data    = training_df[,-"PatientID"], -->
<!--   mtry = 2 -->
<!-- ) -->

<!-- m1 -->
<!-- plot(m1) -->
<!-- ``` -->

<!-- ```{r rf-mse} -->
<!-- # number of trees with lowest MSE -->
<!-- which.min(m1$mse) -->

<!-- # RMSE of this optimal random forest -->
<!-- sqrt(m1$mse[which.min(m1$mse)]) -->
<!-- ``` -->

<!-- ```{r rf-validation-set} -->
<!-- require(tidyr) -->
<!-- require(dplyr) -->
<!-- # create training and validation data -->
<!-- set.seed(1) -->

<!-- # split data -->
<!-- training_rows = sample(1:nrow(training_df), floor(nrow(training_df)*0.8)) -->
<!-- train_v2 = training_df[training_rows,] -->
<!-- valid = training_df[-training_rows,] -->
<!-- x_test = valid -->
<!-- y_test = valid$PANSS_Total -->

<!-- rf_oob_comp <- randomForest( -->
<!--   formula = PANSS_Total ~ ., -->
<!--   data    = train_v2[,-"PatientID"], -->
<!--   xtest   = x_test[,-c("PatientID","PANSS_Total")], -->
<!--   ytest   = y_test -->
<!-- ) -->

<!-- # extract OOB & validation errors -->
<!-- oob <- rf_oob_comp$mse -->
<!-- validation <- rf_oob_comp$test$mse -->

<!-- # compare error rates -->
<!-- tibble::tibble( -->
<!--   `Out of Bag Error` = oob, -->
<!--   `Test error` = validation, -->
<!--   ntrees = 1:rf_oob_comp$ntree -->
<!-- ) %>% -->
<!--   gather(Metric, MSE, -ntrees) %>% -->
<!--   ggplot(aes(ntrees, MSE, color = Metric)) + -->
<!--   geom_line() + -->
<!--   xlab("Number of trees") -->
<!-- ``` -->

<!-- ### Tuning via h2o -->
<!-- ```{r h2o-rf} -->
<!-- # start up h2o -->
<!-- h2o.init(max_mem_size = "6g") -->
<!-- set.seed(1) -->

<!-- # create feature names -->
<!-- y <- "PANSS_Total" -->
<!-- x <- setdiff(names(training_df[,-"PatientID"]), y) -->

<!-- # turn training set into h2o object -->
<!-- train.h2o <- as.h2o(training_df[,-"PatientID"]) -->

<!-- # only train on study E -->
<!-- #x <- setdiff(names(training_df[,-c("PatientID","Study")]), y) -->
<!-- #train.h2o <- as.h2o(subset(training_df, Study=="E",select=c(Country, TxGroup, VisitDay, PANSS_Total))) -->

<!-- # second hypergrid -->
<!-- hyper_grid.h2o <- list( -->
<!--   ntrees      = seq(300, 550, by = 50),  -->
<!--   mtries      = 2, -->
<!--   max_depth   = seq(15, 45, by = 5),  -->
<!--   min_rows    = seq(7, 11, by = 1),  -->
<!--   nbins       = seq(5, 25, by = 5),  -->
<!--   sample_rate = c(0.4,0.45,0.5,0.55,.6,.65,.7)  -->
<!-- ) -->

<!-- # random grid search criteria -->
<!-- search_criteria <- list( -->
<!--   strategy = "RandomDiscrete", -->
<!--   stopping_metric = "mse", -->
<!--   stopping_tolerance = 0.005, -->
<!--   stopping_rounds = 10, -->
<!--   max_runtime_secs = 60*60 -->
<!--   ) -->

<!-- # build grid search -->
<!-- random_grid <- h2o.grid( -->
<!--   algorithm = "randomForest", -->
<!--   grid_id = "rf_grid2", -->
<!--   x = x, -->
<!--   y = y, -->
<!--   training_frame = train.h2o, -->
<!--   hyper_params = hyper_grid.h2o, -->
<!--   search_criteria = search_criteria -->
<!--   ) -->

<!-- # collect the results and sort by our model performance metric of choice -->
<!-- grid_perf2 <- h2o.getGrid( -->
<!--   grid_id = "rf_grid2", -->
<!--   sort_by = "mse", -->
<!--   decreasing = FALSE -->
<!--   ) -->
<!-- print(grid_perf2) -->
<!-- ``` -->

<!-- ```{r h20-cv-notes} -->
<!-- # first grid -->
<!-- hyper_grid.h2o <- list( -->
<!--   ntrees      = seq(200, 500, by = 150), # best all had 350 min so set 350 as new min -->
<!--   mtries      = seq(2,4, by = 1), # best all have 2 so set this identically to 2 -->
<!--   max_depth   = seq(20, 40, by = 5), -->
<!--   min_rows    = seq(1, 5, by = 2), # best all have 5 (so set 5 as min) -->
<!--   nbins       = seq(10, 30, by = 5), -->
<!--   sample_rate = c(.55, .632, .75) # best all have 0.55 so vary around this -->
<!-- ) # best model from this one has test MSE of 131.2987 -->

<!-- # second hypergrid -->
<!-- hyper_grid.h2o <- list( -->
<!--   ntrees      = seq(350, 500, by = 75), # none of the top 5 use 500  -->
<!--   mtries      = 2, -->
<!--   max_depth   = seq(20, 40, by = 5), # none of top 5 use 40  -->
<!--   min_rows    = seq(5, 10, by = 2), # none of the top 5 models use 5  -->
<!--   nbins       = seq(10, 30, by = 5), # none of top 5 use 10  -->
<!--   sample_rate = c(0.45,.55, .65) # none of the top 5 models use sample_rate of 0.65  -->
<!-- ) # best model from this has test MSE of 131.1589. best model while using study E gave 130.52. Latter scored 121.37919 on the public leaderboard. second time I ran this I had test MSE of 131.2212. Scored almost the same on Kaggle than when I dropped everything but Study E.  -->
<!-- ``` -->


<!-- ```{r h2o-rf-evaluate} -->
<!-- # Grab the model_id for the top model, chosen by validation error -->
<!-- best_model_id <- grid_perf2@model_ids[[1]] -->
<!-- best_model <- h2o.getModel(best_model_id) -->
<!-- h2o.varimp_plot(best_model) -->

<!-- # Now let’s evaluate the model performance on a test set -->
<!-- test_df.h2o <- as.h2o(test_df) -->
<!-- best_model_perf <- h2o.performance(model = best_model, newdata = test_df.h2o) -->

<!-- # View prediction  -->
<!-- prediction = predict(best_model, test_df.h2o)  -->
<!-- plot(as.vector(prediction), test_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100)) -->
<!-- abline(0,1) # line with y-intercept 0 and slope 1  -->

<!-- # RMSE of best model -->
<!-- h2o.mse(best_model_perf) -->

<!-- # write to csv for Kaggle submission -->
<!-- forecast.h2o <- as.h2o(forecast_df) -->
<!-- forecast.h2o$PANSS_Total = predict(best_model, forecast.h2o) -->
<!-- h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'rf-prediction.csv',force=TRUE) -->
<!-- ``` -->



## Linear models
```{r adjust-dataframes}
#training_df = subset(training_df, select = c(PatientID, TxGroup, VisitDay, Study,PANSS_Total))
#test_df = subset(test_df, select = c(PatientID, TxGroup, VisitDay, Study,PANSS_Total))
```

<!-- ### Linear regression -->
<!-- Fit a linear model using least squares on the training set, and report the test error obtained. -->
<!-- ```{r linear} -->
<!-- linear.mod = lm(PANSS_Total ~., data=training_df) -->
<!-- summary(linear.mod) -->

<!-- # Calculate test MSE -->
<!-- mean((test_df$PANSS_Total - predict(linear.mod, test_df))^2) -->
<!-- ``` -->

<!-- ### Ridge regression -->
<!-- Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained. -->
<!-- ```{r ridge} -->
<!-- library(glmnet) -->
<!-- set.seed(1) -->

<!-- # Create design matrices -->
<!-- train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df) -->
<!-- test.mat = model.matrix(PANSS_Total ~ .-PatientID, data = test_df) -->
<!-- # adding exp(-VisitDay) didn't seem to help much -->

<!-- # Ridge regression for array of lambda values -->
<!-- #grid=10^seq(10,-3,length=100) -->
<!-- #ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12) -->
<!-- ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0) -->
<!-- plot(ridge.mod, xvar = "lambda") -->

<!-- # Find lambda through cross-validation -->
<!-- cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0) -->
<!-- ridge.cv.out = cv.out  -->
<!-- plot(cv.out) -->
<!-- bestlam = cv.out$lambda.min -->
<!-- bestlam -->

<!-- # Calculate test MSE -->
<!-- ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat) -->
<!-- mean((ridge.pred - test_df$PANSS_Total)^2) -->
<!-- ``` -->
<!-- Note that the first and second vertical dashed lines represent the λ value with the minimum MSE and the largest λ value within one standard error of the minimum MSE. -->

<!-- ```{r ridge-lambda} -->
<!-- min(cv.out$cvm)       # minimum MSE -->
<!-- cv.out$lambda.min     # lambda for this min MSE -->

<!-- cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE -->
<!-- cv.out$lambda.1se  # lambda for this MSE -->

<!-- # visualize how much we can restrain coefficients while still having predictive accuracy  -->
<!-- ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0) -->
<!-- plot(ridge_min, xvar = "lambda") -->
<!-- abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed") -->

<!-- library(broom) -->
<!-- coef(cv.out, s = "lambda.1se") %>% -->
<!--   tidy() %>% -->
<!--   filter(row != "(Intercept)") %>% -->
<!--   top_n(25, wt = abs(value)) %>% -->
<!--   ggplot(aes(value, reorder(row, value))) + -->
<!--   geom_point() + -->
<!--   ggtitle("Top 25 influential variables") + -->
<!--   xlab("Coefficient") + -->
<!--   ylab(NULL) -->
<!-- ``` -->

<!-- ### Lasso regression -->
<!-- Fit a lasso model on the training set, with λ chosen by crossvalidation -->
<!-- ```{r lasso} -->
<!-- library(glmnet) -->
<!-- set.seed(1) -->

<!-- # Lasso regression for array of lambda values -->
<!-- #grid=10^seq(10,-3,length=100) -->
<!-- #lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1,lambda=grid, thresh=1e-12) -->
<!-- lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1) -->
<!-- plot(lasso.mod, xvar = "lambda") -->

<!-- # Find lambda through cross-validation -->
<!-- cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1) -->
<!-- lasso.cv.out = cv.out  -->
<!-- plot(cv.out) -->
<!-- bestlam=cv.out$lambda.min -->
<!-- bestlam -->

<!-- # Calculate test MSE -->
<!-- lasso.pred=predict(lasso.mod,s=bestlam,newx=test.mat) -->
<!-- mean((lasso.pred-test_df$PANSS_Total)^2) -->

<!-- predict(cv.out, s = bestlam, type = "coefficients") -->
<!-- ``` -->

<!-- ```{r lasso-error} -->
<!-- min(cv.out$cvm)       # minimum MSE -->
<!-- cv.out$lambda.min     # lambda for this min MSE -->

<!-- cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE -->
<!-- cv.out$lambda.1se  # lambda for this MSE -->

<!-- # visualize lasso results -->
<!-- lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1) -->
<!-- plot(lasso.mod, xvar = "lambda") -->
<!-- abline(v = log(cv.out$lambda.min), col = "red", lty = "dashed") -->
<!-- abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed") -->

<!-- # most influential variables -->
<!-- coef(cv.out, s = "lambda.1se") %>% -->
<!--   tidy() %>% -->
<!--   filter(row != "(Intercept)") %>% -->
<!--   ggplot(aes(value, reorder(row, value), color = value > 0)) + -->
<!--   geom_point(show.legend = FALSE) + -->
<!--   ggtitle("Influential variables") + -->
<!--   xlab("Coefficient") + -->
<!--   ylab(NULL) -->
<!-- ``` -->

<!-- ```{r ridge-vs-lasso-error} -->
<!-- # minimum Ridge MSE -->
<!-- min(ridge.cv.out$cvm) -->
<!-- plot(ridge.pred,test_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100)) -->

<!-- # minimum Lasso MSE -->
<!-- min(lasso.cv.out$cvm) -->
<!-- plot(lasso.pred,test_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100)) -->
<!-- ``` -->

## Multivariate Adaptive Regression Spline (MARS)
```{r basic-mars}
library(earth)     # fit MARS models

# Fit a basic MARS model
mars1 <- earth(
  PANSS_Total ~ .,  
  data = training_df[,-"PatientID"]
)

# Print model summary
print(mars1)
summary(mars1) %>% .$coefficients %>% head(10)
plot(mars1, which = 1)
```

```{r mars-interactions}
# Fit a basic MARS model
mars2 <- earth(
  PANSS_Total ~ .,  
  data = training_df[,-"PatientID"],
  degree = 3
)

# check out the first 10 coefficient terms
print(mars2)
summary(mars2) %>% .$coefficients %>% head(10)
plot(mars2, which = 1)
```

### Tuning
```{r mars-grid}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(1, 16, by = 2)
  )

head(hyper_grid)
```

```{r mars-tune}
library(caret)
set.seed(1)

# cross validated model
tuned_mars <- train(
  x = subset(training_df[,-"PatientID"], select = -PANSS_Total),
  y = training_df$PANSS_Total,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# best model
tuned_mars$bestTune
##    nprune degree
## 14     34      2

# plot results
ggplot(tuned_mars)
```
See http://uc-r.github.io/mars for how to visualize/interpret results further. 