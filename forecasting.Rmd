---
title: "Forecasting"
author: "Jeremy Binagia and Sai Gourisankar"
date: "7/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment 
```

## Notes/Assumptions
### General 
- predict "18th" week total PANSS score 
- i.e. create a csv file that contains the PatientID and the predicted 18th-week PANSS score
- can submit predictions here https://www.kaggle.com/c/stanford-stats-202-forecasting/data
- challenges
  - typically data in class ranges over time OR over individual. Here we have both. 
  - we can't average over individuals because we need distinct predictions for each individual given. 
  - we can't average over time because visitDay is likely a decently strong predictor for the level. 
- gradient descent reference http://uc-r.github.io/gbm_regression
- no need to standardize data if using decision trees!
  
### Model 
  - clearly PANSS total is the response 
  - likely predictors include
    - (maybe) study 
    - Country 
    - TxGroup
    - VisitDay
    - PatientID - this one is tricky. Technically setting a PatientID (I believe??) uniquely determines VisitDay, TxGroup, Country, and Study for our final prediction. HOWEVER, there could be a scenario where two patients completely align on all of these factors EXCEPT for the fact that they are different Patients (in which case including this predictor could be valuable). For example, perhaps patient 1 and patient 2 are identical in all ways except for over the course of the study the former consistently has lower scores. We want this information in our model. 
  - exclude
    - individual scores (if you had these, you could simply sum them to get the total score)
    - SiteID - we don't know where they will be evaluated 
    - RaterID - we don't know who will evaluate them (see for example https://groups.google.com/forum/#!searchin/stats202/model%7Csort:date/stats202/y7IxH2KYTk4/U-NfpWgGCQAJ)
  - when it comes time to predict, we specify a patient ID which then sets the above listed predictors except for VisitDay. *For the final predictions, let's assume VisitDay is one week after the given patient's last visit?*

### Splitting the data 
  - Following the suggested methodology found in slide 14 of the lecture 5 slides, we will split the data into 5 parts
    - test set: Start with the 17th-week for the 379 patients in study E (where we define "17"th-week as second-to-last visit) and 7 days to `VisitDay` as a proxy for "18th-week" visit. We will use the test set to choose between different models. 
    - training/development set: the remaining observations since ultimately we will use k-fold cross-validation.   
    - issues:
      - study E seems to be distinct in the segmentation plots from the other studies... could be a bias in that there is much more data from the other studies in our training set. 
  - following suggests combining studies A-D since they are essentially the same https://groups.google.com/forum/#!searchin/stats202/validation%7Csort:date/stats202/ULXRN3seaBE/5DFG-1LmEgAJs
  - suggests a 70/30 or 70/15/15 split for training vs. validation https://www.researchgate.net/post/Is_there_an_ideal_ratio_between_a_training_set_and_validation_set_Which_trade-off_would_you_suggest
  

## Thoughts/Conclusions

## Setup
```{r Load Libraries, results="hide"}
rm(list = ls()) # clear global environment 
library(plyr)
library(ggplot2)
```

```{r Load Data}
rm(list = ls()) # clear global environment 
A_df=read.csv("Data/Study_A.csv")
B_df=read.csv("Data/Study_B.csv")
C_df=read.csv("Data/Study_C.csv")
D_df=read.csv("Data/Study_D.csv")
E_df=read.csv("Data/Study_E.csv")
summary(E_df)
length(unique(E_df$PatientID))
```
Note that most patients from study E are from USA or Russia (about an even split). 

We should remove true duplicates from the dataset. Note that assessment ID is a completely unique identifier (no two rows have the same assessment id from examing the raw data in Excel). 
```{r remove-true-duplicates}
# # check that there are in fact duplicates 
# dfList = list(A_df,B_df,C_df,D_df,E_df)
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
# 
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
# 
# # check disregarding assessment id 
# A_df = A_df[ , -which(names(A_df) %in% c("AssessmentiD"))]
# B_df = B_df[ , -which(names(B_df) %in% c("AssessmentiD"))]
# C_df = C_df[ , -which(names(C_df) %in% c("AssessmentiD"))]
# D_df = D_df[ , -which(names(D_df) %in% c("AssessmentiD"))]
# E_df = E_df[ , -which(names(E_df) %in% c("AssessmentiD"))]
# 
# for (df in dfList){
#   print(dim(df))
#   print(dim(distinct(df)))
# }
# 
# # remove duplicates
# A_df = distinct(A_df)
# B_df = distinct(B_df)
# C_df = distinct(C_df)
# D_df = distinct(D_df)
# E_df = distinct(E_df)
```

The following creates a list of the patients we should consider for the Kaggle submission. 
```{r prediction-patients}
sample_submission_df = read.csv("Data/sample_submission_PANSS.csv")
prediction.patients = sample_submission_df$PatientID # the PatientID #s we should use for Kaggle submission 
length(prediction.patients)         # 379 values
length(unique(prediction.patients)) # 379 distinct values 
#n_distinct(prediction.patients)   # gives same result 
```

How many visits did each patient have?
```{r number-visits}
# number.visits = count(E_df, vars = "PatientID")
# 
# # Basic barplot
# p<-ggplot(data=number.visits, aes(x=PatientID, y=freq)) +
#   geom_bar(stat="identity") # meaning of stat option: "If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."
# p
```


## Data Cleaning
Remove columns not corresponding to our predictors and response (total PANSS score)
```{r subset-df}
A_df = subset(A_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
B_df = subset(B_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
C_df = subset(C_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
D_df = subset(D_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
E_df = subset(E_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
```

Combine studies
```{r combine-studies}
combined_df = rbind(A_df,B_df,C_df,D_df,E_df)
summary(combined_df)
```

What was the final visit day for each patient? We need this info since our test set will the final observation for each of the 379 selected study E patients we will evaluate our models on. 
```{r final-visit-day}
for (i in 1:dim(combined_df)[1]) {
  id = combined_df[i,"PatientID"]
  patient_df = subset(combined_df,PatientID == id)
  final.day = max(patient_df$VisitDay)
  #if (final.day==0){ # several patients must have dropped out immediately 
  #  print(combined_df[i,])
  #}
  combined_df[i,"FinalDay"] = final.day
}
```

### Create test set 
Set aside the test set, i.e. the patient's whose score we will predict on Kaggle. 
```{r test-set}
#test_df = combined_df[VisitDay==FinalDay & (PatientID %in% prediction.patients)  , ]
test_df = subset(combined_df, VisitDay==FinalDay & PatientID %in% prediction.patients)
dim(test_df)[1] 
```
Note that this subsetting does not produce 379 as expected. What is going on here? 
```{r explore-test-set}
for (id in unique(test_df$PatientID)) { # for each unique id
  sub_df = subset(test_df, PatientID==id)
  if (dim(sub_df)[1]>1){
    print(sub_df)
  }
}
```
We see that multiple patients were assessed multiple times on the final day (perhaps by different people, at different locations). We can remove such duplicates with the `distinct()` function.
```{r remove-duplicates}
library(dplyr)
test_df = distinct(test_df)
dim(test_df)[1]
```
This still doesn't yield a dataset of size 379 since there are PANSS_Total scores that differ! Thus, there must be patients who were assessed multiple times in the same day by the same person and at the same location (for example, PatientID 50505). 
```{r explore-test-set-v2}
for (id in unique(test_df$PatientID)) { # for each unique id
  sub_df = subset(test_df, PatientID==id)
  if (dim(sub_df)[1]>1){
    print(sub_df)
  }
}
```
We see that in each of these cases the difference in PANSS_Total is a few points (except PatientID 50299). Simplest solution is to average over these values: 
```{r simple-average}
pre_test_df = test_df # save what we have so far ... we will exclude this from the total data 

library(data.table)
keys <- colnames(test_df)[!grepl('PANSS_Total',colnames(test_df))] # all column names except for PANSS_Total
X <- as.data.table(test_df)
test_df = X[,list(mm=mean(PANSS_Total)),keys]
names(test_df)[7] = "PANSS_Total"
dim(test_df)
```
Which returns the desired number of 379 rows. 

Finally, we would like our test set to best reflect the "18-th week" visit. To do so, we imagine each patient going back for assessment one week after whenever their final week was. So we add a value of 7 days to their `VisitDay`. We also drop the `FinalDay` column at this point. Also potentially scale the data here. 
```{r create-test-set}
test_df = subset(test_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = test_df$VisitDay + 7 
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
```

### Create training set
```{r remove-test-from-total}
# dim(combined_df)
# combined_df = anti_join(combined_df, test_df)
# dim(combined_df)
```
This removes 410 elements as expected. We should also remove any duplicates from here as we did for the test set. Note that I ended up not removing these 410 observations since I considered them distinct from the test set (the test set has VisitDay shifted upwards by a value of 7).  

```{r remove-training-dups}
training_df = distinct(combined_df)
dim(training_df)[1]
``` 
We should also average over cases where all is identical except for the total PANSS score: 
```{r simple-average-training}
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[7] = "PANSS_Total"
dim(training_df)

training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
```

Finally, we may want to scale some variables.
```{r training-scale}
#test_df$PatientID = scale(test_df$PatientID)
#test_df$VisitDay = scale(test_df$VisitDay)
#test_df$PANSS_Total = scale(test_df$PANSS_Total)
```

## Gradient Boosting (gbm)
Note that in this section we make frequent use of code snippets given by http://uc-r.github.io/gbm_regression. 
```{r gboost}
set.seed(1)
library(gbm)          # basic implementation

# train GBM model
gbm.fit <- gbm(
  formula = PANSS_Total ~ .,
  distribution = "gaussian",
  data = training_df,
  n.trees = 5000,
  interaction.depth = 5, # 3 is optimal. 1 led to decrease. 
  shrinkage = 0.1,
  cv.folds = 10,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

# print results
print(gbm.fit)
## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = training_df, 
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.
```
Displayed values are best so far, as found via manual tuning. 

```{r visualize-boosting}
# get MSE
min(gbm.fit$cv.error)

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
```

### Tune boosting 
```{r gbm-tuning}
# create hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1),    # introduce stochastic gradient descent by allowing bag.fraction < 1
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)


# randomize data
random_index <- sample(1:nrow(training_df), nrow(training_df))
random_training_df <- training_df[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = PANSS_Total ~ .,
    distribution = "gaussian",
    data = random_training_df,
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r train-top-gbm-model}
set.seed(1)

# train GBM model
gbm.fit.final <- gbm(
  formula = PANSS_Total ~ .,
  distribution = "gaussian",
  data = training_df,
  n.trees = 5000,
  interaction.depth = 3,
  shrinkage = 0.1,
  n.minobsinnode = 5,
  bag.fraction = .65, 
  train.fraction = 1,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )
```

### Visualize
```{r gbm-visualize}
par(mar = c(5, 8, 1, 1))
summary(
  gbm.fit.final, 
  cBars = 10,
  method = relative.influence, # also can use permutation.test.gbm
  las = 2
  )
```

### Prediction
```{r gbm-prediction}
# predict values for test data
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, test_df)

# results
caret::RMSE(pred, test_df$PANSS_Total)

# write to csv for Kaggle submission
write.csv(pred,'healthstudy2.csv')
```

## Gradient Boosting (xgboost)
### xgboost setup
```{r xgboost-setup}
# variable names
features <- setdiff(names(ames_train), "Sale_Price")

# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(ames_train, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)     

# Prepare the training data
features_train <- vtreat::prepare(treatplan, ames_train, varRestriction = new_vars) %>% as.matrix()
response_train <- ames_train$Sale_Price

# Prepare the test data
features_test <- vtreat::prepare(treatplan, ames_test, varRestriction = new_vars) %>% as.matrix()
response_test <- ames_test$Sale_Price

# dimensions of one-hot encoded data
dim(features_train)
dim(features_test)
```

```{r sample-xgboost-fit}
# reproducibility
set.seed(123)

xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
```

```{r xgboost-visualize}
# get number of trees that minimize error
xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )
##   ntrees.train rmse.train ntrees.test rmse.test
## 1          965  0.5022836          60  27572.31

# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```

### xgboost Tuning 
```{r xgboost-grid}
# create hyperparameter grid
hyper_grid <- expand.grid(
  eta = c(.01, .05, .1, .3),
  max_depth = c(1, 3, 5, 7),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.65, .8, 1), 
  colsample_bytree = c(.8, .9, 1),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

nrow(hyper_grid)
```

```{r xgboost-grid-search}
# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
  
  # reproducibility
  set.seed(123)
  
  # train model
  xgb.tune <- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 5,
    objective = "reg:linear",  # for regression models
    verbose = 0,               # silent,
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
}

hyper_grid %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r xgboost-fit-final}
# parameter list
params <- list(
  eta = 0.01,
  max_depth = 5,
  min_child_weight = 5,
  subsample = 0.65,
  colsample_bytree = 1
)

# train final model
xgb.fit.final <- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1576,
  objective = "reg:linear",
  verbose = 0
)
```

### Visualization 

### Prediciton
```{r xgboost-prediction}
# predict values for test data
pred <- predict(xgb.fit.final, test_df)

# results
caret::RMSE(pred, response_test)
```


## Gradient Boosting (h2o)
```{r h20-startup}
library(h2o)          # a java-based platform
h2o.no_progress()
h2o.init(max_mem_size = "10g") # have 16g total 
```

```{r h20-model}
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df), y)

# turn training set into h2o object
train.h2o <- as.h2o(training_df)

# training basic GBM model with defaults
h2o.fit1 <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10
)

# assess model results
h2o.fit1
```

```{r h20-automated-stopping}
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10,
  ntrees = 10000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 1,
  max_runtime_secs = 60*5
)

# model stopped after xx trees
h2o.fit2@parameters$ntrees

# cross validated MSE
h2o.rmse(h2o.fit2, xval = TRUE)^2
```

### Full grid search
```{r h20-full-grid}
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]

# create hyperparameter grid
hyper_grid <- list(
  max_depth = c(1,5,10),
  min_rows = c(1, 5, 10), # minimum observations in a terminal node
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(1),
  sample_rate = c(.4, .6, .8, 1),
  col_sample_rate = c(.8, .9, 1)
)

# # perform grid search 
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x, 
#   y = y, 
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 5000,
#   stopping_rounds = 10,
#   stopping_tolerance = 0,
#   seed = 123
#   )
# 
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1", 
#   sort_by = "mse", 
#   decreasing = FALSE
#   )
# grid_perf
```

### Random discrete grid search 
```{r random-discrete-grid}
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement 
  max_runtime_secs = 60*5 # limit how long it runs
  )

# perform grid search 
grid <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid2",
  x = x, 
  y = y, 
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  search_criteria = search_criteria, # add search criteria
  ntrees = 1000,
  stopping_rounds = 10, 
  stopping_tolerance = 0,
  seed = 123
  )

# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid2", 
  sort_by = "mse", 
  decreasing = FALSE
  )
grid_perf
```

62 models took 5 mins to evaluate. Top 5 models after 5 mins all had max_depth of 5. Top 4 has min_rows = 1. All had learning rate > 0.05. 

```{r h20-performance}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
```

```{r h20-train-final}
# train final model
h2o.final <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10,
  ntrees = 10000,
  learn_rate = 0.1,
  learn_rate_annealing = 1,
  max_depth = 5,
  min_rows = 1,
  sample_rate = 0.6,
  col_sample_rate = 0.9,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 1
)

# model stopped after xx trees
h2o.final@parameters$ntrees

# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
```

### Visualization
```{r variable-importance}
h2o.varimp_plot(h2o.final, num_of_features = 10)
```

### Prediction
```{r h20-prediction}
# convert test set to h2o object
test.h2o <- as.h2o(test_df)

# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)

# predict with h2o.predict
h2o.predict(h2o.final, newdata = test.h2o)

# predict values with predict
predict(h2o.final, test.h2o)

# write to csv for Kaggle submission
write.csv(pred,'healthstudy2.csv')
```